# Statistical Tests and Models


## Tests for Exploratory Data Analysis

A collection of functions are available from `scipy.stats`.

+ Comparing the locations of two samples
    - `ttest_ind`: t-test for two independent samples
    - `ttest_rel`: t-test for paired samples
	- `ranksums`: Wilcoxon rank-sum test for two independent samples
	- `wilcoxon`: Wilcoxon signed-rank test for paired samples
+ Comparing the locations of multiple samples
    - `f_oneway`: one-way ANOVA
	- `kruskal`: Kruskal-Wallis H-test
+ Tests for associations in contigency tables
    - `chi2_contingency`: Chi-square test of independence of variables
	- `fisher_exact`:  Fisher exact test on a 2x2 contingency table
+ Goodness of fit
    - `goodness_of_fit`: distribution could contain unspecified parameters
	- `anderson`: Anderson-Darling test 
    - `kstest`: Kolmogorov-Smirnov test 
	- `chisquare`: one-way chi-square test
	- `normaltest`: test for normality


Since R has a richer collections of statistical functions, we can call 
R function from Python with `rpy2`. See, for example, a [blog on this
subject](https://rviews.rstudio.com/2022/05/25/calling-r-from-python-with-rpy2/).

## Linear Model

Let's simulate some data for illustrations.

```{python}
import numpy as np
import statsmodels.api as sm

nobs = 200
ncov = 5
np.random.seed(123)
x = np.random.random((nobs, ncov)) # Uniform over [0, 1)
beta = np.repeat(1, ncov)
y = 2 + np.dot(x, beta) + np.random.normal(size = nobs)
```

Check the shape of `y`:
```{python}
y.shape
```

Check the shape of `x`:
```{python}
x.shape
```

That is, the true linear regression model is
$$
y = 2 + x_1 + x_2 + x_3 + x_4 + x_5 + \epsilon.
$$

A regression model for the observed data can be fitted as

```{python}
xmat = sm.add_constant(x)
mymod = sm.OLS(y, xmat)
myfit = mymod.fit()
myfit.summary()
```

Questions to review:

+ How are the regression coefficients interpreted? Intercept?
+ Why does it make sense to center the covariates?


Now we form a data frame with the variables

```{python}
import pandas as pd
df = np.concatenate((y.reshape((nobs, 1)), x), axis = 1)
df = pd.DataFrame(data = df,
                  columns = ["y"] + ["x" + str(i) for i in range(1,
                  ncov + 1)])
df.info()
```

Let's use a formula to specify the regression model as in R, and fit
a robust linear model (`rlm`) instead of OLS. Note that the model specification
and the function interface is similar to R.

```{python}
import statsmodels.formula.api as smf
mymod = smf.rlm(formula = "y ~ x1 + x2 + x3 + x4 + x5", data = df)
myfit = mymod.fit()
myfit.summary()
```

For model diagnostics, one can check residual plots.

```{python}
import matplotlib.pyplot as plt

myOlsFit = smf.ols(formula = "y ~ x1 + x2 + x3 + x4 + x5", data = df).fit()
fig = plt.figure(figsize = (6, 6))
## residual versus x1; can do the same for other covariates
fig = sm.graphics.plot_regress_exog(myOlsFit, 'x1', fig=fig)
```

See more on [residual diagnostics and specification
tests](https://www.statsmodels.org/stable/stats.html#residual-diagnostics-and-specification-tests).


## Generalized Linear Regression

A linear regression model cannot be applied to presence/absence or
count data. 


Binary or count data need to be modeled under a generlized
framework. Consider a binary or count variable $Y$ with possible
covariates $X$.  A generalized model describes a transformation $g$
of the conditional mean $E[Y | X]$ by a linear predictor
$X^{\top}\beta$. That is
$$
g( E[Y | X] ) = X^{\top} \beta.
$$
The transformation $g$ is known as the link function.


For logistic regression with binary outcomes, the link function is
the logit function
$$
g(u) = \log \frac{u}{1 - u}, \quad u \in (0, 1).
$$

What is the interpretation of the regression coefficients in a
logistic regression? Intercept?

A logistic regression can be fit with `statsmodels.api.glm`.

Let's generate some binary data first by dichotomizing existing variables.

```{python}
df['yb' ] = np.where(df['y' ] > 2.5, 1, 0)
df['x1b'] = np.where(df['x1'] > 0.5, 1, 0)
```

Fit a logistic regression for `y1b`.

```{python}
mylogistic = smf.glm(formula = 'yb ~ x1b + x2 + x3 + x4 + x5', data = df,
                     family = sm.families.Binomial())
mylfit = mylogistic.fit()
mylfit.summary()
```

If we treat `y1b` as count data, a Poisson regression can be fitted.

```{python}
myPois = smf.glm(formula = 'yb ~ x1b + x2 + x3 + x4 + x5', data = df,
                 family = sm.families.Poisson())
mypfit = myPois.fit()
mypfit.summary()
```
