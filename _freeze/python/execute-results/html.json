{
  "hash": "744293052ba76228f15eb89a47017cd8",
  "result": {
    "markdown": "# Python Refreshment\n\n\nYou have programmed in Python. Regardless of your skill level, let us\ndo some refreshing.\n\n## The Python World\n\n+ Function: a block of organized, reusable code to complete certain\n  task.\n+ Module: a file containing a collection of functions, variables, and\n  statements.\n+ Package: a structured directory containing collections of modules\n  and an `__init.py__` file by which the directory is interpreted as a\n  package.\n+ Library: a collection of related functionality of codes. It is a\n  reusable chunk of code that we can use by importing it in our\n  program, we can just use it by importing that library and calling\n  the method of that library with period(.).\n\nSee, for example, [how to build a Python\nlibratry](https://medium.com/analytics-vidhya/how-to-create-a-python-library-7d5aea80cc3f).\n\n\n## Standard Library\n\nPython’s has an extensive standard library that offers a wide range of\nfacilities as indicated by the long table of contents listed below. \nSee documentation [online](https://docs.python.org/3/library/).\n\n> The library contains built-in modules (written in C) that provide access to\n> system functionality such as file I/O that would otherwise be inaccessible to\n> Python programmers, as well as modules written in Python that provide\n> standardized solutions for many problems that occur in everyday\n> programming. Some of these modules are explicitly designed to encourage and\n> enhance the portability of Python programs by abstracting away\n> platform-specifics into platform-neutral APIs.\n\n\nQuestion: How to get the constant $e$ to an arbitary precision?\n\nThe constant is only represented by a given double precision.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport math\nprint(\"%0.20f\" % math.e)\nprint(\"%0.80f\" % math.e)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n2.71828182845904509080\n2.71828182845904509079559829842764884233474731445312500000000000000000000000000000\n```\n:::\n:::\n\n\nNow use package `decimal` to export with an arbitary precision.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport decimal  # for what?\n\n## set the required number digits to 150\ndecimal.getcontext().prec = 150\ndecimal.Decimal(1).exp().to_eng_string()\ndecimal.Decimal(1).exp().to_eng_string()[2:]\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\n'71828182845904523536028747135266249775724709369995957496696762772407663035354759457138217852516642742746639193200305992181741359662904357290033429526'\n```\n:::\n:::\n\n\n## Important Libraries\n\n+ NumPy\n+ pandas\n+ matplotlib\n+ IPython/Jupyter\n+ SciPy\n+ scikit-learn\n+ statsmodels\n\nQuestion: how to draw a random sample from a normal distribution and\nevaluate the density and distributions at these points?\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfrom scipy.stats import norm\n\nmu, sigma = 2, 4\nmean, var, skew, kurt = norm.stats(mu, sigma, moments='mvsk')\nprint(mean, var, skew, kurt)\nx = norm.rvs(loc = mu, scale = sigma, size = 10)\nx\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n2.0 16.0 0.0 0.0\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\narray([ 0.07461869, -3.94412908, -1.55035816,  0.11643293,  1.11222155,\n       -1.83876486,  0.79395469, -7.84008326,  3.86468282,  2.73404202])\n```\n:::\n:::\n\n\nThe pdf and cdf can be evaluated:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nnorm.pdf(x, loc = mu, scale = sigma)\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\narray([0.08882567, 0.03306173, 0.0672633 , 0.08926887, 0.09730912,\n       0.06292973, 0.09530362, 0.00483882, 0.08946655, 0.09807028])\n```\n:::\n:::\n\n\n## Writing a Function\n\nConsider the Fibonacci Sequence\n$1, 1, 2, 3, 5, 8, 13, 21, 34, ...$.\nThe next number is found by adding up the two numbers before it.\nWe are going to use 3 ways to solve the problems.\n\n\nThe first is a recursive solution.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ndef fib_rs(n):\n    if (n==1 or n==2):\n        return 1\n    else:\n        return fib_rs(n - 1) + fib_rs(n - 2)\n\n%timeit fib_rs(10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n10.7 µs ± 175 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n```\n:::\n:::\n\n\nThe second uses dynamic programming memoization.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\ndef fib_dm_helper(n, mem):\n    if mem[n] is not None:\n        return mem[n]\n    elif (n == 1 or n == 2):\n        result = 1\n    else:\n        result = fib_dm_helper(n - 1, mem) + fib_dm_helper(n - 2, mem)\n    mem[n] = result\n    return result\n\ndef fib_dm(n):\n    mem = [None] * (n + 1)\n    return fib_dm_helper(n, mem)\n\n%timeit fib_dm(10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n2.78 µs ± 301 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n```\n:::\n:::\n\n\nThe third is still dynamic programming but bottom-up.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ndef fib_dbu(n):\n    mem = [None] * (n + 1)\n    mem[1]=1;\n    mem[2]=1;\n    for i in range(3,n+1):\n        mem[i] = mem[i-1] + mem[i-2]\n    return mem[n]\n\n\n%timeit fib_dbu(500)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n70.1 µs ± 4.79 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n```\n:::\n:::\n\n\nApparently, the three solutions have very different performance for\nlarger `n`.\n\n\n### Monte Hall\nHere is a function that performs the Monte Hall experiments.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nimport numpy as np\n\ndef montehall(ndoors, ntrials):\n    doors = np.arange(1, ndoors + 1) / 10\n    prize = np.random.choice(doors, size=ntrials)\n    player = np.random.choice(doors, size=ntrials)\n    host = np.array([np.random.choice([d for d in doors\n\t\t\t\t\t\t\t\t\t   if d not in [player[x], prize[x]]])\n\t\t\t\t\t for x in range(ntrials)])\n    player2 = np.array([np.random.choice([d for d in doors\n\t\t\t\t\t\t\t\t\t\t  if d not in [player[x], host[x]]])\n\t\t\t\t\t\tfor x in range(ntrials)])\n    return {'noswitch': np.sum(prize == player), 'switch': np.sum(prize == player2)}\n```\n:::\n\n\nTest it out:\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nmontehall(3, 1000)\nmontehall(4, 1000)\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\n{'noswitch': 279, 'switch': 372}\n```\n:::\n:::\n\n\nThe true value for the two strategies with $n$ doors are, respectively,\n$1 / n$ and $\\frac{n - 1}{n (n - 2)}$.\n\n\n## Variables versus Objects\n\nIn Python, variables and the objects they point to actually live in\ntwo different places in the computer memory. Think of variables as\npointers to the objects they’re associated with, rather than being\nthose objects. This matters when multiple variables point to the same\nobject.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nx = [1, 2, 3]  # create a list; x points to the list\ny = x          # y also points to the same list in the memory\ny.append(4)    # append to y\nx              # x changed!\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\n[1, 2, 3, 4]\n```\n:::\n:::\n\n\nNow check their addresses\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nprint(id(x))   # address of x\nprint(id(y))   # address of y\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n4849331008\n4849331008\n```\n:::\n:::\n\n\nNonetheless, some data types in Python are \"immutable\", meaning that\ntheir values cannot be changed in place. One such example is strings.\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nx = \"abc\"\ny = x\ny = \"xyz\"\nx\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\n'abc'\n```\n:::\n:::\n\n\nNow check their addresses\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nprint(id(x))   # address of x\nprint(id(y))   # address of y\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n4443925640\n4564582896\n```\n:::\n:::\n\n\nQuestion: What's mutable and what's immutable?\n\nAnything that is a collection of other objects is mutable, except\n``tuples``.\n\nNot all manipulations of mutable objects change the object rather than\ncreate a new object. Sometimes when you do something to a mutable\nobject, you get back a new object. Manipulations that change an\nexisting object, rather than create a new one, are referred to as\n“in-place mutations” or just “mutations.” So:\n\n+ __All__ manipulations of immutable types create new objects.\n+ __Some__ manipulations of mutable types create new objects.\n\nDifferent variables may all be pointing at the same object is\npreserved through function calls (a behavior known as “pass by\nobject-reference”). So if you pass a list to a function, and that\nfunction manipulates that list using an in-place mutation, that change\nwill affect any variable that was pointing to that same object outside\nthe function.\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nx = [1, 2, 3]\ny = x\n\ndef append_42(input_list):\n    input_list.append(42)\n    return input_list\n\nappend_42(x)\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\n[1, 2, 3, 42]\n```\n:::\n:::\n\n\nNote that both `x` and `y` have been appended by $42$.\n\n## Number Representation\n\nNumers in a computer's memory are represented by binary styles (on and\noff of bits).\n\n### Integers\nIf not careful, It is easy to be bitten by overflow with integers when\nusing Numpy and Pandas in Python.\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nimport numpy as np\n\nx = np.array(2**63 - 1 , dtype='int')\nx\n# This should be the largest number numpy can display, with\n# the default int8 type (64 bits)\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\narray(9223372036854775807)\n```\n:::\n:::\n\n\nWhat if we increment it by 1?\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\ny = np.array(x + 1, dtype='int')\ny\n# Because of the overflow, it becomes negative!\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```\narray(-9223372036854775808)\n```\n:::\n:::\n\n\nFor vanilla Python, the overflow errors are checked and more digits\nare allocated when needed, at the cost of being slow.\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\n2**63 * 1000\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```\n9223372036854775808000\n```\n:::\n:::\n\n\nThis number is 1000 times largger than the prior number,\nbut still displayed perfectly without any overflows\n\n### Floating Number\n\nStandard double-precision floating point number uses 64 bits. Among\nthem, 1 is for sign, 11 is for exponent, and 52 are fraction significand,\nSee <https://en.wikipedia.org/wiki/Double-precision_floating-point_format>.\nThe bottom line is that, of course, not every real number is exactly\nrepresentable.\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\n0.1 + 0.1 + 0.1 == 0.3\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```\nFalse\n```\n:::\n:::\n\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\n0.3 - 0.2 == 0.1\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```\nFalse\n```\n:::\n:::\n\n\nWhat is really going on?\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\nimport decimal\ndecimal.Decimal(0.1)\n```\n\n::: {.cell-output .cell-output-display execution_count=20}\n```\nDecimal('0.1000000000000000055511151231257827021181583404541015625')\n```\n:::\n:::\n\n\nBecause the mantissa bits are limited, it can not represent a floating point\nthat's both very big and very precise. Most computers can represent all integers\nup to $2^{53}$, after that it starts skipping numbers.\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\n2.1**53 + 1 == 2.1**53\n\n# Find a number larger than 2 to the 53rd\n```\n\n::: {.cell-output .cell-output-display execution_count=21}\n```\nTrue\n```\n:::\n:::\n\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\nx = 2.1**53\nfor i in range(1000000):\n    x = x + 1\nx == 2.1**53\n```\n\n::: {.cell-output .cell-output-display execution_count=22}\n```\nTrue\n```\n:::\n:::\n\n\nWe add 1 to `x` by 1000000 times, but it still equal to its initial\nvalue,  2.1**53. This is because this number is too big that computer\ncan't handle it with precision like add 1.\n\nMachine epsilon is the smallest positive floating-point number `x` such that\n`1 + x != 1`.\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\nprint(np.finfo(float).eps)\nprint(np.finfo(np.float32).eps)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n2.220446049250313e-16\n1.1920929e-07\n```\n:::\n:::\n\n\n",
    "supporting": [
      "python_files"
    ],
    "filters": [],
    "includes": {}
  }
}