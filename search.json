[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Data Science",
    "section": "",
    "text": "Preface\nThe notets are a Quarto book; for details about Quarto, visit https://quarto.org/docs/books.\nThe notes are a joint effort of the instructor and the students in STAT 3255/5255, Spring 2023. Students’ contributions were made through pull requests to our GitHub repo at https://github.com/statds/ids-s23. The GitHub repo of the notes from Spring 2022 are available at https://github.com/statds/ids-s22.\nOur mid-term project on the 311 requests of New York City is to be showcased at the NYC Open Data Week during 2-3 pm ET, Monday, March 13, 2023..\nAn interesting quote from VanderPlas (2016):\n\nWhen a technologically-minded person is asked to help a friend, family member, or colleague with a computer problem, most of the time it’s less a matter of knowing the answer as much as knowing how to quickly find an unknown answer. In data science it’s the same: searchable web resources such as online documentation, mailing-list threads, and StackOverflow answers contain a wealth of information, even (especially?) if it is a topic you’ve found yourself searching before. Being an effective practitioner of data science is less about memorizing the tool or command you should use for every possible situation, and more about learning to effectively find the information you don’t know, whether through a web search engine or another means.\n\n\n\n\n\nVanderPlas, Jake. 2016. Python Data Science Handbook: Essential Tools for Working with Data. O’Reilly Media, Inc."
  },
  {
    "objectID": "intro.html#what-is-data-science",
    "href": "intro.html#what-is-data-science",
    "title": "1  Introduction",
    "section": "1.1 What Is Data Science?",
    "text": "1.1 What Is Data Science?\nOne widely accepted concept is the three pillars of data science: mathematics/statistics, computer science, and domain knowledge.\nIn her 2014 Presidential Address, Prof. Bin Yu, then President of the Institute of Mathematical Statistics, gave an interesting definition: \\[\n\\mbox{Data Science} =\n\\mbox{S}\\mbox{D}\\mbox{C}^3,\n\\] where S is Statistics, D is domain/science knowledge, and the three C’s are computing, collaboration/teamwork, and communication to outsiders."
  },
  {
    "objectID": "intro.html#expectations-from-this-course",
    "href": "intro.html#expectations-from-this-course",
    "title": "1  Introduction",
    "section": "1.2 Expectations from This Course",
    "text": "1.2 Expectations from This Course\n\nProficiency in project management with Git.\nProficiency in project report with Quarto.\nHands-on experience with real-world data science project.\nCompetency in using Python and its extensions for data science.\nFull grasp of the meaning of the results from data science algorithms.\nBasic understanding the principles of the data science methods."
  },
  {
    "objectID": "intro.html#computing-environment",
    "href": "intro.html#computing-environment",
    "title": "1  Introduction",
    "section": "1.3 Computing Environment",
    "text": "1.3 Computing Environment\nAll setups are operating system dependent. As soon as possible, stay away from Windows. Otherwise, good luck (you will need it).\n\n1.3.1 Command Line Interface\nOn Linux or MacOS, simply open a terminal.\nOn Windows, several options can be considered.\n\nCygwin (with X): https://x.cygwin.com\nGit Bash: https://www.gitkraken.com/blog/what-is-git-bash\n\nTo jump start, here is a tutorial: Ubunto Linux for beginners.\nAt least, you need to know how to handle files and traverse across directories. The tab completion and introspection supports are very useful.\n\n\n1.3.2 Python\nSet up Python on your computer:\n\nPython 3.\nPython package manager miniconda or pip.\nIntegrated Development Environment (IDE) (Jupyter Notebook; RStudio; VS Code; Emacs; etc.)\n\nI will be using IPython and Jupyter Notebook in class.\nReadability is important! Check your Python coding styles against the recommended styles: https://peps.python.org/pep-0008/. A good place to start is the Section on “Code Lay-out”.\nOnline books on Python for data science:\n\n“Python Data Science Handbook: Essential Tools for Working with Data,” First Edition, by Jake VanderPlas, O’Reilly Media, 2016.\n\n\n“Python for Data Analysis: Data Wrangling with Pan- das, NumPy, and IPython.” Third Edition, by Wes McK- inney, O’Reilly Media, 2022."
  },
  {
    "objectID": "intro.html#data-challenges",
    "href": "intro.html#data-challenges",
    "title": "1  Introduction",
    "section": "1.4 Data Challenges",
    "text": "1.4 Data Challenges\n\nASA Data Challenge Expo\nKaggle\nDrivenData\nTop 10 Data Science Competitions in 2023"
  },
  {
    "objectID": "intro.html#wishlist",
    "href": "intro.html#wishlist",
    "title": "1  Introduction",
    "section": "1.5 Wishlist",
    "text": "1.5 Wishlist\nThis is a wish list from all members of the class (alphabetical order). Add yours; note the syntax of nested list in Markdown.\n\nAlsubai, Nadia\n\nBecome familiar with both machine learning and deep learning\nBecome proficient in at least one machine or deep learning library for Python\n\nBedard, Kaitlyn\n\nLearn how to use Git proficiently\nGain practical experience using data science methods\nLearn to use python libraries for data science\n\nCheu, Catherine\n\nLearn more about command prompts in Git Bash\nLearn more about data science principles\nLearn better programming techniques in Python\nBecome better in simulation techniques.\n\nHo, Garrick\n\nI want to be more confident with git\nBe able to create a data science project\nLearn more about data science\n\nJones, Courtney\n\nBecome proficient in Git and VS Code\nDeepen my understanding of data science\nBecome comfortable with the Terminal on my computer\n\nKarandikar, Shivaram\n\nUnderstand the workflow and life cycle of a data science project.\nLearn how to code efficiently.\n\nLunetta, Giovanni\n\nLearn to properly clean a dataset\nBecome proficient in building a machine learning model\n\nMastrorilli, Ginamarie\n\nBecome more comfortable with git.\nIncrease my ability to learn new programs.\n\nNguyen, Christine\n\nBecome proficient in Git and apply it.\nBuild my Python programming skills further.\n\nNhan, Nathan\n\nProperly learn how to use Git\nIncrease my understanding of data science and data collection\n\n\nNoel, Luke\n\nBecome proficient in Git/Github\nLearn data science techniques like deep/machine learning, etc.\n\nParchekani, Kian\n\nLearn how to use Git, Quarto\nGet an introduction to data science\nLearn if a career in this field is right for me\nCollaborate with others and gain project experience\n\nShen, Tong\n\nGet familiar with machine learning\nLearn how to deal with big secondary data\nGain some experiences with python\n\nSullivan, Collin\n\nI would like to learn to be able to use data science well enough to get a job in the field\nDiscover if this area of statistics is one that I am passionate about\nGain some project experience that I can cite or reference in interviews\nBe able to speak intelligently about data science and it’s facets\nGain practical experience\n\nWang, Chaoyang\n\nLearn Deep Learning and application on Finance\n\nYan, Jun\n\nMake data science more accessible to undergraduates\nCo-develop a Quarto Book in collaboration with the students\n\nYang Kang, Chua\n\nLearn more about git and github\nApply Data Science skill to my research\nDevelop a new machine learning model\n\nYeung, Shannon\n\nLearn more about Git\nget aa more well rounded understanding of data science\n\nYi, Guanghong\n\nKnow more about Data Science, and what Data Scientists do\nDo one(or more) real life data science project, and gain some practical experience.\n\nZheng, Michael\n\nBecome more comfortable with git\nLearn how to complete a data science project from start to finish\n\n\n\n1.5.1 Presentation Orders\nThe topic presentation order is set up in class.\n\npresenters = [\"Alsubai, Nadia\",\n              \"Bedard, Kaitlyn\",\n              \"Cheu, Catherine\",\n              \"Chua, Yang Kang\",\n              \"Cummins, Patrick\",\n              \"Ho, Garrick\",\n              \"Jones, Courtney\",\n              \"Karandikar, Shivaram\",\n              \"Lunetta, Giovanni\",\n              \"Mastrorilli, Ginamarie\",\n              \"Nguyen, Christine\",\n              \"Nhan, Nathan\",\n              \"Noel, Luke\",\n              \"Parchekani, Kian\",\n              \"Shen, Tong\",\n              \"Sullivan, Colin\",\n              \"Wang, Chaoyang\",\n              \"Whitney, William\",\n              \"Yeung, Shannon\",\n              \"Yi, Guanghong\",\n              \"Zheng, Michael\"]\n\nimport random\nrandom.seed(71323498112697523) # jointly set by the class on 01/30/2023\nrandom.sample(presenters, len(presenters))\n\n['Cheu, Catherine',\n 'Ho, Garrick',\n 'Mastrorilli, Ginamarie',\n 'Yi, Guanghong',\n 'Karandikar, Shivaram',\n 'Chua, Yang Kang',\n 'Jones, Courtney',\n 'Sullivan, Colin',\n 'Shen, Tong',\n 'Alsubai, Nadia',\n 'Yeung, Shannon',\n 'Bedard, Kaitlyn',\n 'Nhan, Nathan',\n 'Parchekani, Kian',\n 'Noel, Luke',\n 'Whitney, William',\n 'Wang, Chaoyang',\n 'Nguyen, Christine',\n 'Cummins, Patrick',\n 'Zheng, Michael',\n 'Lunetta, Giovanni']"
  },
  {
    "objectID": "intro.html#presentation-task-board",
    "href": "intro.html#presentation-task-board",
    "title": "1  Introduction",
    "section": "1.6 Presentation Task Board",
    "text": "1.6 Presentation Task Board\nHere are some example tasks:\n\nImport/Export data\nDescriptive statistics\nStatistical hypothesis tests scypy.stats\nModel formulas with patsy\nStatistical models with statsmodels\nData visualization with matplotlib\nGrammer of graphics for python plotnine\nHandling spatial data with geopandas\nShow your Data in a Google map with gmplot\nRandom forest\nNaive Bayes\nBagging vs boosting\nCalling C/C++ from Python\nCalling R from Python and vice versa\nDevelop a Python module\nNeural networks\nDeep learning\nTensorFlow\nAutoencoders\nReinforcement learning\n\nPlease use the following table to sign up.\n\n\n\n\n\n\n\n\nDate\nPresenter\nTopic\n\n\n\n\n02/06\nCheu, Catherine\nVisualization with matplotlib\n\n\n02/08\nHo, Garrick\nPandas part 1\n\n\n02/13\nMastrorilli, Ginamarie\nPandas part 2\n\n\n02/13\nYi, Guanghong\nGrammer of graphics for python plotnine\n\n\n02/15\nKarandikar, Shivaram\nText processing with nltk\n\n\n02/20\nChua, Yang Kang\nSupport Vector Machine with scikit-learn\n\n\n02/20\nJones, Courtney\nDescriptive Statistics\n\n\n02/22\nSullivan, Colin\nStatistical hypothesis tests scypy.stats\n\n\n02/27\nShen, Tong\nDecision tree with scikit-learn\n\n\n03/01\nBedard, Kaitlyn\nHandling spatial data with geopandas\n\n\n03/06\nNhan, Nathan\nBagging vs boosting\n\n\n03/08\nParchekani, Kian\nNaive Bayes\n\n\n03/20\nNoel, Luke\nPlotting on maps with gmplot\n\n\n03/20\nWhitney, William\n\n\n\n03/22\nNguyen, Christine\nCalling R from Python and vice versa\n\n\n03/27\nCummins, Patrick\n\n\n\n03/29\nZheng, Michael\nWeb Scraping with Selenium\n\n\n04/03\nLunetta, Giovanni\nSoftmax Regression & Neural Networks with TensorFlow"
  },
  {
    "objectID": "intro.html#final-project-presentation-schedule",
    "href": "intro.html#final-project-presentation-schedule",
    "title": "1  Introduction",
    "section": "1.7 Final Project Presentation Schedule",
    "text": "1.7 Final Project Presentation Schedule\nWe use the same order as the topic presentation for undergraduate final presentation.\n\n\n\nDate\nPresenter\n\n\n\n\n04/17\nHo, Garrick\n\n\n04/17\nMastrorilli, Ginamarie\n\n\n04/17\nYi, Guanghong\n\n\n04/17\nKarandikar, Shivaram\n\n\n04/19\nJones, Courtney\n\n\n04/19\nSullivan, Colin\n\n\n04/19\nBedard, Kaitlyn\n\n\n04/19\nNhan, Nathan\n\n\n04/24\nParchekani, Kian\n\n\n04/24\nNoel, Luke\n\n\n04/24\nWhitney, William\n\n\n04/24\nNguyen, Christine\n\n\n04/26\nCummins, Patrick\n\n\n04/26\nZheng, Michael\n\n\n04/26\nLunetta, Giovanni\n\n\n\nI encourage you to work on NYC open data or other open data for your projects and submit an abstract to the Government Advances in Statistical Programming (GASP) 2023 conference, June 14-15, 2023. The deadline for abstract submission is April 1."
  },
  {
    "objectID": "intro.html#contribute-to-the-class-notes",
    "href": "intro.html#contribute-to-the-class-notes",
    "title": "1  Introduction",
    "section": "1.8 Contribute to the Class Notes",
    "text": "1.8 Contribute to the Class Notes\n\nStart a new branch and switch to the new branch.\nOn the new branch, add a qmd file for your presentation\nEdit _quarto.yml add a line for your qmd file to include it in the notes.\nWork on your qmd file, test with quarto render.\nWhen satisfied, commit and make a pull request.\n\nI have added a template file mysection.qmd and a new line to _quarto.yml as an example.\nFor more detailed style guidance, please see my notes on statistical wrirting."
  },
  {
    "objectID": "intro.html#homework-requirements",
    "href": "intro.html#homework-requirements",
    "title": "1  Introduction",
    "section": "1.9 Homework Requirements",
    "text": "1.9 Homework Requirements\n\nUse the repo from Git Classroom to submit your work. See Chapter 2.\n\nKeep the repo clean (no tracking generated files).\nNever “Upload” your files; use the git command lines.\nMake commit message informative (think about the readers).\n\nUse quarto source only. See Chapter 3.\nFor the conveinence of greading, add your html output to a release in your repo."
  },
  {
    "objectID": "intro.html#my-presentation-topic-template",
    "href": "intro.html#my-presentation-topic-template",
    "title": "1  Introduction",
    "section": "1.10 My Presentation Topic (Template)",
    "text": "1.10 My Presentation Topic (Template)\n\n1.10.1 Introduction\nPut an overview here. Use Markdown syntax.\n\n\n1.10.2 Sub Topic 1\nPut materials on topic 1 here\nPython examples can be put into python code chunks:\n\nimport pandas as pd\n\n# do something\n\n\n\n1.10.3 Sub Topic 2\nPut materials on topic 2 here.\n\n\n1.10.4 Conclusion\nPut sumaries here."
  },
  {
    "objectID": "intro.html#practical-tips",
    "href": "intro.html#practical-tips",
    "title": "1  Introduction",
    "section": "1.11 Practical Tips",
    "text": "1.11 Practical Tips\n\n1.11.1 Data analysis\n\nUse an IDE so you can play with the data interactively\nCollect codes that have tested out into a script for batch processing\nDuring data cleaning, keep in mind how each variable will be used later\nNo keeping large data files in a repo; assume a reasonable location with your collaborators\n\n\n\n1.11.2 Presentation\n\nDon’t forget to introduce yourself if there is no moderator\nHighlight your research questions and results, not code\nGive an outline, carry it out, and summarize"
  },
  {
    "objectID": "git.html#set-up",
    "href": "git.html#set-up",
    "title": "2  Project Management with Git",
    "section": "2.1 Set Up",
    "text": "2.1 Set Up\nTo set up GitHub (other services like Bitbucket or GitLab are similar), you need to\n\nGenerate an SSH key if you don’t have one already.\nSign up an GitHub account.\nAdd the SSH key to your GitHub account"
  },
  {
    "objectID": "git.html#most-frequently-used-git-commands",
    "href": "git.html#most-frequently-used-git-commands",
    "title": "2  Project Management with Git",
    "section": "2.2 Most Frequently Used Git Commands",
    "text": "2.2 Most Frequently Used Git Commands\n\ngit clone\ngit pull\ngit status\ngit add\ngit remove\ngit commit\ngit push"
  },
  {
    "objectID": "git.html#tips-on-using-git",
    "href": "git.html#tips-on-using-git",
    "title": "2  Project Management with Git",
    "section": "2.3 Tips on using Git:",
    "text": "2.3 Tips on using Git:\n\nUse the command line interface instead of the web interface (e.g., upload on GitHub)\nMake frequent small commits instead of rare large commits.\nMake commit messages informative and meaningful.\nName your files/folders by some reasonable convention.\n\nLower cases are better than upper cases.\nNo blanks in file/folder names.\n\nKeep the repo clean by not tracking generated files.\nCreat a .gitignore file for better output from git status.\nKeep the linewidth of sources to under 80 for better git diff view."
  },
  {
    "objectID": "git.html#pull-request",
    "href": "git.html#pull-request",
    "title": "2  Project Management with Git",
    "section": "2.4 Pull Request",
    "text": "2.4 Pull Request\nTo contribute to an open source project (e.g., our classnotes), use pull requests. Pull requests “let you tell others about changes you’ve pushed to a branch in a repository on GitHub. Once a pull request is opened, you can discuss and review the potential changes with collaborators and add follow-up commits before your changes are merged into the base branch.”\nWatch this YouTube video: GitHub pull requests in 100 seconds."
  },
  {
    "objectID": "quarto.html",
    "href": "quarto.html",
    "title": "3  Reproducibile Data Science with Quarto",
    "section": "",
    "text": "Data science projects should be reproducible to be trustworthy. Dynamic documents facilitate reproducibility. Quarto is an open-source dynamic document preparation system, ideal for scientific and technical publishing. From the official websites, Quarto can be used to:\n\nCreate dynamic content with Python, R, Julia, and Observable.\nAuthor documents as plain text markdown or Jupyter notebooks.\nPublish high-quality articles, reports, presentations, websites, blogs, and books in HTML, PDF, MS Word, ePub, and more.\nAuthor with scientific markdown, including equations, citations, crossrefs, figure panels, callouts, advanced layout, and more.\n\nOf course, Quarto can be used to write homework, exams, and reports in this course.\nTo get started, see documentation at Quarto.\nThis is an extra line."
  },
  {
    "objectID": "python.html#the-python-world",
    "href": "python.html#the-python-world",
    "title": "4  Python Refreshment",
    "section": "4.1 The Python World",
    "text": "4.1 The Python World\n\nFunction: a block of organized, reusable code to complete certain task.\nModule: a file containing a collection of functions, variables, and statements.\nPackage: a structured directory containing collections of modules and an __init.py__ file by which the directory is interpreted as a package.\nLibrary: a collection of related functionality of codes. It is a reusable chunk of code that we can use by importing it in our program, we can just use it by importing that library and calling the method of that library with period(.).\n\nSee, for example, how to build a Python libratry."
  },
  {
    "objectID": "python.html#standard-library",
    "href": "python.html#standard-library",
    "title": "4  Python Refreshment",
    "section": "4.2 Standard Library",
    "text": "4.2 Standard Library\nPython’s has an extensive standard library that offers a wide range of facilities as indicated by the long table of contents listed below. See documentation online.\n\nThe library contains built-in modules (written in C) that provide access to system functionality such as file I/O that would otherwise be inaccessible to Python programmers, as well as modules written in Python that provide standardized solutions for many problems that occur in everyday programming. Some of these modules are explicitly designed to encourage and enhance the portability of Python programs by abstracting away platform-specifics into platform-neutral APIs.\n\nQuestion: How to get the constant \\(e\\) to an arbitary precision?\nThe constant is only represented by a given double precision.\n\nimport math\nprint(\"%0.20f\" % math.e)\nprint(\"%0.80f\" % math.e)\n\n2.71828182845904509080\n2.71828182845904509079559829842764884233474731445312500000000000000000000000000000\n\n\nNow use package decimal to export with an arbitary precision.\n\nimport decimal  # for what?\n\n## set the required number digits to 150\ndecimal.getcontext().prec = 150\ndecimal.Decimal(1).exp().to_eng_string()\ndecimal.Decimal(1).exp().to_eng_string()[2:]\n\n'71828182845904523536028747135266249775724709369995957496696762772407663035354759457138217852516642742746639193200305992181741359662904357290033429526'"
  },
  {
    "objectID": "python.html#important-libraries",
    "href": "python.html#important-libraries",
    "title": "4  Python Refreshment",
    "section": "4.3 Important Libraries",
    "text": "4.3 Important Libraries\n\nNumPy\npandas\nmatplotlib\nIPython/Jupyter\nSciPy\nscikit-learn\nstatsmodels\n\nQuestion: how to draw a random sample from a normal distribution and evaluate the density and distributions at these points?\n\nfrom scipy.stats import norm\n\nmu, sigma = 2, 4\nmean, var, skew, kurt = norm.stats(mu, sigma, moments='mvsk')\nprint(mean, var, skew, kurt)\nx = norm.rvs(loc = mu, scale = sigma, size = 10)\nx\n\n2.0 16.0 0.0 0.0\n\n\narray([ -4.44562667,   5.4660809 ,   3.5125469 , -11.83950295,\n         2.90749401,   4.70256047,   4.90975057,   7.73920953,\n         3.50416655,  -4.78896058])\n\n\nThe pdf and cdf can be evaluated:\n\nnorm.pdf(x, loc = mu, scale = sigma)\n\narray([0.02722693, 0.06851781, 0.09285403, 0.00025086, 0.09720154,\n       0.07938247, 0.07654966, 0.03563019, 0.09292742, 0.02362275])"
  },
  {
    "objectID": "python.html#writing-a-function",
    "href": "python.html#writing-a-function",
    "title": "4  Python Refreshment",
    "section": "4.4 Writing a Function",
    "text": "4.4 Writing a Function\nConsider the Fibonacci Sequence \\(1, 1, 2, 3, 5, 8, 13, 21, 34, ...\\). The next number is found by adding up the two numbers before it. We are going to use 3 ways to solve the problems.\nThe first is a recursive solution.\n\ndef fib_rs(n):\n    if (n==1 or n==2):\n        return 1\n    else:\n        return fib_rs(n - 1) + fib_rs(n - 2)\n\n%timeit fib_rs(10)\n\n10.5 µs ± 331 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\nThe second uses dynamic programming memoization.\n\ndef fib_dm_helper(n, mem):\n    if mem[n] is not None:\n        return mem[n]\n    elif (n == 1 or n == 2):\n        result = 1\n    else:\n        result = fib_dm_helper(n - 1, mem) + fib_dm_helper(n - 2, mem)\n    mem[n] = result\n    return result\n\ndef fib_dm(n):\n    mem = [None] * (n + 1)\n    return fib_dm_helper(n, mem)\n\n%timeit fib_dm(10)\n\n2.38 µs ± 26.5 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\nThe third is still dynamic programming but bottom-up.\n\ndef fib_dbu(n):\n    mem = [None] * (n + 1)\n    mem[1]=1;\n    mem[2]=1;\n    for i in range(3,n+1):\n        mem[i] = mem[i-1] + mem[i-2]\n    return mem[n]\n\n\n%timeit fib_dbu(500)\n\n63.9 µs ± 478 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\nApparently, the three solutions have very different performance for larger n.\n\n4.4.1 Monte Hall\nHere is a function that performs the Monte Hall experiments.\n\nimport numpy as np\n\ndef montehall(ndoors, ntrials):\n    doors = np.arange(1, ndoors + 1) / 10\n    prize = np.random.choice(doors, size=ntrials)\n    player = np.random.choice(doors, size=ntrials)\n    host = np.array([np.random.choice([d for d in doors\n                                       if d not in [player[x], prize[x]]])\n                     for x in range(ntrials)])\n    player2 = np.array([np.random.choice([d for d in doors\n                                          if d not in [player[x], host[x]]])\n                        for x in range(ntrials)])\n    return {'noswitch': np.sum(prize == player), 'switch': np.sum(prize == player2)}\n\nTest it out:\n\nmontehall(3, 1000)\nmontehall(4, 1000)\n\n{'noswitch': 236, 'switch': 388}\n\n\nThe true value for the two strategies with \\(n\\) doors are, respectively, \\(1 / n\\) and \\(\\frac{n - 1}{n (n - 2)}\\)."
  },
  {
    "objectID": "python.html#variables-versus-objects",
    "href": "python.html#variables-versus-objects",
    "title": "4  Python Refreshment",
    "section": "4.5 Variables versus Objects",
    "text": "4.5 Variables versus Objects\nIn Python, variables and the objects they point to actually live in two different places in the computer memory. Think of variables as pointers to the objects they’re associated with, rather than being those objects. This matters when multiple variables point to the same object.\n\nx = [1, 2, 3]  # create a list; x points to the list\ny = x          # y also points to the same list in the memory\ny.append(4)    # append to y\nx              # x changed!\n\n[1, 2, 3, 4]\n\n\nNow check their addresses\n\nprint(id(x))   # address of x\nprint(id(y))   # address of y\n\n4959635584\n4959635584\n\n\nNonetheless, some data types in Python are “immutable”, meaning that their values cannot be changed in place. One such example is strings.\n\nx = \"abc\"\ny = x\ny = \"xyz\"\nx\n\n'abc'\n\n\nNow check their addresses\n\nprint(id(x))   # address of x\nprint(id(y))   # address of y\n\n4554513544\n4675105776\n\n\nQuestion: What’s mutable and what’s immutable?\nAnything that is a collection of other objects is mutable, except tuples.\nNot all manipulations of mutable objects change the object rather than create a new object. Sometimes when you do something to a mutable object, you get back a new object. Manipulations that change an existing object, rather than create a new one, are referred to as “in-place mutations” or just “mutations.” So:\n\nAll manipulations of immutable types create new objects.\nSome manipulations of mutable types create new objects.\n\nDifferent variables may all be pointing at the same object is preserved through function calls (a behavior known as “pass by object-reference”). So if you pass a list to a function, and that function manipulates that list using an in-place mutation, that change will affect any variable that was pointing to that same object outside the function.\n\nx = [1, 2, 3]\ny = x\n\ndef append_42(input_list):\n    input_list.append(42)\n    return input_list\n\nappend_42(x)\n\n[1, 2, 3, 42]\n\n\nNote that both x and y have been appended by \\(42\\)."
  },
  {
    "objectID": "python.html#number-representation",
    "href": "python.html#number-representation",
    "title": "4  Python Refreshment",
    "section": "4.6 Number Representation",
    "text": "4.6 Number Representation\nNumers in a computer’s memory are represented by binary styles (on and off of bits).\n\n4.6.1 Integers\nIf not careful, It is easy to be bitten by overflow with integers when using Numpy and Pandas in Python.\n\nimport numpy as np\n\nx = np.array(2**63 - 1 , dtype='int')\nx\n# This should be the largest number numpy can display, with\n# the default int8 type (64 bits)\n\narray(9223372036854775807)\n\n\nWhat if we increment it by 1?\n\ny = np.array(x + 1, dtype='int')\ny\n# Because of the overflow, it becomes negative!\n\narray(-9223372036854775808)\n\n\nFor vanilla Python, the overflow errors are checked and more digits are allocated when needed, at the cost of being slow.\n\n2**63 * 1000\n\n9223372036854775808000\n\n\nThis number is 1000 times largger than the prior number, but still displayed perfectly without any overflows\n\n\n4.6.2 Floating Number\nStandard double-precision floating point number uses 64 bits. Among them, 1 is for sign, 11 is for exponent, and 52 are fraction significand, See https://en.wikipedia.org/wiki/Double-precision_floating-point_format. The bottom line is that, of course, not every real number is exactly representable.\n\n0.1 + 0.1 + 0.1 == 0.3\n\nFalse\n\n\n\n0.3 - 0.2 == 0.1\n\nFalse\n\n\nWhat is really going on?\n\nimport decimal\ndecimal.Decimal(0.1)\n\nDecimal('0.1000000000000000055511151231257827021181583404541015625')\n\n\nBecause the mantissa bits are limited, it can not represent a floating point that’s both very big and very precise. Most computers can represent all integers up to \\(2^{53}\\), after that it starts skipping numbers.\n\n2.1**53 + 1 == 2.1**53\n\n# Find a number larger than 2 to the 53rd\n\nTrue\n\n\n\nx = 2.1**53\nfor i in range(1000000):\n    x = x + 1\nx == 2.1**53\n\nTrue\n\n\nWe add 1 to x by 1000000 times, but it still equal to its initial value, 2.1**53. This is because this number is too big that computer can’t handle it with precision like add 1.\nMachine epsilon is the smallest positive floating-point number x such that 1 + x != 1.\n\nprint(np.finfo(float).eps)\nprint(np.finfo(np.float32).eps)\n\n2.220446049250313e-16\n1.1920929e-07"
  },
  {
    "objectID": "pandas.html#series",
    "href": "pandas.html#series",
    "title": "5  Data Manipulation with Pandas",
    "section": "5.1 Series",
    "text": "5.1 Series\n\nA Series is a one-dimensional array-like object containing a sequence of values and an associated array of data labels, called its index\n\n\n[1, 3, 4]\n\n[1, 3, 4]\n\n\nTo create a series, usepd.Series().\n\ns = pd.Series([4, 7, -5, 3])\ns\n\n0    4\n1    7\n2   -5\n3    3\ndtype: int64\n\n\nThe string representation of a Series displayed interactively shows the index on the left and the values on the right. Since we did not specify an index for the data, a default one consisting of the integers 0 through n-1 (where n is the length of the data).\n\ns = pd.Series([4, 7, -5, 3], index = ['a', 'b', 'c', 'd'])\ns\n\na    4\nb    7\nc   -5\nd    3\ndtype: int64\n\n\n\ns.values\n\narray([ 4,  7, -5,  3])\n\n\n\ns.index\n\nIndex(['a', 'b', 'c', 'd'], dtype='object')\n\n\n\n5.1.1 Selecting single or a set of values using index\nTo select a single value, use [ ].\n\ns['b']\n\n7\n\n\nTo select multiple values, add a comma between each value and use double brackets.\n\ns[['c', 'a', 'b']]\n\nc   -5\na    4\nb    7\ndtype: int64\n\n\nSelecting by the index is also possible.\n\ns[2]\n\n-5\n\n\nTo select multiple values in a row, use : and the first index is where it starts and the second index is where it stops. It does not include the second index.\n\ns[1:3]\n\nb    7\nc   -5\ndtype: int64\n\n\nTo select multiple values, add a comma between each value and use double brackets.\n\ns[[0,3]]\n\na    4\nd    3\ndtype: int64\n\n\n\n\n5.1.2 Filtering\nFiltering values in a series can be done with <, >, =.\n\ns > 0\n\na     True\nb     True\nc    False\nd     True\ndtype: bool\n\n\n\ns[s > 0]\n\na    4\nb    7\nd    3\ndtype: int64\n\n\n\n\n5.1.3 Math operation\nMath functions are able to be apply to a series.\n\ns**2\n\na    16\nb    49\nc    25\nd     9\ndtype: int64\n\n\n\nnp.exp(s)\n\na      54.598150\nb    1096.633158\nc       0.006738\nd      20.085537\ndtype: float64\n\n\n\ns.mean()\n\n2.25\n\n\nSeries are aligned by index label in arithmetic operations.\n\ns2 = pd.Series([1, 2, 3, 4], index = ['a', 'c', 'd', 'e'])\n\n\nzero = s + s2\nzero\n\na    5.0\nb    NaN\nc   -3.0\nd    6.0\ne    NaN\ndtype: float64\n\n\nNote: “NaN” stands for missing values in pandas\n\n\n5.1.4 Finding NaN values\nTo find all the missing values in a series, use .isnull().\n\nzero.isnull()\n\na    False\nb     True\nc    False\nd    False\ne     True\ndtype: bool\n\n\nTo find all the non-missing vales, use .notnull().\n\nzero.notnull()\n\na     True\nb    False\nc     True\nd     True\ne    False\ndtype: bool\n\n\n\nzero[zero.notnull()]\n\na    5.0\nc   -3.0\nd    6.0\ndtype: float64\n\n\n\n\n5.1.5 Replacing NaN\nTo change NaN to 0, use .fillna().\n\nzero.fillna(0)\n\na    5.0\nb    0.0\nc   -3.0\nd    6.0\ne    0.0\ndtype: float64\n\n\n\n\n5.1.6 Forward-fill\nFill all the NaN values with the previous value in the series.\n\nzero.fillna(method = 'ffill')\n\na    5.0\nb    5.0\nc   -3.0\nd    6.0\ne    6.0\ndtype: float64\n\n\n\n\n5.1.7 Back-fill\nFill all the NaN values with the next value in the series.\nNote that e is Nan because there is no next value in the series.\n\nzero.fillna(method = 'bfill')\n\na    5.0\nb   -3.0\nc   -3.0\nd    6.0\ne    NaN\ndtype: float64\n\n\n\n\n5.1.8 Drop\nTo drop all NaN, use .dropna().\n\nzero.dropna()\n\na    5.0\nc   -3.0\nd    6.0\ndtype: float64\n\n\nNotice how that zero hasn’t change at all. If the function wants to be applied to the original series, set it to its self.\n\nzero\n\na    5.0\nb    NaN\nc   -3.0\nd    6.0\ne    NaN\ndtype: float64\n\n\n\nzero = zero.dropna()\nzero\n\na    5.0\nc   -3.0\nd    6.0\ndtype: float64\n\n\nchange the index to be the same as s2 so there is no missing value\n\ns.index = ['a', 'c', 'd', 'e']\ns + s2\n\na    5\nc    9\nd   -2\ne    7\ndtype: int64"
  },
  {
    "objectID": "pandas.html#dataframe",
    "href": "pandas.html#dataframe",
    "title": "5  Data Manipulation with Pandas",
    "section": "5.2 DataFrame",
    "text": "5.2 DataFrame\n\nA DataFrame represents a rectangular table of data and contains an ordered collection of columns. The DataFrame has both a row and column index.\n\n\nSince each column of a DataFrame is essentially a Series with its column index, it can be thought of as a dictionary of Series all sharing the same index.\nEach column (Series) has to be the same type, whereas, each row can contain mixed types.\n\n\n5.2.1 Creating DataFrame\n\n5.2.1.1 from a dict of equal-length lists\n\ndata = {'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada', 'Nevada'],\n'year': [2000, 2001, 2002, 2001, 2002, 2003],\n'pop': [1.5, 1.7, 3.6, 2.4, 2.9, 3.2]}\nd = pd.DataFrame(data)\nd\n\n\n\n\n\n  \n    \n      \n      state\n      year\n      pop\n    \n  \n  \n    \n      0\n      Ohio\n      2000\n      1.5\n    \n    \n      1\n      Ohio\n      2001\n      1.7\n    \n    \n      2\n      Ohio\n      2002\n      3.6\n    \n    \n      3\n      Nevada\n      2001\n      2.4\n    \n    \n      4\n      Nevada\n      2002\n      2.9\n    \n    \n      5\n      Nevada\n      2003\n      3.2\n    \n  \n\n\n\n\n\n\n5.2.1.2 from an DataFrame\nStarting with an empty dataframe, series are able to be added to the Dataframe.\n\nd1 = pd.DataFrame()\n\n\nd1['state'] = ['Ohio', 'Nevada']\nd1['year'] = [2001, 2001]\nd1['pop'] = [1.7, 2.4]\n\n\nd1\n\n\n\n\n\n  \n    \n      \n      state\n      year\n      pop\n    \n  \n  \n    \n      0\n      Ohio\n      2001\n      1.7\n    \n    \n      1\n      Nevada\n      2001\n      2.4\n    \n  \n\n\n\n\n\n\n\n5.2.2 select columns\n\nd\n\n\n\n\n\n  \n    \n      \n      state\n      year\n      pop\n    \n  \n  \n    \n      0\n      Ohio\n      2000\n      1.5\n    \n    \n      1\n      Ohio\n      2001\n      1.7\n    \n    \n      2\n      Ohio\n      2002\n      3.6\n    \n    \n      3\n      Nevada\n      2001\n      2.4\n    \n    \n      4\n      Nevada\n      2002\n      2.9\n    \n    \n      5\n      Nevada\n      2003\n      3.2\n    \n  \n\n\n\n\n\nd['state']\n\n0      Ohio\n1      Ohio\n2      Ohio\n3    Nevada\n4    Nevada\n5    Nevada\nName: state, dtype: object\n\n\n\nd[['state','pop']]\n\n\n\n\n\n  \n    \n      \n      state\n      pop\n    \n  \n  \n    \n      0\n      Ohio\n      1.5\n    \n    \n      1\n      Ohio\n      1.7\n    \n    \n      2\n      Ohio\n      3.6\n    \n    \n      3\n      Nevada\n      2.4\n    \n    \n      4\n      Nevada\n      2.9\n    \n    \n      5\n      Nevada\n      3.2\n    \n  \n\n\n\n\n\n\n5.2.3 select rows\n\nd2 = pd.DataFrame(np.arange(16).reshape((4, 4)),\n                    index=['Ohio', 'Colorado', 'Utah', 'New York'],\n                     columns=['one', 'two', 'three', 'four'])\nd2\n\n\n\n\n\n  \n    \n      \n      one\n      two\n      three\n      four\n    \n  \n  \n    \n      Ohio\n      0\n      1\n      2\n      3\n    \n    \n      Colorado\n      4\n      5\n      6\n      7\n    \n    \n      Utah\n      8\n      9\n      10\n      11\n    \n    \n      New York\n      12\n      13\n      14\n      15\n    \n  \n\n\n\n\n\nd2.loc['Colorado': 'Utah']\n\n\n\n\n\n  \n    \n      \n      one\n      two\n      three\n      four\n    \n  \n  \n    \n      Colorado\n      4\n      5\n      6\n      7\n    \n    \n      Utah\n      8\n      9\n      10\n      11\n    \n  \n\n\n\n\n\nd2.iloc[1:3]\n\n\n\n\n\n  \n    \n      \n      one\n      two\n      three\n      four\n    \n  \n  \n    \n      Colorado\n      4\n      5\n      6\n      7\n    \n    \n      Utah\n      8\n      9\n      10\n      11\n    \n  \n\n\n\n\n\n\n5.2.4 change row index and column name\nUse .rename ti rename any row or column.\n\nd2.rename(index={'Colorado':'Connecticut'},columns={'one':'five'})\n\n\n\n\n\n  \n    \n      \n      five\n      two\n      three\n      four\n    \n  \n  \n    \n      Ohio\n      0\n      1\n      2\n      3\n    \n    \n      Connecticut\n      4\n      5\n      6\n      7\n    \n    \n      Utah\n      8\n      9\n      10\n      11\n    \n    \n      New York\n      12\n      13\n      14\n      15\n    \n  \n\n\n\n\nNotice how d2 does not change.\n\nd2\n\n\n\n\n\n  \n    \n      \n      one\n      two\n      three\n      four\n    \n  \n  \n    \n      Ohio\n      0\n      1\n      2\n      3\n    \n    \n      Colorado\n      4\n      5\n      6\n      7\n    \n    \n      Utah\n      8\n      9\n      10\n      11\n    \n    \n      New York\n      12\n      13\n      14\n      15\n    \n  \n\n\n\n\nYou can use inplace = True to change the original Dataframe.\n\nd2.rename(index = {'Colorado':'Connecticut'}, columns = {'one':'five'}, inplace = True)\n\n\nd2\n\n\n\n\n\n  \n    \n      \n      five\n      two\n      three\n      four\n    \n  \n  \n    \n      Ohio\n      0\n      1\n      2\n      3\n    \n    \n      Connecticut\n      4\n      5\n      6\n      7\n    \n    \n      Utah\n      8\n      9\n      10\n      11\n    \n    \n      New York\n      12\n      13\n      14\n      15\n    \n  \n\n\n\n\n\n\n5.2.5 basics attributes and methods\n\nd2.index\n\nIndex(['Ohio', 'Connecticut', 'Utah', 'New York'], dtype='object')\n\n\n\nd2.columns\n\nIndex(['five', 'two', 'three', 'four'], dtype='object')\n\n\n\nd2.values\n\narray([[ 0,  1,  2,  3],\n       [ 4,  5,  6,  7],\n       [ 8,  9, 10, 11],\n       [12, 13, 14, 15]])\n\n\n\nd2.shape\n\n(4, 4)\n\n\n\nd2.mean()\n\nfive     6.0\ntwo      7.0\nthree    8.0\nfour     9.0\ndtype: float64\n\n\n\n\n5.2.6 add and delete rows and columns\nFor dropping a row or column use .drop.\n\nd2.drop(index = \"Connecticut\", columns = \"five\") # add \"inplace=True\" will change the original DataFrame\n\n\n\n\n\n  \n    \n      \n      two\n      three\n      four\n    \n  \n  \n    \n      Ohio\n      1\n      2\n      3\n    \n    \n      Utah\n      9\n      10\n      11\n    \n    \n      New York\n      13\n      14\n      15\n    \n  \n\n\n\n\n\nd2\n\n\n\n\n\n  \n    \n      \n      five\n      two\n      three\n      four\n    \n  \n  \n    \n      Ohio\n      0\n      1\n      2\n      3\n    \n    \n      Connecticut\n      4\n      5\n      6\n      7\n    \n    \n      Utah\n      8\n      9\n      10\n      11\n    \n    \n      New York\n      12\n      13\n      14\n      15\n    \n  \n\n\n\n\nFor deleting a column use del.\n\ndel d2['five']\nd2\n\n\n\n\n\n  \n    \n      \n      two\n      three\n      four\n    \n  \n  \n    \n      Ohio\n      1\n      2\n      3\n    \n    \n      Connecticut\n      5\n      6\n      7\n    \n    \n      Utah\n      9\n      10\n      11\n    \n    \n      New York\n      13\n      14\n      15\n    \n  \n\n\n\n\n\nd2['one'] = [1, 2, 3, 4]\nd2\n\n\n\n\n\n  \n    \n      \n      two\n      three\n      four\n      one\n    \n  \n  \n    \n      Ohio\n      1\n      2\n      3\n      1\n    \n    \n      Connecticut\n      5\n      6\n      7\n      2\n    \n    \n      Utah\n      9\n      10\n      11\n      3\n    \n    \n      New York\n      13\n      14\n      15\n      4\n    \n  \n\n\n\n\n.pop returns the values and removes it from the original Dataframe.\n\nd2.pop('one')\n\nOhio           1\nConnecticut    2\nUtah           3\nNew York       4\nName: one, dtype: int64\n\n\n\nd2\n\n\n\n\n\n  \n    \n      \n      two\n      three\n      four\n    \n  \n  \n    \n      Ohio\n      1\n      2\n      3\n    \n    \n      Connecticut\n      5\n      6\n      7\n    \n    \n      Utah\n      9\n      10\n      11\n    \n    \n      New York\n      13\n      14\n      15\n    \n  \n\n\n\n\n\n\n5.2.7 Common method\nYou can import dataset as well\n\n5.2.7.1 csv file\n\nimport pandas as pd\ncrashes = pd.read_csv(\"data/nyc_crashes_202301.csv\")\n\n\nsub_set_1 = crashes.iloc[0:35, 0:8]\n\n\n\n5.2.7.2 Head and Tail\nThese two methods show the first and the last a few records from a DataFrame, default is 5.\n\nsub_set_1.head()\n\n\n\n\n\n  \n    \n      \n      CRASH DATE\n      CRASH TIME\n      BOROUGH\n      ZIP CODE\n      LATITUDE\n      LONGITUDE\n      LOCATION\n      ON STREET NAME\n    \n  \n  \n    \n      0\n      01/01/2023\n      14:38\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      BROOKLYN QUEENS EXPRESSWAY RAMP\n    \n    \n      1\n      01/01/2023\n      8:04\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NASSAU EXPRESSWAY\n    \n    \n      2\n      01/01/2023\n      18:05\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      10 AVENUE\n    \n    \n      3\n      01/01/2023\n      23:45\n      NaN\n      NaN\n      40.769737\n      -73.91244\n      (40.769737, -73.91244)\n      ASTORIA BOULEVARD\n    \n    \n      4\n      01/01/2023\n      4:50\n      NaN\n      NaN\n      40.830555\n      -73.85072\n      (40.830555, -73.85072)\n      CASTLE HILL AVENUE\n    \n  \n\n\n\n\n\nsub_set_1.tail()\n\n\n\n\n\n  \n    \n      \n      CRASH DATE\n      CRASH TIME\n      BOROUGH\n      ZIP CODE\n      LATITUDE\n      LONGITUDE\n      LOCATION\n      ON STREET NAME\n    \n  \n  \n    \n      30\n      01/01/2023\n      17:05\n      NaN\n      NaN\n      40.795250\n      -73.97321\n      (40.79525, -73.97321)\n      WEST 96 STREET\n    \n    \n      31\n      01/01/2023\n      5:45\n      QUEENS\n      11411.0\n      40.690640\n      -73.72832\n      (40.69064, -73.72832)\n      NaN\n    \n    \n      32\n      01/01/2023\n      0:55\n      NaN\n      NaN\n      40.818470\n      -73.94140\n      (40.81847, -73.9414)\n      WEST 140 STREET\n    \n    \n      33\n      01/01/2023\n      10:16\n      QUEENS\n      11369.0\n      40.764633\n      -73.86537\n      (40.764633, -73.86537)\n      NaN\n    \n    \n      34\n      01/01/2023\n      3:45\n      QUEENS\n      11694.0\n      40.580510\n      -73.84773\n      (40.58051, -73.84773)\n      NaN\n    \n  \n\n\n\n\n\nsub_set_1.head(3)\n\n\n\n\n\n  \n    \n      \n      CRASH DATE\n      CRASH TIME\n      BOROUGH\n      ZIP CODE\n      LATITUDE\n      LONGITUDE\n      LOCATION\n      ON STREET NAME\n    \n  \n  \n    \n      0\n      01/01/2023\n      14:38\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      BROOKLYN QUEENS EXPRESSWAY RAMP\n    \n    \n      1\n      01/01/2023\n      8:04\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NASSAU EXPRESSWAY\n    \n    \n      2\n      01/01/2023\n      18:05\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      10 AVENUE\n    \n  \n\n\n\n\n\n\n\n5.2.8 unique and nunique\nTo show only unique values, use .unique.\n\nsub_set_1['BOROUGH'].unique()\n\narray([nan, 'BROOKLYN', 'QUEENS', 'MANHATTAN', 'STATEN ISLAND', 'BRONX'],\n      dtype=object)\n\n\nTo get the number of unique values, use .nunique.\n\nsub_set_1['BOROUGH'].nunique()\n\n5\n\n\n\n\n5.2.9 count and value_counts\nTo count the non missing values, use .count.\n\nsub_set_1['BOROUGH'].count()\n\n21\n\n\nTo count the number in each categroy, use .value_counts.\n\nsub_set_1['BOROUGH'].value_counts()\n\nQUEENS           12\nBROOKLYN          6\nMANHATTAN         1\nSTATEN ISLAND     1\nBRONX             1\nName: BOROUGH, dtype: int64\n\n\n\n\n5.2.10 describe and info\n\nsub_set_1.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 35 entries, 0 to 34\nData columns (total 8 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   CRASH DATE      35 non-null     object \n 1   CRASH TIME      35 non-null     object \n 2   BOROUGH         21 non-null     object \n 3   ZIP CODE        21 non-null     float64\n 4   LATITUDE        30 non-null     float64\n 5   LONGITUDE       30 non-null     float64\n 6   LOCATION        30 non-null     object \n 7   ON STREET NAME  27 non-null     object \ndtypes: float64(3), object(5)\nmemory usage: 2.3+ KB\n\n\nSummary statistics for numeric type columns.\nUse .describe to get an quick summary of the data.\n\nsub_set_1.describe()\n\n\n\n\n\n  \n    \n      \n      ZIP CODE\n      LATITUDE\n      LONGITUDE\n    \n  \n  \n    \n      count\n      21.000000\n      30.000000\n      30.000000\n    \n    \n      mean\n      11188.095238\n      40.728138\n      -73.885913\n    \n    \n      std\n      416.736476\n      0.086772\n      0.076496\n    \n    \n      min\n      10026.000000\n      40.580510\n      -74.093414\n    \n    \n      25%\n      11207.000000\n      40.664881\n      -73.937413\n    \n    \n      50%\n      11354.000000\n      40.747835\n      -73.886998\n    \n    \n      75%\n      11369.000000\n      40.793048\n      -73.848292\n    \n    \n      max\n      11694.000000\n      40.861810\n      -73.728320\n    \n  \n\n\n\n\n\nsub_set_1.describe(percentiles=[x/10 for x in list(range(1, 10, 1))])\n\n\n\n\n\n  \n    \n      \n      ZIP CODE\n      LATITUDE\n      LONGITUDE\n    \n  \n  \n    \n      count\n      21.000000\n      30.000000\n      30.000000\n    \n    \n      mean\n      11188.095238\n      40.728138\n      -73.885913\n    \n    \n      std\n      416.736476\n      0.086772\n      0.076496\n    \n    \n      min\n      10026.000000\n      40.580510\n      -74.093414\n    \n    \n      10%\n      10468.000000\n      40.598224\n      -73.959692\n    \n    \n      20%\n      11207.000000\n      40.649558\n      -73.943165\n    \n    \n      30%\n      11223.000000\n      40.673481\n      -73.925244\n    \n    \n      40%\n      11233.000000\n      40.708116\n      -73.906082\n    \n    \n      50%\n      11354.000000\n      40.747835\n      -73.886998\n    \n    \n      60%\n      11357.000000\n      40.760311\n      -73.871886\n    \n    \n      70%\n      11369.000000\n      40.776043\n      -73.850365\n    \n    \n      80%\n      11372.000000\n      40.808170\n      -73.828438\n    \n    \n      90%\n      11411.000000\n      40.833641\n      -73.805664\n    \n    \n      max\n      11694.000000\n      40.861810\n      -73.728320\n    \n  \n\n\n\n\nchoose a specific column to get a summary for.\n\nsub_set_1['BOROUGH'].describe()\n\ncount         21\nunique         5\ntop       QUEENS\nfreq          12\nName: BOROUGH, dtype: object\n\n\n\n\n5.2.11 idxmax and nlargest\n.idxmax() returns the index of the largest value.\n\nsub_set_1['ZIP CODE'].idxmax()\n\n34\n\n\n.idxmin() returns the index of the smallest value\n\nsub_set_1['ZIP CODE'].idxmin()\n\n7\n\n\n.nlargest returns the largest values with their index (default is 5).\n\nsub_set_1['ZIP CODE'].nlargest()\n\n34    11694.0\n11    11691.0\n31    11411.0\n13    11375.0\n26    11372.0\nName: ZIP CODE, dtype: float64\n\n\n.nsmallest returns the smallest 3 values with their index (default is 5).\n\nsub_set_1['ZIP CODE'].nsmallest()\n\n7     10026.0\n17    10305.0\n28    10468.0\n6     11101.0\n12    11207.0\nName: ZIP CODE, dtype: float64\n\n\n\n\n5.2.12 sort\nuse .sort_values to sort values\n\nsub_set_1.sort_values(by = 'BOROUGH')\n\n\n\n\n\n  \n    \n      \n      CRASH DATE\n      CRASH TIME\n      BOROUGH\n      ZIP CODE\n      LATITUDE\n      LONGITUDE\n      LOCATION\n      ON STREET NAME\n    \n  \n  \n    \n      28\n      01/01/2023\n      7:40\n      BRONX\n      10468.0\n      40.861810\n      -73.912320\n      (40.86181, -73.91232)\n      NaN\n    \n    \n      5\n      01/01/2023\n      0:35\n      BROOKLYN\n      11229.0\n      40.601310\n      -73.954720\n      (40.60131, -73.95472)\n      NaN\n    \n    \n      27\n      01/01/2023\n      5:36\n      BROOKLYN\n      11207.0\n      40.663136\n      -73.883250\n      (40.663136, -73.88325)\n      NaN\n    \n    \n      22\n      01/01/2023\n      4:20\n      BROOKLYN\n      11233.0\n      40.670116\n      -73.922480\n      (40.670116, -73.92248)\n      SAINT JOHNS PLACE\n    \n    \n      15\n      01/01/2023\n      3:30\n      BROOKLYN\n      11236.0\n      40.636720\n      -73.887695\n      (40.63672, -73.887695)\n      NaN\n    \n    \n      19\n      01/01/2023\n      1:10\n      BROOKLYN\n      11223.0\n      40.593760\n      -73.982740\n      (40.59376, -73.98274)\n      AVENUE V\n    \n    \n      12\n      01/01/2023\n      1:45\n      BROOKLYN\n      11207.0\n      40.652767\n      -73.886300\n      (40.652767, -73.8863)\n      PENNSYLVANIA AVENUE\n    \n    \n      7\n      01/01/2023\n      1:00\n      MANHATTAN\n      10026.0\n      40.805595\n      -73.958190\n      (40.805595, -73.95819)\n      WEST 116 STREET\n    \n    \n      14\n      01/01/2023\n      17:35\n      QUEENS\n      11354.0\n      40.771587\n      -73.810070\n      (40.771587, -73.81007)\n      BAYSIDE AVENUE\n    \n    \n      16\n      01/01/2023\n      10:30\n      QUEENS\n      11355.0\n      40.751800\n      -73.817314\n      (40.7518, -73.817314)\n      HOLLY AVENUE\n    \n    \n      33\n      01/01/2023\n      10:16\n      QUEENS\n      11369.0\n      40.764633\n      -73.865370\n      (40.764633, -73.86537)\n      NaN\n    \n    \n      21\n      01/01/2023\n      20:25\n      QUEENS\n      11357.0\n      40.786440\n      -73.829155\n      (40.78644, -73.829155)\n      140 STREET\n    \n    \n      11\n      01/01/2023\n      23:06\n      QUEENS\n      11691.0\n      40.598720\n      -73.766010\n      (40.59872, -73.76601)\n      BEACH 32 STREET\n    \n    \n      23\n      01/01/2023\n      6:45\n      QUEENS\n      11368.0\n      40.739280\n      -73.850530\n      (40.73928, -73.85053)\n      SAULTELL AVENUE\n    \n    \n      24\n      01/01/2023\n      23:15\n      QUEENS\n      11369.0\n      40.757430\n      -73.876230\n      (40.75743, -73.87623)\n      NaN\n    \n    \n      26\n      01/01/2023\n      4:00\n      QUEENS\n      11372.0\n      40.751003\n      -73.892130\n      (40.751003, -73.89213)\n      74 STREET\n    \n    \n      6\n      01/01/2023\n      5:45\n      QUEENS\n      11101.0\n      40.744667\n      -73.931694\n      (40.744667, -73.931694)\n      QUEENS BOULEVARD\n    \n    \n      31\n      01/01/2023\n      5:45\n      QUEENS\n      11411.0\n      40.690640\n      -73.728320\n      (40.69064, -73.72832)\n      NaN\n    \n    \n      13\n      01/01/2023\n      17:55\n      QUEENS\n      11375.0\n      40.710320\n      -73.849980\n      (40.71032, -73.84998)\n      METROPOLITAN AVENUE\n    \n    \n      34\n      01/01/2023\n      3:45\n      QUEENS\n      11694.0\n      40.580510\n      -73.847730\n      (40.58051, -73.84773)\n      NaN\n    \n    \n      17\n      01/01/2023\n      6:40\n      STATEN ISLAND\n      10305.0\n      40.585240\n      -74.093414\n      (40.58524, -74.093414)\n      HYLAN BOULEVARD\n    \n    \n      0\n      01/01/2023\n      14:38\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      BROOKLYN QUEENS EXPRESSWAY RAMP\n    \n    \n      1\n      01/01/2023\n      8:04\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NASSAU EXPRESSWAY\n    \n    \n      2\n      01/01/2023\n      18:05\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      10 AVENUE\n    \n    \n      3\n      01/01/2023\n      23:45\n      NaN\n      NaN\n      40.769737\n      -73.912440\n      (40.769737, -73.91244)\n      ASTORIA BOULEVARD\n    \n    \n      4\n      01/01/2023\n      4:50\n      NaN\n      NaN\n      40.830555\n      -73.850720\n      (40.830555, -73.85072)\n      CASTLE HILL AVENUE\n    \n    \n      8\n      01/01/2023\n      19:53\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      EAST 222 STREET\n    \n    \n      9\n      01/01/2023\n      3:11\n      NaN\n      NaN\n      40.842110\n      -73.825570\n      (40.84211, -73.82557)\n      BRUCKNER EXPRESSWAY\n    \n    \n      10\n      01/01/2023\n      18:49\n      NaN\n      NaN\n      40.674923\n      -73.736940\n      (40.674923, -73.73694)\n      MERRICK BOULEVARD\n    \n    \n      18\n      01/01/2023\n      5:30\n      NaN\n      NaN\n      40.704810\n      -73.939320\n      (40.70481, -73.93932)\n      SEIGEL STREET\n    \n    \n      20\n      01/01/2023\n      0:04\n      NaN\n      NaN\n      40.858090\n      -73.901924\n      (40.85809, -73.901924)\n      EAST 183 STREET\n    \n    \n      25\n      01/01/2023\n      3:35\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      GRAND CENTRAL PARKWAY\n    \n    \n      29\n      01/01/2023\n      10:50\n      NaN\n      NaN\n      40.832700\n      -73.950226\n      (40.8327, -73.950226)\n      HENRY HUDSON PARKWAY\n    \n    \n      30\n      01/01/2023\n      17:05\n      NaN\n      NaN\n      40.795250\n      -73.973210\n      (40.79525, -73.97321)\n      WEST 96 STREET\n    \n    \n      32\n      01/01/2023\n      0:55\n      NaN\n      NaN\n      40.818470\n      -73.941400\n      (40.81847, -73.9414)\n      WEST 140 STREET\n    \n  \n\n\n\n\n\nsub_set_1.sort_values(by = ['CRASH DATE', 'ZIP CODE'], ascending = True)\n\n\n\n\n\n  \n    \n      \n      CRASH DATE\n      CRASH TIME\n      BOROUGH\n      ZIP CODE\n      LATITUDE\n      LONGITUDE\n      LOCATION\n      ON STREET NAME\n    \n  \n  \n    \n      7\n      01/01/2023\n      1:00\n      MANHATTAN\n      10026.0\n      40.805595\n      -73.958190\n      (40.805595, -73.95819)\n      WEST 116 STREET\n    \n    \n      17\n      01/01/2023\n      6:40\n      STATEN ISLAND\n      10305.0\n      40.585240\n      -74.093414\n      (40.58524, -74.093414)\n      HYLAN BOULEVARD\n    \n    \n      28\n      01/01/2023\n      7:40\n      BRONX\n      10468.0\n      40.861810\n      -73.912320\n      (40.86181, -73.91232)\n      NaN\n    \n    \n      6\n      01/01/2023\n      5:45\n      QUEENS\n      11101.0\n      40.744667\n      -73.931694\n      (40.744667, -73.931694)\n      QUEENS BOULEVARD\n    \n    \n      12\n      01/01/2023\n      1:45\n      BROOKLYN\n      11207.0\n      40.652767\n      -73.886300\n      (40.652767, -73.8863)\n      PENNSYLVANIA AVENUE\n    \n    \n      27\n      01/01/2023\n      5:36\n      BROOKLYN\n      11207.0\n      40.663136\n      -73.883250\n      (40.663136, -73.88325)\n      NaN\n    \n    \n      19\n      01/01/2023\n      1:10\n      BROOKLYN\n      11223.0\n      40.593760\n      -73.982740\n      (40.59376, -73.98274)\n      AVENUE V\n    \n    \n      5\n      01/01/2023\n      0:35\n      BROOKLYN\n      11229.0\n      40.601310\n      -73.954720\n      (40.60131, -73.95472)\n      NaN\n    \n    \n      22\n      01/01/2023\n      4:20\n      BROOKLYN\n      11233.0\n      40.670116\n      -73.922480\n      (40.670116, -73.92248)\n      SAINT JOHNS PLACE\n    \n    \n      15\n      01/01/2023\n      3:30\n      BROOKLYN\n      11236.0\n      40.636720\n      -73.887695\n      (40.63672, -73.887695)\n      NaN\n    \n    \n      14\n      01/01/2023\n      17:35\n      QUEENS\n      11354.0\n      40.771587\n      -73.810070\n      (40.771587, -73.81007)\n      BAYSIDE AVENUE\n    \n    \n      16\n      01/01/2023\n      10:30\n      QUEENS\n      11355.0\n      40.751800\n      -73.817314\n      (40.7518, -73.817314)\n      HOLLY AVENUE\n    \n    \n      21\n      01/01/2023\n      20:25\n      QUEENS\n      11357.0\n      40.786440\n      -73.829155\n      (40.78644, -73.829155)\n      140 STREET\n    \n    \n      23\n      01/01/2023\n      6:45\n      QUEENS\n      11368.0\n      40.739280\n      -73.850530\n      (40.73928, -73.85053)\n      SAULTELL AVENUE\n    \n    \n      24\n      01/01/2023\n      23:15\n      QUEENS\n      11369.0\n      40.757430\n      -73.876230\n      (40.75743, -73.87623)\n      NaN\n    \n    \n      33\n      01/01/2023\n      10:16\n      QUEENS\n      11369.0\n      40.764633\n      -73.865370\n      (40.764633, -73.86537)\n      NaN\n    \n    \n      26\n      01/01/2023\n      4:00\n      QUEENS\n      11372.0\n      40.751003\n      -73.892130\n      (40.751003, -73.89213)\n      74 STREET\n    \n    \n      13\n      01/01/2023\n      17:55\n      QUEENS\n      11375.0\n      40.710320\n      -73.849980\n      (40.71032, -73.84998)\n      METROPOLITAN AVENUE\n    \n    \n      31\n      01/01/2023\n      5:45\n      QUEENS\n      11411.0\n      40.690640\n      -73.728320\n      (40.69064, -73.72832)\n      NaN\n    \n    \n      11\n      01/01/2023\n      23:06\n      QUEENS\n      11691.0\n      40.598720\n      -73.766010\n      (40.59872, -73.76601)\n      BEACH 32 STREET\n    \n    \n      34\n      01/01/2023\n      3:45\n      QUEENS\n      11694.0\n      40.580510\n      -73.847730\n      (40.58051, -73.84773)\n      NaN\n    \n    \n      0\n      01/01/2023\n      14:38\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      BROOKLYN QUEENS EXPRESSWAY RAMP\n    \n    \n      1\n      01/01/2023\n      8:04\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NASSAU EXPRESSWAY\n    \n    \n      2\n      01/01/2023\n      18:05\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      10 AVENUE\n    \n    \n      3\n      01/01/2023\n      23:45\n      NaN\n      NaN\n      40.769737\n      -73.912440\n      (40.769737, -73.91244)\n      ASTORIA BOULEVARD\n    \n    \n      4\n      01/01/2023\n      4:50\n      NaN\n      NaN\n      40.830555\n      -73.850720\n      (40.830555, -73.85072)\n      CASTLE HILL AVENUE\n    \n    \n      8\n      01/01/2023\n      19:53\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      EAST 222 STREET\n    \n    \n      9\n      01/01/2023\n      3:11\n      NaN\n      NaN\n      40.842110\n      -73.825570\n      (40.84211, -73.82557)\n      BRUCKNER EXPRESSWAY\n    \n    \n      10\n      01/01/2023\n      18:49\n      NaN\n      NaN\n      40.674923\n      -73.736940\n      (40.674923, -73.73694)\n      MERRICK BOULEVARD\n    \n    \n      18\n      01/01/2023\n      5:30\n      NaN\n      NaN\n      40.704810\n      -73.939320\n      (40.70481, -73.93932)\n      SEIGEL STREET\n    \n    \n      20\n      01/01/2023\n      0:04\n      NaN\n      NaN\n      40.858090\n      -73.901924\n      (40.85809, -73.901924)\n      EAST 183 STREET\n    \n    \n      25\n      01/01/2023\n      3:35\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      GRAND CENTRAL PARKWAY\n    \n    \n      29\n      01/01/2023\n      10:50\n      NaN\n      NaN\n      40.832700\n      -73.950226\n      (40.8327, -73.950226)\n      HENRY HUDSON PARKWAY\n    \n    \n      30\n      01/01/2023\n      17:05\n      NaN\n      NaN\n      40.795250\n      -73.973210\n      (40.79525, -73.97321)\n      WEST 96 STREET\n    \n    \n      32\n      01/01/2023\n      0:55\n      NaN\n      NaN\n      40.818470\n      -73.941400\n      (40.81847, -73.9414)\n      WEST 140 STREET\n    \n  \n\n\n\n\n\n\n5.2.13 [] method\n[] method can be used to select column(s) by passing column name(s).\n\nsub_set_1['ZIP CODE'].head()\n\n0   NaN\n1   NaN\n2   NaN\n3   NaN\n4   NaN\nName: ZIP CODE, dtype: float64\n\n\n\nsub_set_1[['BOROUGH', 'ZIP CODE', 'LOCATION']].head()\n\n\n\n\n\n  \n    \n      \n      BOROUGH\n      ZIP CODE\n      LOCATION\n    \n  \n  \n    \n      0\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      NaN\n      NaN\n      (40.769737, -73.91244)\n    \n    \n      4\n      NaN\n      NaN\n      (40.830555, -73.85072)\n    \n  \n\n\n\n\n\n\n5.2.14 loc method\nloc can be used to index row(s) and column(s) by providing the row and column labels.\ndf.loc[row_label(s)] Selects single row or subset of rows from the DataFrame by label.\nIndex single row\n\nsub_set_1.loc[7]\n\nCRASH DATE                    01/01/2023\nCRASH TIME                          1:00\nBOROUGH                        MANHATTAN\nZIP CODE                         10026.0\nLATITUDE                       40.805595\nLONGITUDE                      -73.95819\nLOCATION          (40.805595, -73.95819)\nON STREET NAME           WEST 116 STREET\nName: 7, dtype: object\n\n\nIndex multiple rows\n\nsub_set_1.loc[:8]\n\n\n\n\n\n  \n    \n      \n      CRASH DATE\n      CRASH TIME\n      BOROUGH\n      ZIP CODE\n      LATITUDE\n      LONGITUDE\n      LOCATION\n      ON STREET NAME\n    \n  \n  \n    \n      0\n      01/01/2023\n      14:38\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      BROOKLYN QUEENS EXPRESSWAY RAMP\n    \n    \n      1\n      01/01/2023\n      8:04\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NASSAU EXPRESSWAY\n    \n    \n      2\n      01/01/2023\n      18:05\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      10 AVENUE\n    \n    \n      3\n      01/01/2023\n      23:45\n      NaN\n      NaN\n      40.769737\n      -73.912440\n      (40.769737, -73.91244)\n      ASTORIA BOULEVARD\n    \n    \n      4\n      01/01/2023\n      4:50\n      NaN\n      NaN\n      40.830555\n      -73.850720\n      (40.830555, -73.85072)\n      CASTLE HILL AVENUE\n    \n    \n      5\n      01/01/2023\n      0:35\n      BROOKLYN\n      11229.0\n      40.601310\n      -73.954720\n      (40.60131, -73.95472)\n      NaN\n    \n    \n      6\n      01/01/2023\n      5:45\n      QUEENS\n      11101.0\n      40.744667\n      -73.931694\n      (40.744667, -73.931694)\n      QUEENS BOULEVARD\n    \n    \n      7\n      01/01/2023\n      1:00\n      MANHATTAN\n      10026.0\n      40.805595\n      -73.958190\n      (40.805595, -73.95819)\n      WEST 116 STREET\n    \n    \n      8\n      01/01/2023\n      19:53\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      EAST 222 STREET\n    \n  \n\n\n\n\n\nsub_set_1.loc[[0, 7, 4, 6]]\n\n\n\n\n\n  \n    \n      \n      CRASH DATE\n      CRASH TIME\n      BOROUGH\n      ZIP CODE\n      LATITUDE\n      LONGITUDE\n      LOCATION\n      ON STREET NAME\n    \n  \n  \n    \n      0\n      01/01/2023\n      14:38\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      BROOKLYN QUEENS EXPRESSWAY RAMP\n    \n    \n      7\n      01/01/2023\n      1:00\n      MANHATTAN\n      10026.0\n      40.805595\n      -73.958190\n      (40.805595, -73.95819)\n      WEST 116 STREET\n    \n    \n      4\n      01/01/2023\n      4:50\n      NaN\n      NaN\n      40.830555\n      -73.850720\n      (40.830555, -73.85072)\n      CASTLE HILL AVENUE\n    \n    \n      6\n      01/01/2023\n      5:45\n      QUEENS\n      11101.0\n      40.744667\n      -73.931694\n      (40.744667, -73.931694)\n      QUEENS BOULEVARD\n    \n  \n\n\n\n\ndf.loc[:, col_labels] Selects single column or subset of columns by label.\n\nsub_set_1.loc[:, 'LOCATION'].head()\n\n0                       NaN\n1                       NaN\n2                       NaN\n3    (40.769737, -73.91244)\n4    (40.830555, -73.85072)\nName: LOCATION, dtype: object\n\n\n\nsub_set_1.loc[:, 'LATITUDE': 'LOCATION'].head()\n\n\n\n\n\n  \n    \n      \n      LATITUDE\n      LONGITUDE\n      LOCATION\n    \n  \n  \n    \n      0\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      40.769737\n      -73.91244\n      (40.769737, -73.91244)\n    \n    \n      4\n      40.830555\n      -73.85072\n      (40.830555, -73.85072)\n    \n  \n\n\n\n\n\nsub_set_1.loc[:, ['BOROUGH', 'ZIP CODE', 'LOCATION']].head()\n\n\n\n\n\n  \n    \n      \n      BOROUGH\n      ZIP CODE\n      LOCATION\n    \n  \n  \n    \n      0\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      NaN\n      NaN\n      (40.769737, -73.91244)\n    \n    \n      4\n      NaN\n      NaN\n      (40.830555, -73.85072)\n    \n  \n\n\n\n\ndf.loc[row_label(s), col_label(s)] Select both rows and columns by label.\n\nsub_set_1.loc[7, 'BOROUGH']\n\n'MANHATTAN'\n\n\n\nsub_set_1.loc[:8, ['BOROUGH', 'LOCATION']]\n\n\n\n\n\n  \n    \n      \n      BOROUGH\n      LOCATION\n    \n  \n  \n    \n      0\n      NaN\n      NaN\n    \n    \n      1\n      NaN\n      NaN\n    \n    \n      2\n      NaN\n      NaN\n    \n    \n      3\n      NaN\n      (40.769737, -73.91244)\n    \n    \n      4\n      NaN\n      (40.830555, -73.85072)\n    \n    \n      5\n      BROOKLYN\n      (40.60131, -73.95472)\n    \n    \n      6\n      QUEENS\n      (40.744667, -73.931694)\n    \n    \n      7\n      MANHATTAN\n      (40.805595, -73.95819)\n    \n    \n      8\n      NaN\n      NaN\n    \n  \n\n\n\n\nIndex by Boolean Series\n\nsub_set_1['BOROUGH'].isin(['MANHATTAN','QUEENS']).head()\n\n0    False\n1    False\n2    False\n3    False\n4    False\nName: BOROUGH, dtype: bool\n\n\n\nsub_set_1.loc[sub_set_1['BOROUGH'].isin(['MANHATTAN','QUEENS'])].head()\n\n\n\n\n\n  \n    \n      \n      CRASH DATE\n      CRASH TIME\n      BOROUGH\n      ZIP CODE\n      LATITUDE\n      LONGITUDE\n      LOCATION\n      ON STREET NAME\n    \n  \n  \n    \n      6\n      01/01/2023\n      5:45\n      QUEENS\n      11101.0\n      40.744667\n      -73.931694\n      (40.744667, -73.931694)\n      QUEENS BOULEVARD\n    \n    \n      7\n      01/01/2023\n      1:00\n      MANHATTAN\n      10026.0\n      40.805595\n      -73.958190\n      (40.805595, -73.95819)\n      WEST 116 STREET\n    \n    \n      11\n      01/01/2023\n      23:06\n      QUEENS\n      11691.0\n      40.598720\n      -73.766010\n      (40.59872, -73.76601)\n      BEACH 32 STREET\n    \n    \n      13\n      01/01/2023\n      17:55\n      QUEENS\n      11375.0\n      40.710320\n      -73.849980\n      (40.71032, -73.84998)\n      METROPOLITAN AVENUE\n    \n    \n      14\n      01/01/2023\n      17:35\n      QUEENS\n      11354.0\n      40.771587\n      -73.810070\n      (40.771587, -73.81007)\n      BAYSIDE AVENUE\n    \n  \n\n\n\n\nUse “&” (and), “|” (or) “~” (not) for Pandas\n\nsub_set_1.loc[(sub_set_1[\"BOROUGH\"] == \"MANHATTAN\") & (sub_set_1[\"ZIP CODE\"] \n>= 1000)]\n\n\n\n\n\n  \n    \n      \n      CRASH DATE\n      CRASH TIME\n      BOROUGH\n      ZIP CODE\n      LATITUDE\n      LONGITUDE\n      LOCATION\n      ON STREET NAME\n    \n  \n  \n    \n      7\n      01/01/2023\n      1:00\n      MANHATTAN\n      10026.0\n      40.805595\n      -73.95819\n      (40.805595, -73.95819)\n      WEST 116 STREET\n    \n  \n\n\n\n\n\n\n5.2.15 iloc method\niloc can be used to index row(s) and column(s) by providing the row and column integer(s).\ndf.iloc[row_integer(s)] Selects single row or subset of rows from the DataFrame by integer position\nNote: same as indexing for sequence (but different with loc, it is 0 basis and the selection is close to the left and open to the right (the last item is excluded).\n\nsub_set_1.iloc[3]\n\nCRASH DATE                    01/01/2023\nCRASH TIME                         23:45\nBOROUGH                              NaN\nZIP CODE                             NaN\nLATITUDE                       40.769737\nLONGITUDE                      -73.91244\nLOCATION          (40.769737, -73.91244)\nON STREET NAME         ASTORIA BOULEVARD\nName: 3, dtype: object\n\n\n\nsub_set_1.iloc[:8]\n\n\n\n\n\n  \n    \n      \n      CRASH DATE\n      CRASH TIME\n      BOROUGH\n      ZIP CODE\n      LATITUDE\n      LONGITUDE\n      LOCATION\n      ON STREET NAME\n    \n  \n  \n    \n      0\n      01/01/2023\n      14:38\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      BROOKLYN QUEENS EXPRESSWAY RAMP\n    \n    \n      1\n      01/01/2023\n      8:04\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NASSAU EXPRESSWAY\n    \n    \n      2\n      01/01/2023\n      18:05\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      10 AVENUE\n    \n    \n      3\n      01/01/2023\n      23:45\n      NaN\n      NaN\n      40.769737\n      -73.912440\n      (40.769737, -73.91244)\n      ASTORIA BOULEVARD\n    \n    \n      4\n      01/01/2023\n      4:50\n      NaN\n      NaN\n      40.830555\n      -73.850720\n      (40.830555, -73.85072)\n      CASTLE HILL AVENUE\n    \n    \n      5\n      01/01/2023\n      0:35\n      BROOKLYN\n      11229.0\n      40.601310\n      -73.954720\n      (40.60131, -73.95472)\n      NaN\n    \n    \n      6\n      01/01/2023\n      5:45\n      QUEENS\n      11101.0\n      40.744667\n      -73.931694\n      (40.744667, -73.931694)\n      QUEENS BOULEVARD\n    \n    \n      7\n      01/01/2023\n      1:00\n      MANHATTAN\n      10026.0\n      40.805595\n      -73.958190\n      (40.805595, -73.95819)\n      WEST 116 STREET\n    \n  \n\n\n\n\n\nsub_set_1.iloc[:, 1:3].head()\n\n\n\n\n\n  \n    \n      \n      CRASH TIME\n      BOROUGH\n    \n  \n  \n    \n      0\n      14:38\n      NaN\n    \n    \n      1\n      8:04\n      NaN\n    \n    \n      2\n      18:05\n      NaN\n    \n    \n      3\n      23:45\n      NaN\n    \n    \n      4\n      4:50\n      NaN\n    \n  \n\n\n\n\ndf.iloc[row_integer(s), col_integer(s)] Selects row and columns from the DataFrame by integer positions.\n\nsub_set_1.iloc[0:5, :6] \n\n\n\n\n\n  \n    \n      \n      CRASH DATE\n      CRASH TIME\n      BOROUGH\n      ZIP CODE\n      LATITUDE\n      LONGITUDE\n    \n  \n  \n    \n      0\n      01/01/2023\n      14:38\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      01/01/2023\n      8:04\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      01/01/2023\n      18:05\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      01/01/2023\n      23:45\n      NaN\n      NaN\n      40.769737\n      -73.91244\n    \n    \n      4\n      01/01/2023\n      4:50\n      NaN\n      NaN\n      40.830555\n      -73.85072\n    \n  \n\n\n\n\n\n\n5.2.16 concat method\npd.concat([df1, df2], axis = 0) can be used to combine two dataframe either row-wise or column-wise depends on value of axis:\n\n0 (default, row-wise)\n1 (column-wise)\n\n\nsub_set_1\n\n\n\n\n\n  \n    \n      \n      CRASH DATE\n      CRASH TIME\n      BOROUGH\n      ZIP CODE\n      LATITUDE\n      LONGITUDE\n      LOCATION\n      ON STREET NAME\n    \n  \n  \n    \n      0\n      01/01/2023\n      14:38\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      BROOKLYN QUEENS EXPRESSWAY RAMP\n    \n    \n      1\n      01/01/2023\n      8:04\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NASSAU EXPRESSWAY\n    \n    \n      2\n      01/01/2023\n      18:05\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      10 AVENUE\n    \n    \n      3\n      01/01/2023\n      23:45\n      NaN\n      NaN\n      40.769737\n      -73.912440\n      (40.769737, -73.91244)\n      ASTORIA BOULEVARD\n    \n    \n      4\n      01/01/2023\n      4:50\n      NaN\n      NaN\n      40.830555\n      -73.850720\n      (40.830555, -73.85072)\n      CASTLE HILL AVENUE\n    \n    \n      5\n      01/01/2023\n      0:35\n      BROOKLYN\n      11229.0\n      40.601310\n      -73.954720\n      (40.60131, -73.95472)\n      NaN\n    \n    \n      6\n      01/01/2023\n      5:45\n      QUEENS\n      11101.0\n      40.744667\n      -73.931694\n      (40.744667, -73.931694)\n      QUEENS BOULEVARD\n    \n    \n      7\n      01/01/2023\n      1:00\n      MANHATTAN\n      10026.0\n      40.805595\n      -73.958190\n      (40.805595, -73.95819)\n      WEST 116 STREET\n    \n    \n      8\n      01/01/2023\n      19:53\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      EAST 222 STREET\n    \n    \n      9\n      01/01/2023\n      3:11\n      NaN\n      NaN\n      40.842110\n      -73.825570\n      (40.84211, -73.82557)\n      BRUCKNER EXPRESSWAY\n    \n    \n      10\n      01/01/2023\n      18:49\n      NaN\n      NaN\n      40.674923\n      -73.736940\n      (40.674923, -73.73694)\n      MERRICK BOULEVARD\n    \n    \n      11\n      01/01/2023\n      23:06\n      QUEENS\n      11691.0\n      40.598720\n      -73.766010\n      (40.59872, -73.76601)\n      BEACH 32 STREET\n    \n    \n      12\n      01/01/2023\n      1:45\n      BROOKLYN\n      11207.0\n      40.652767\n      -73.886300\n      (40.652767, -73.8863)\n      PENNSYLVANIA AVENUE\n    \n    \n      13\n      01/01/2023\n      17:55\n      QUEENS\n      11375.0\n      40.710320\n      -73.849980\n      (40.71032, -73.84998)\n      METROPOLITAN AVENUE\n    \n    \n      14\n      01/01/2023\n      17:35\n      QUEENS\n      11354.0\n      40.771587\n      -73.810070\n      (40.771587, -73.81007)\n      BAYSIDE AVENUE\n    \n    \n      15\n      01/01/2023\n      3:30\n      BROOKLYN\n      11236.0\n      40.636720\n      -73.887695\n      (40.63672, -73.887695)\n      NaN\n    \n    \n      16\n      01/01/2023\n      10:30\n      QUEENS\n      11355.0\n      40.751800\n      -73.817314\n      (40.7518, -73.817314)\n      HOLLY AVENUE\n    \n    \n      17\n      01/01/2023\n      6:40\n      STATEN ISLAND\n      10305.0\n      40.585240\n      -74.093414\n      (40.58524, -74.093414)\n      HYLAN BOULEVARD\n    \n    \n      18\n      01/01/2023\n      5:30\n      NaN\n      NaN\n      40.704810\n      -73.939320\n      (40.70481, -73.93932)\n      SEIGEL STREET\n    \n    \n      19\n      01/01/2023\n      1:10\n      BROOKLYN\n      11223.0\n      40.593760\n      -73.982740\n      (40.59376, -73.98274)\n      AVENUE V\n    \n    \n      20\n      01/01/2023\n      0:04\n      NaN\n      NaN\n      40.858090\n      -73.901924\n      (40.85809, -73.901924)\n      EAST 183 STREET\n    \n    \n      21\n      01/01/2023\n      20:25\n      QUEENS\n      11357.0\n      40.786440\n      -73.829155\n      (40.78644, -73.829155)\n      140 STREET\n    \n    \n      22\n      01/01/2023\n      4:20\n      BROOKLYN\n      11233.0\n      40.670116\n      -73.922480\n      (40.670116, -73.92248)\n      SAINT JOHNS PLACE\n    \n    \n      23\n      01/01/2023\n      6:45\n      QUEENS\n      11368.0\n      40.739280\n      -73.850530\n      (40.73928, -73.85053)\n      SAULTELL AVENUE\n    \n    \n      24\n      01/01/2023\n      23:15\n      QUEENS\n      11369.0\n      40.757430\n      -73.876230\n      (40.75743, -73.87623)\n      NaN\n    \n    \n      25\n      01/01/2023\n      3:35\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      GRAND CENTRAL PARKWAY\n    \n    \n      26\n      01/01/2023\n      4:00\n      QUEENS\n      11372.0\n      40.751003\n      -73.892130\n      (40.751003, -73.89213)\n      74 STREET\n    \n    \n      27\n      01/01/2023\n      5:36\n      BROOKLYN\n      11207.0\n      40.663136\n      -73.883250\n      (40.663136, -73.88325)\n      NaN\n    \n    \n      28\n      01/01/2023\n      7:40\n      BRONX\n      10468.0\n      40.861810\n      -73.912320\n      (40.86181, -73.91232)\n      NaN\n    \n    \n      29\n      01/01/2023\n      10:50\n      NaN\n      NaN\n      40.832700\n      -73.950226\n      (40.8327, -73.950226)\n      HENRY HUDSON PARKWAY\n    \n    \n      30\n      01/01/2023\n      17:05\n      NaN\n      NaN\n      40.795250\n      -73.973210\n      (40.79525, -73.97321)\n      WEST 96 STREET\n    \n    \n      31\n      01/01/2023\n      5:45\n      QUEENS\n      11411.0\n      40.690640\n      -73.728320\n      (40.69064, -73.72832)\n      NaN\n    \n    \n      32\n      01/01/2023\n      0:55\n      NaN\n      NaN\n      40.818470\n      -73.941400\n      (40.81847, -73.9414)\n      WEST 140 STREET\n    \n    \n      33\n      01/01/2023\n      10:16\n      QUEENS\n      11369.0\n      40.764633\n      -73.865370\n      (40.764633, -73.86537)\n      NaN\n    \n    \n      34\n      01/01/2023\n      3:45\n      QUEENS\n      11694.0\n      40.580510\n      -73.847730\n      (40.58051, -73.84773)\n      NaN\n    \n  \n\n\n\n\n\nsub_set_2 = crashes.iloc[35:60, 0:8]\nsub_set_2\n\n\n\n\n\n  \n    \n      \n      CRASH DATE\n      CRASH TIME\n      BOROUGH\n      ZIP CODE\n      LATITUDE\n      LONGITUDE\n      LOCATION\n      ON STREET NAME\n    \n  \n  \n    \n      35\n      01/01/2023\n      1:20\n      QUEENS\n      11377.0\n      40.748720\n      -73.896250\n      (40.74872, -73.89625)\n      BROOKLYN QUEENS EXPRESSWAY\n    \n    \n      36\n      01/01/2023\n      18:26\n      BROOKLYN\n      11208.0\n      40.678246\n      -73.870186\n      (40.678246, -73.870186)\n      NaN\n    \n    \n      37\n      01/01/2023\n      0:00\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      WILLIAMSBURG BRIDGE OUTER ROADWA\n    \n    \n      38\n      01/01/2023\n      22:35\n      NaN\n      NaN\n      40.755780\n      -74.001990\n      (40.75578, -74.00199)\n      WEST 34 STREET\n    \n    \n      39\n      01/01/2023\n      12:00\n      BROOKLYN\n      11211.0\n      40.706690\n      -73.958840\n      (40.70669, -73.95884)\n      NaN\n    \n    \n      40\n      01/01/2023\n      8:45\n      NaN\n      NaN\n      40.828114\n      -73.931070\n      (40.828114, -73.93107)\n      MAJOR DEEGAN EXPRESSWAY\n    \n    \n      41\n      01/01/2023\n      1:20\n      MANHATTAN\n      10022.0\n      40.758980\n      -73.962440\n      (40.75898, -73.96244)\n      1 AVENUE\n    \n    \n      42\n      01/01/2023\n      16:45\n      QUEENS\n      11420.0\n      40.679070\n      -73.831540\n      (40.67907, -73.83154)\n      NaN\n    \n    \n      43\n      01/01/2023\n      0:10\n      BROOKLYN\n      11236.0\n      40.638350\n      -73.908890\n      (40.63835, -73.90889)\n      FLATLANDS AVENUE\n    \n    \n      44\n      01/01/2023\n      14:53\n      BROOKLYN\n      11208.0\n      40.660797\n      -73.871830\n      (40.660797, -73.87183)\n      ATKINS AVENUE\n    \n    \n      45\n      01/01/2023\n      13:10\n      NaN\n      NaN\n      40.831290\n      -73.929040\n      (40.83129, -73.92904)\n      WOODYCREST AVENUE\n    \n    \n      46\n      01/01/2023\n      14:16\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      VERRAZANO BRIDGE UPPER\n    \n    \n      47\n      01/01/2023\n      15:12\n      NaN\n      NaN\n      40.712780\n      -74.011690\n      (40.71278, -74.01169)\n      GREENWICH STREET\n    \n    \n      48\n      01/01/2023\n      0:00\n      NaN\n      NaN\n      40.688370\n      -73.944916\n      (40.68837, -73.944916)\n      LEXINGTON AVENUE\n    \n    \n      49\n      01/01/2023\n      3:35\n      NaN\n      NaN\n      40.825935\n      -73.859130\n      (40.825935, -73.85913)\n      BRUCKNER EXPRESSWAY\n    \n    \n      50\n      01/01/2023\n      9:45\n      NaN\n      NaN\n      40.882645\n      -73.886566\n      (40.882645, -73.886566)\n      SEDGWICK AVENUE\n    \n    \n      51\n      01/01/2023\n      12:20\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      102 CROSS DRIVE\n    \n    \n      52\n      01/01/2023\n      12:00\n      QUEENS\n      11354.0\n      40.764650\n      -73.823494\n      (40.76465, -73.823494)\n      NORTHERN BOULEVARD\n    \n    \n      53\n      01/01/2023\n      1:17\n      QUEENS\n      11375.0\n      40.724308\n      -73.842575\n      (40.724308, -73.842575)\n      110 STREET\n    \n    \n      54\n      01/01/2023\n      7:15\n      MANHATTAN\n      10025.0\n      40.795250\n      -73.973210\n      (40.79525, -73.97321)\n      WEST END AVENUE\n    \n    \n      55\n      01/01/2023\n      1:30\n      BRONX\n      10457.0\n      40.836480\n      -73.897736\n      (40.83648, -73.897736)\n      CLAREMONT PARKWAY\n    \n    \n      56\n      01/01/2023\n      8:08\n      NaN\n      NaN\n      40.686085\n      -73.982666\n      (40.686085, -73.982666)\n      ATLANTIC AVENUE\n    \n    \n      57\n      01/01/2023\n      6:08\n      QUEENS\n      11420.0\n      40.677242\n      -73.816720\n      (40.677242, -73.81672)\n      NaN\n    \n    \n      58\n      01/01/2023\n      8:20\n      BRONX\n      10451.0\n      NaN\n      NaN\n      NaN\n      EAST 138 STREET\n    \n    \n      59\n      01/01/2023\n      10:11\n      QUEENS\n      11358.0\n      40.760440\n      -73.804924\n      (40.76044, -73.804924)\n      161 STREET\n    \n  \n\n\n\n\ncombining by rows\n\nsub_set_3 = pd.concat([sub_set_1, sub_set_2])\nsub_set_3\n\n\n\n\n\n  \n    \n      \n      CRASH DATE\n      CRASH TIME\n      BOROUGH\n      ZIP CODE\n      LATITUDE\n      LONGITUDE\n      LOCATION\n      ON STREET NAME\n    \n  \n  \n    \n      0\n      01/01/2023\n      14:38\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      BROOKLYN QUEENS EXPRESSWAY RAMP\n    \n    \n      1\n      01/01/2023\n      8:04\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NASSAU EXPRESSWAY\n    \n    \n      2\n      01/01/2023\n      18:05\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      10 AVENUE\n    \n    \n      3\n      01/01/2023\n      23:45\n      NaN\n      NaN\n      40.769737\n      -73.912440\n      (40.769737, -73.91244)\n      ASTORIA BOULEVARD\n    \n    \n      4\n      01/01/2023\n      4:50\n      NaN\n      NaN\n      40.830555\n      -73.850720\n      (40.830555, -73.85072)\n      CASTLE HILL AVENUE\n    \n    \n      5\n      01/01/2023\n      0:35\n      BROOKLYN\n      11229.0\n      40.601310\n      -73.954720\n      (40.60131, -73.95472)\n      NaN\n    \n    \n      6\n      01/01/2023\n      5:45\n      QUEENS\n      11101.0\n      40.744667\n      -73.931694\n      (40.744667, -73.931694)\n      QUEENS BOULEVARD\n    \n    \n      7\n      01/01/2023\n      1:00\n      MANHATTAN\n      10026.0\n      40.805595\n      -73.958190\n      (40.805595, -73.95819)\n      WEST 116 STREET\n    \n    \n      8\n      01/01/2023\n      19:53\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      EAST 222 STREET\n    \n    \n      9\n      01/01/2023\n      3:11\n      NaN\n      NaN\n      40.842110\n      -73.825570\n      (40.84211, -73.82557)\n      BRUCKNER EXPRESSWAY\n    \n    \n      10\n      01/01/2023\n      18:49\n      NaN\n      NaN\n      40.674923\n      -73.736940\n      (40.674923, -73.73694)\n      MERRICK BOULEVARD\n    \n    \n      11\n      01/01/2023\n      23:06\n      QUEENS\n      11691.0\n      40.598720\n      -73.766010\n      (40.59872, -73.76601)\n      BEACH 32 STREET\n    \n    \n      12\n      01/01/2023\n      1:45\n      BROOKLYN\n      11207.0\n      40.652767\n      -73.886300\n      (40.652767, -73.8863)\n      PENNSYLVANIA AVENUE\n    \n    \n      13\n      01/01/2023\n      17:55\n      QUEENS\n      11375.0\n      40.710320\n      -73.849980\n      (40.71032, -73.84998)\n      METROPOLITAN AVENUE\n    \n    \n      14\n      01/01/2023\n      17:35\n      QUEENS\n      11354.0\n      40.771587\n      -73.810070\n      (40.771587, -73.81007)\n      BAYSIDE AVENUE\n    \n    \n      15\n      01/01/2023\n      3:30\n      BROOKLYN\n      11236.0\n      40.636720\n      -73.887695\n      (40.63672, -73.887695)\n      NaN\n    \n    \n      16\n      01/01/2023\n      10:30\n      QUEENS\n      11355.0\n      40.751800\n      -73.817314\n      (40.7518, -73.817314)\n      HOLLY AVENUE\n    \n    \n      17\n      01/01/2023\n      6:40\n      STATEN ISLAND\n      10305.0\n      40.585240\n      -74.093414\n      (40.58524, -74.093414)\n      HYLAN BOULEVARD\n    \n    \n      18\n      01/01/2023\n      5:30\n      NaN\n      NaN\n      40.704810\n      -73.939320\n      (40.70481, -73.93932)\n      SEIGEL STREET\n    \n    \n      19\n      01/01/2023\n      1:10\n      BROOKLYN\n      11223.0\n      40.593760\n      -73.982740\n      (40.59376, -73.98274)\n      AVENUE V\n    \n    \n      20\n      01/01/2023\n      0:04\n      NaN\n      NaN\n      40.858090\n      -73.901924\n      (40.85809, -73.901924)\n      EAST 183 STREET\n    \n    \n      21\n      01/01/2023\n      20:25\n      QUEENS\n      11357.0\n      40.786440\n      -73.829155\n      (40.78644, -73.829155)\n      140 STREET\n    \n    \n      22\n      01/01/2023\n      4:20\n      BROOKLYN\n      11233.0\n      40.670116\n      -73.922480\n      (40.670116, -73.92248)\n      SAINT JOHNS PLACE\n    \n    \n      23\n      01/01/2023\n      6:45\n      QUEENS\n      11368.0\n      40.739280\n      -73.850530\n      (40.73928, -73.85053)\n      SAULTELL AVENUE\n    \n    \n      24\n      01/01/2023\n      23:15\n      QUEENS\n      11369.0\n      40.757430\n      -73.876230\n      (40.75743, -73.87623)\n      NaN\n    \n    \n      25\n      01/01/2023\n      3:35\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      GRAND CENTRAL PARKWAY\n    \n    \n      26\n      01/01/2023\n      4:00\n      QUEENS\n      11372.0\n      40.751003\n      -73.892130\n      (40.751003, -73.89213)\n      74 STREET\n    \n    \n      27\n      01/01/2023\n      5:36\n      BROOKLYN\n      11207.0\n      40.663136\n      -73.883250\n      (40.663136, -73.88325)\n      NaN\n    \n    \n      28\n      01/01/2023\n      7:40\n      BRONX\n      10468.0\n      40.861810\n      -73.912320\n      (40.86181, -73.91232)\n      NaN\n    \n    \n      29\n      01/01/2023\n      10:50\n      NaN\n      NaN\n      40.832700\n      -73.950226\n      (40.8327, -73.950226)\n      HENRY HUDSON PARKWAY\n    \n    \n      30\n      01/01/2023\n      17:05\n      NaN\n      NaN\n      40.795250\n      -73.973210\n      (40.79525, -73.97321)\n      WEST 96 STREET\n    \n    \n      31\n      01/01/2023\n      5:45\n      QUEENS\n      11411.0\n      40.690640\n      -73.728320\n      (40.69064, -73.72832)\n      NaN\n    \n    \n      32\n      01/01/2023\n      0:55\n      NaN\n      NaN\n      40.818470\n      -73.941400\n      (40.81847, -73.9414)\n      WEST 140 STREET\n    \n    \n      33\n      01/01/2023\n      10:16\n      QUEENS\n      11369.0\n      40.764633\n      -73.865370\n      (40.764633, -73.86537)\n      NaN\n    \n    \n      34\n      01/01/2023\n      3:45\n      QUEENS\n      11694.0\n      40.580510\n      -73.847730\n      (40.58051, -73.84773)\n      NaN\n    \n    \n      35\n      01/01/2023\n      1:20\n      QUEENS\n      11377.0\n      40.748720\n      -73.896250\n      (40.74872, -73.89625)\n      BROOKLYN QUEENS EXPRESSWAY\n    \n    \n      36\n      01/01/2023\n      18:26\n      BROOKLYN\n      11208.0\n      40.678246\n      -73.870186\n      (40.678246, -73.870186)\n      NaN\n    \n    \n      37\n      01/01/2023\n      0:00\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      WILLIAMSBURG BRIDGE OUTER ROADWA\n    \n    \n      38\n      01/01/2023\n      22:35\n      NaN\n      NaN\n      40.755780\n      -74.001990\n      (40.75578, -74.00199)\n      WEST 34 STREET\n    \n    \n      39\n      01/01/2023\n      12:00\n      BROOKLYN\n      11211.0\n      40.706690\n      -73.958840\n      (40.70669, -73.95884)\n      NaN\n    \n    \n      40\n      01/01/2023\n      8:45\n      NaN\n      NaN\n      40.828114\n      -73.931070\n      (40.828114, -73.93107)\n      MAJOR DEEGAN EXPRESSWAY\n    \n    \n      41\n      01/01/2023\n      1:20\n      MANHATTAN\n      10022.0\n      40.758980\n      -73.962440\n      (40.75898, -73.96244)\n      1 AVENUE\n    \n    \n      42\n      01/01/2023\n      16:45\n      QUEENS\n      11420.0\n      40.679070\n      -73.831540\n      (40.67907, -73.83154)\n      NaN\n    \n    \n      43\n      01/01/2023\n      0:10\n      BROOKLYN\n      11236.0\n      40.638350\n      -73.908890\n      (40.63835, -73.90889)\n      FLATLANDS AVENUE\n    \n    \n      44\n      01/01/2023\n      14:53\n      BROOKLYN\n      11208.0\n      40.660797\n      -73.871830\n      (40.660797, -73.87183)\n      ATKINS AVENUE\n    \n    \n      45\n      01/01/2023\n      13:10\n      NaN\n      NaN\n      40.831290\n      -73.929040\n      (40.83129, -73.92904)\n      WOODYCREST AVENUE\n    \n    \n      46\n      01/01/2023\n      14:16\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      VERRAZANO BRIDGE UPPER\n    \n    \n      47\n      01/01/2023\n      15:12\n      NaN\n      NaN\n      40.712780\n      -74.011690\n      (40.71278, -74.01169)\n      GREENWICH STREET\n    \n    \n      48\n      01/01/2023\n      0:00\n      NaN\n      NaN\n      40.688370\n      -73.944916\n      (40.68837, -73.944916)\n      LEXINGTON AVENUE\n    \n    \n      49\n      01/01/2023\n      3:35\n      NaN\n      NaN\n      40.825935\n      -73.859130\n      (40.825935, -73.85913)\n      BRUCKNER EXPRESSWAY\n    \n    \n      50\n      01/01/2023\n      9:45\n      NaN\n      NaN\n      40.882645\n      -73.886566\n      (40.882645, -73.886566)\n      SEDGWICK AVENUE\n    \n    \n      51\n      01/01/2023\n      12:20\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      102 CROSS DRIVE\n    \n    \n      52\n      01/01/2023\n      12:00\n      QUEENS\n      11354.0\n      40.764650\n      -73.823494\n      (40.76465, -73.823494)\n      NORTHERN BOULEVARD\n    \n    \n      53\n      01/01/2023\n      1:17\n      QUEENS\n      11375.0\n      40.724308\n      -73.842575\n      (40.724308, -73.842575)\n      110 STREET\n    \n    \n      54\n      01/01/2023\n      7:15\n      MANHATTAN\n      10025.0\n      40.795250\n      -73.973210\n      (40.79525, -73.97321)\n      WEST END AVENUE\n    \n    \n      55\n      01/01/2023\n      1:30\n      BRONX\n      10457.0\n      40.836480\n      -73.897736\n      (40.83648, -73.897736)\n      CLAREMONT PARKWAY\n    \n    \n      56\n      01/01/2023\n      8:08\n      NaN\n      NaN\n      40.686085\n      -73.982666\n      (40.686085, -73.982666)\n      ATLANTIC AVENUE\n    \n    \n      57\n      01/01/2023\n      6:08\n      QUEENS\n      11420.0\n      40.677242\n      -73.816720\n      (40.677242, -73.81672)\n      NaN\n    \n    \n      58\n      01/01/2023\n      8:20\n      BRONX\n      10451.0\n      NaN\n      NaN\n      NaN\n      EAST 138 STREET\n    \n    \n      59\n      01/01/2023\n      10:11\n      QUEENS\n      11358.0\n      40.760440\n      -73.804924\n      (40.76044, -73.804924)\n      161 STREET\n    \n  \n\n\n\n\ncombining by columns\n\nsub_set_4 = pd.concat([sub_set_1, sub_set_2], axis = 1)\nsub_set_4\n\n\n\n\n\n  \n    \n      \n      CRASH DATE\n      CRASH TIME\n      BOROUGH\n      ZIP CODE\n      LATITUDE\n      LONGITUDE\n      LOCATION\n      ON STREET NAME\n      CRASH DATE\n      CRASH TIME\n      BOROUGH\n      ZIP CODE\n      LATITUDE\n      LONGITUDE\n      LOCATION\n      ON STREET NAME\n    \n  \n  \n    \n      0\n      01/01/2023\n      14:38\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      BROOKLYN QUEENS EXPRESSWAY RAMP\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      01/01/2023\n      8:04\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NASSAU EXPRESSWAY\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      01/01/2023\n      18:05\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      10 AVENUE\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      01/01/2023\n      23:45\n      NaN\n      NaN\n      40.769737\n      -73.912440\n      (40.769737, -73.91244)\n      ASTORIA BOULEVARD\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      01/01/2023\n      4:50\n      NaN\n      NaN\n      40.830555\n      -73.850720\n      (40.830555, -73.85072)\n      CASTLE HILL AVENUE\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      5\n      01/01/2023\n      0:35\n      BROOKLYN\n      11229.0\n      40.601310\n      -73.954720\n      (40.60131, -73.95472)\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      6\n      01/01/2023\n      5:45\n      QUEENS\n      11101.0\n      40.744667\n      -73.931694\n      (40.744667, -73.931694)\n      QUEENS BOULEVARD\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      7\n      01/01/2023\n      1:00\n      MANHATTAN\n      10026.0\n      40.805595\n      -73.958190\n      (40.805595, -73.95819)\n      WEST 116 STREET\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      8\n      01/01/2023\n      19:53\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      EAST 222 STREET\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      9\n      01/01/2023\n      3:11\n      NaN\n      NaN\n      40.842110\n      -73.825570\n      (40.84211, -73.82557)\n      BRUCKNER EXPRESSWAY\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      10\n      01/01/2023\n      18:49\n      NaN\n      NaN\n      40.674923\n      -73.736940\n      (40.674923, -73.73694)\n      MERRICK BOULEVARD\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      11\n      01/01/2023\n      23:06\n      QUEENS\n      11691.0\n      40.598720\n      -73.766010\n      (40.59872, -73.76601)\n      BEACH 32 STREET\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      12\n      01/01/2023\n      1:45\n      BROOKLYN\n      11207.0\n      40.652767\n      -73.886300\n      (40.652767, -73.8863)\n      PENNSYLVANIA AVENUE\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      13\n      01/01/2023\n      17:55\n      QUEENS\n      11375.0\n      40.710320\n      -73.849980\n      (40.71032, -73.84998)\n      METROPOLITAN AVENUE\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      14\n      01/01/2023\n      17:35\n      QUEENS\n      11354.0\n      40.771587\n      -73.810070\n      (40.771587, -73.81007)\n      BAYSIDE AVENUE\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      15\n      01/01/2023\n      3:30\n      BROOKLYN\n      11236.0\n      40.636720\n      -73.887695\n      (40.63672, -73.887695)\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      16\n      01/01/2023\n      10:30\n      QUEENS\n      11355.0\n      40.751800\n      -73.817314\n      (40.7518, -73.817314)\n      HOLLY AVENUE\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      17\n      01/01/2023\n      6:40\n      STATEN ISLAND\n      10305.0\n      40.585240\n      -74.093414\n      (40.58524, -74.093414)\n      HYLAN BOULEVARD\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      18\n      01/01/2023\n      5:30\n      NaN\n      NaN\n      40.704810\n      -73.939320\n      (40.70481, -73.93932)\n      SEIGEL STREET\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      19\n      01/01/2023\n      1:10\n      BROOKLYN\n      11223.0\n      40.593760\n      -73.982740\n      (40.59376, -73.98274)\n      AVENUE V\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      20\n      01/01/2023\n      0:04\n      NaN\n      NaN\n      40.858090\n      -73.901924\n      (40.85809, -73.901924)\n      EAST 183 STREET\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      21\n      01/01/2023\n      20:25\n      QUEENS\n      11357.0\n      40.786440\n      -73.829155\n      (40.78644, -73.829155)\n      140 STREET\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      22\n      01/01/2023\n      4:20\n      BROOKLYN\n      11233.0\n      40.670116\n      -73.922480\n      (40.670116, -73.92248)\n      SAINT JOHNS PLACE\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      23\n      01/01/2023\n      6:45\n      QUEENS\n      11368.0\n      40.739280\n      -73.850530\n      (40.73928, -73.85053)\n      SAULTELL AVENUE\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      24\n      01/01/2023\n      23:15\n      QUEENS\n      11369.0\n      40.757430\n      -73.876230\n      (40.75743, -73.87623)\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      25\n      01/01/2023\n      3:35\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      GRAND CENTRAL PARKWAY\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      26\n      01/01/2023\n      4:00\n      QUEENS\n      11372.0\n      40.751003\n      -73.892130\n      (40.751003, -73.89213)\n      74 STREET\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      27\n      01/01/2023\n      5:36\n      BROOKLYN\n      11207.0\n      40.663136\n      -73.883250\n      (40.663136, -73.88325)\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      28\n      01/01/2023\n      7:40\n      BRONX\n      10468.0\n      40.861810\n      -73.912320\n      (40.86181, -73.91232)\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      29\n      01/01/2023\n      10:50\n      NaN\n      NaN\n      40.832700\n      -73.950226\n      (40.8327, -73.950226)\n      HENRY HUDSON PARKWAY\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      30\n      01/01/2023\n      17:05\n      NaN\n      NaN\n      40.795250\n      -73.973210\n      (40.79525, -73.97321)\n      WEST 96 STREET\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      31\n      01/01/2023\n      5:45\n      QUEENS\n      11411.0\n      40.690640\n      -73.728320\n      (40.69064, -73.72832)\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      32\n      01/01/2023\n      0:55\n      NaN\n      NaN\n      40.818470\n      -73.941400\n      (40.81847, -73.9414)\n      WEST 140 STREET\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      33\n      01/01/2023\n      10:16\n      QUEENS\n      11369.0\n      40.764633\n      -73.865370\n      (40.764633, -73.86537)\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      34\n      01/01/2023\n      3:45\n      QUEENS\n      11694.0\n      40.580510\n      -73.847730\n      (40.58051, -73.84773)\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      35\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      01/01/2023\n      1:20\n      QUEENS\n      11377.0\n      40.748720\n      -73.896250\n      (40.74872, -73.89625)\n      BROOKLYN QUEENS EXPRESSWAY\n    \n    \n      36\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      01/01/2023\n      18:26\n      BROOKLYN\n      11208.0\n      40.678246\n      -73.870186\n      (40.678246, -73.870186)\n      NaN\n    \n    \n      37\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      01/01/2023\n      0:00\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      WILLIAMSBURG BRIDGE OUTER ROADWA\n    \n    \n      38\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      01/01/2023\n      22:35\n      NaN\n      NaN\n      40.755780\n      -74.001990\n      (40.75578, -74.00199)\n      WEST 34 STREET\n    \n    \n      39\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      01/01/2023\n      12:00\n      BROOKLYN\n      11211.0\n      40.706690\n      -73.958840\n      (40.70669, -73.95884)\n      NaN\n    \n    \n      40\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      01/01/2023\n      8:45\n      NaN\n      NaN\n      40.828114\n      -73.931070\n      (40.828114, -73.93107)\n      MAJOR DEEGAN EXPRESSWAY\n    \n    \n      41\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      01/01/2023\n      1:20\n      MANHATTAN\n      10022.0\n      40.758980\n      -73.962440\n      (40.75898, -73.96244)\n      1 AVENUE\n    \n    \n      42\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      01/01/2023\n      16:45\n      QUEENS\n      11420.0\n      40.679070\n      -73.831540\n      (40.67907, -73.83154)\n      NaN\n    \n    \n      43\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      01/01/2023\n      0:10\n      BROOKLYN\n      11236.0\n      40.638350\n      -73.908890\n      (40.63835, -73.90889)\n      FLATLANDS AVENUE\n    \n    \n      44\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      01/01/2023\n      14:53\n      BROOKLYN\n      11208.0\n      40.660797\n      -73.871830\n      (40.660797, -73.87183)\n      ATKINS AVENUE\n    \n    \n      45\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      01/01/2023\n      13:10\n      NaN\n      NaN\n      40.831290\n      -73.929040\n      (40.83129, -73.92904)\n      WOODYCREST AVENUE\n    \n    \n      46\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      01/01/2023\n      14:16\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      VERRAZANO BRIDGE UPPER\n    \n    \n      47\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      01/01/2023\n      15:12\n      NaN\n      NaN\n      40.712780\n      -74.011690\n      (40.71278, -74.01169)\n      GREENWICH STREET\n    \n    \n      48\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      01/01/2023\n      0:00\n      NaN\n      NaN\n      40.688370\n      -73.944916\n      (40.68837, -73.944916)\n      LEXINGTON AVENUE\n    \n    \n      49\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      01/01/2023\n      3:35\n      NaN\n      NaN\n      40.825935\n      -73.859130\n      (40.825935, -73.85913)\n      BRUCKNER EXPRESSWAY\n    \n    \n      50\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      01/01/2023\n      9:45\n      NaN\n      NaN\n      40.882645\n      -73.886566\n      (40.882645, -73.886566)\n      SEDGWICK AVENUE\n    \n    \n      51\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      01/01/2023\n      12:20\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      102 CROSS DRIVE\n    \n    \n      52\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      01/01/2023\n      12:00\n      QUEENS\n      11354.0\n      40.764650\n      -73.823494\n      (40.76465, -73.823494)\n      NORTHERN BOULEVARD\n    \n    \n      53\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      01/01/2023\n      1:17\n      QUEENS\n      11375.0\n      40.724308\n      -73.842575\n      (40.724308, -73.842575)\n      110 STREET\n    \n    \n      54\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      01/01/2023\n      7:15\n      MANHATTAN\n      10025.0\n      40.795250\n      -73.973210\n      (40.79525, -73.97321)\n      WEST END AVENUE\n    \n    \n      55\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      01/01/2023\n      1:30\n      BRONX\n      10457.0\n      40.836480\n      -73.897736\n      (40.83648, -73.897736)\n      CLAREMONT PARKWAY\n    \n    \n      56\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      01/01/2023\n      8:08\n      NaN\n      NaN\n      40.686085\n      -73.982666\n      (40.686085, -73.982666)\n      ATLANTIC AVENUE\n    \n    \n      57\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      01/01/2023\n      6:08\n      QUEENS\n      11420.0\n      40.677242\n      -73.816720\n      (40.677242, -73.81672)\n      NaN\n    \n    \n      58\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      01/01/2023\n      8:20\n      BRONX\n      10451.0\n      NaN\n      NaN\n      NaN\n      EAST 138 STREET\n    \n    \n      59\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      01/01/2023\n      10:11\n      QUEENS\n      11358.0\n      40.760440\n      -73.804924\n      (40.76044, -73.804924)\n      161 STREET\n    \n  \n\n\n\n\nuse .fillna() to fill in the missing values"
  },
  {
    "objectID": "pandas.html#merge-and-join",
    "href": "pandas.html#merge-and-join",
    "title": "5  Data Manipulation with Pandas",
    "section": "5.3 Merge and Join",
    "text": "5.3 Merge and Join\n\npd.merge( ): allows a user to do one-to-one, one-to-many, and many-to-many joins\n\nOne-to-one joins mean each row is related on a single row in a different table using a key column\nOne-to-many joins mean each row in one table in related to one or more rows in a different table using a key column\nMany-to-many joins mean one or more rows in one table is related to one or more rows in a seperate table using a key column\n\n\n\n5.3.1 Ex: One-to One Join\n\nCreate 2 new Data Frames from the January 2023 data with a common column (Zip Code)\n\nFor this example, we are using .drop_duplicates() to get unique ZIP CODE values\n\nUsing pd.merge( ) the two data frames are combined using that common column as a key\n\n\njan23 = pd.read_csv(\"data/nyc_crashes_202301.csv\") \ncrash_zip = jan23[[\"CRASH DATE\", \"ZIP CODE\"]].copy().drop_duplicates(subset = [ \"ZIP CODE\"])\ncrash_zip.tail() # to view DF of crash date & zip code\n\n\n\n\n\n  \n    \n      \n      CRASH DATE\n      ZIP CODE\n    \n  \n  \n    \n      3590\n      01/16/2023\n      10023.0\n    \n    \n      4304\n      01/19/2023\n      11363.0\n    \n    \n      5530\n      01/24/2023\n      11109.0\n    \n    \n      5732\n      01/25/2023\n      10280.0\n    \n    \n      7140\n      01/31/2023\n      10169.0\n    \n  \n\n\n\n\n\nborough_zip = jan23[[\"ZIP CODE\", \"BOROUGH\"]].copy().drop_duplicates(subset = [ \"ZIP CODE\"])\nborough_zip.tail() # to view DF of zip code and borough\n\n\n\n\n\n  \n    \n      \n      ZIP CODE\n      BOROUGH\n    \n  \n  \n    \n      3590\n      10023.0\n      MANHATTAN\n    \n    \n      4304\n      11363.0\n      QUEENS\n    \n    \n      5530\n      11109.0\n      QUEENS\n    \n    \n      5732\n      10280.0\n      MANHATTAN\n    \n    \n      7140\n      10169.0\n      MANHATTAN\n    \n  \n\n\n\n\n\nmerge_w_zip = pd.merge(crash_zip, borough_zip)\nmerge_w_zip.tail() # to view joined data frames\n\n\n\n\n\n  \n    \n      \n      CRASH DATE\n      ZIP CODE\n      BOROUGH\n    \n  \n  \n    \n      176\n      01/16/2023\n      10023.0\n      MANHATTAN\n    \n    \n      177\n      01/19/2023\n      11363.0\n      QUEENS\n    \n    \n      178\n      01/24/2023\n      11109.0\n      QUEENS\n    \n    \n      179\n      01/25/2023\n      10280.0\n      MANHATTAN\n    \n    \n      180\n      01/31/2023\n      10169.0\n      MANHATTAN\n    \n  \n\n\n\n\nWe can specify the name of the key column using on:\n\npd.merge(crash_zip, borough_zip, on = 'ZIP CODE').tail()\n\n\n\n\n\n  \n    \n      \n      CRASH DATE\n      ZIP CODE\n      BOROUGH\n    \n  \n  \n    \n      176\n      01/16/2023\n      10023.0\n      MANHATTAN\n    \n    \n      177\n      01/19/2023\n      11363.0\n      QUEENS\n    \n    \n      178\n      01/24/2023\n      11109.0\n      QUEENS\n    \n    \n      179\n      01/25/2023\n      10280.0\n      MANHATTAN\n    \n    \n      180\n      01/31/2023\n      10169.0\n      MANHATTAN\n    \n  \n\n\n\n\n\n\n5.3.2 Ex: One-to-Many Join\n\nCreate a new DataFrame consisting of Employee and Department\nCreate a new DataFrame consisting of Employee and Hire Year\n\n\nemp_dept = pd.DataFrame({'employee': ['Emily', 'Jake', 'Paul', 'Jackie'],\n                        'department': ['Accounting', 'HR', 'Engineering', 'Accounting']})\n\nemp_hire = pd.DataFrame({'employee': ['Emily', 'Jake', 'Paul', 'Jackie'],\n                        'hire_year': ['2020', '2019', '2023', '2011']})\n\n# combining to create a one to one join\ndept_hire = pd.merge(emp_dept,emp_hire) # to merge Employee, Department and Hire Year into one dataframe\ndept_hire\n\n\n\n\n\n  \n    \n      \n      employee\n      department\n      hire_year\n    \n  \n  \n    \n      0\n      Emily\n      Accounting\n      2020\n    \n    \n      1\n      Jake\n      HR\n      2019\n    \n    \n      2\n      Paul\n      Engineering\n      2023\n    \n    \n      3\n      Jackie\n      Accounting\n      2011\n    \n  \n\n\n\n\n\nCreate a new DataFrame that consists of Department and the Supervisor for that department\nMerge this new DataFrame with ‘dept_hire’ to create a Many-to-One join using Department as a key\n\n\ndept_sup = pd.DataFrame({'supervisor': ['Lily', 'Angela', 'Steven'],\n                        'department': ['Accounting', 'HR', 'Engineering']})\n\nsup_emp_dept_hire = pd.merge(dept_sup,dept_hire) # merge using Department as the key\nsup_emp_dept_hire\n\n\n\n\n\n  \n    \n      \n      supervisor\n      department\n      employee\n      hire_year\n    \n  \n  \n    \n      0\n      Lily\n      Accounting\n      Emily\n      2020\n    \n    \n      1\n      Lily\n      Accounting\n      Jackie\n      2011\n    \n    \n      2\n      Angela\n      HR\n      Jake\n      2019\n    \n    \n      3\n      Steven\n      Engineering\n      Paul\n      2023\n    \n  \n\n\n\n\n\n\n5.3.3 Ex: Merging when the Key has Different Variable Names\n\nWe will merge two dataframes that have a similar column containing the same information, but are named differently\nUsing the employee data from above, but changing employee in emp_hire to employee_name\nNeed to drop either employee or employee_name after merging to not have redundant information\n\n\nemp_dept_names = pd.DataFrame({'employee': ['Emily', 'Jake', 'Paul', 'Jackie'],\n                        'department': ['Accounting', 'HR', 'Engineering', 'Accounting']})\n\nemp_hire_names = pd.DataFrame({'employee_name': ['Emily', 'Jake', 'Paul', 'Jackie'],\n                        'hire_year': ['2020', '2019', '2023', '2011']})\n\n# to merge Employee, Department and Hire Year into one dataframe & drop column 'employee_name'\ndept_hire_names = pd.merge(emp_dept_names,emp_hire_names, left_on = 'employee', \n                           right_on = 'employee_name').drop('employee_name', axis = 1) \n\ndept_hire_names\n\n\n\n\n\n  \n    \n      \n      employee\n      department\n      hire_year\n    \n  \n  \n    \n      0\n      Emily\n      Accounting\n      2020\n    \n    \n      1\n      Jake\n      HR\n      2019\n    \n    \n      2\n      Paul\n      Engineering\n      2023\n    \n    \n      3\n      Jackie\n      Accounting\n      2011\n    \n  \n\n\n\n\n\n\n5.3.4 Ex: Joining ‘uszipcode’\n\nCreate a subset of jan23 data with 7 zipcodes\nUsing ‘uszipcode’ data to join the zip codes from jan23 with data provided in this package\n\n\nfrom uszipcode import SearchEngine\n\n\nsearch = SearchEngine()\n\n# create a DF of zip codes from jan23 & convert to integers\nzipcodes = pd.DataFrame(jan23[\"ZIP CODE\"].tail(15).dropna().reset_index(drop = True))\nzipcodes[\"ZIP CODE\"] = zipcodes[\"ZIP CODE\"].astype(int)\n\n# create new,empty column in the df to store the address information\nzipcodes['Address'] = None\n\n\n# using uszipcode library to retreive address info\nfor index, row in zipcodes.iterrows():\n    result = search.by_zipcode(row['ZIP CODE'])\n    zipcodes.at[index, 'Address'] = result.major_city + ', ' + result.state\n\nprint(zipcodes)\n\n    ZIP CODE       Address\n0      11228  Brooklyn, NY\n1      10027  New York, NY\n2      10040  New York, NY\n3      10035  New York, NY\n4      10035  New York, NY\n5      10457     Bronx, NY\n6      11203  Brooklyn, NY\n7      11208  Brooklyn, NY\n8      11230  Brooklyn, NY\n9      10033  New York, NY\n10     11435   Jamaica, NY"
  },
  {
    "objectID": "pandas.html#aggregation-and-grouping",
    "href": "pandas.html#aggregation-and-grouping",
    "title": "5  Data Manipulation with Pandas",
    "section": "5.4 Aggregation and Grouping",
    "text": "5.4 Aggregation and Grouping\nBuilt-In Pandas Aggregations (for DataFrame & Series objects): - count( ) - Total number of items - first( ), last( ) - First and last item - mean( ), median( )\n- Mean and median - min( ), max( )\n- Minimum and maximum - std( ), var( )\n- Standard deviation and variance - mad( )\n- Mean absolute deviation - prod( )\n- Product of all items - sum( )\n- Sum of all items - groupby() - compute aggregates on subsets of data\n\n5.4.1 Ex: Titanic Groupby\n\nWe will use the Titanic Data Set from the ‘seaborn’ library\n\n\nimport seaborn as sns\ntitanic = sns.load_dataset('titanic')\ntitanic.head()\n\n\n\n\n\n  \n    \n      \n      survived\n      pclass\n      sex\n      age\n      sibsp\n      parch\n      fare\n      embarked\n      class\n      who\n      adult_male\n      deck\n      embark_town\n      alive\n      alone\n    \n  \n  \n    \n      0\n      0\n      3\n      male\n      22.0\n      1\n      0\n      7.2500\n      S\n      Third\n      man\n      True\n      NaN\n      Southampton\n      no\n      False\n    \n    \n      1\n      1\n      1\n      female\n      38.0\n      1\n      0\n      71.2833\n      C\n      First\n      woman\n      False\n      C\n      Cherbourg\n      yes\n      False\n    \n    \n      2\n      1\n      3\n      female\n      26.0\n      0\n      0\n      7.9250\n      S\n      Third\n      woman\n      False\n      NaN\n      Southampton\n      yes\n      True\n    \n    \n      3\n      1\n      1\n      female\n      35.0\n      1\n      0\n      53.1000\n      S\n      First\n      woman\n      False\n      C\n      Southampton\n      yes\n      False\n    \n    \n      4\n      0\n      3\n      male\n      35.0\n      0\n      0\n      8.0500\n      S\n      Third\n      man\n      True\n      NaN\n      Southampton\n      no\n      True\n    \n  \n\n\n\n\n\nBelow the data is groupby ‘sex’ and the counts for each row are displayed\n\n\ntitanic.groupby('sex').count()\n\n\n\n\n\n  \n    \n      \n      survived\n      pclass\n      age\n      sibsp\n      parch\n      fare\n      embarked\n      class\n      who\n      adult_male\n      deck\n      embark_town\n      alive\n      alone\n    \n    \n      sex\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      female\n      314\n      314\n      261\n      314\n      314\n      314\n      312\n      314\n      314\n      314\n      97\n      312\n      314\n      314\n    \n    \n      male\n      577\n      577\n      453\n      577\n      577\n      577\n      577\n      577\n      577\n      577\n      106\n      577\n      577\n      577\n    \n  \n\n\n\n\n\n\n5.4.2 Ex: Crash Data Group By\n\nUsing the crash_zip DataFrame from above, grouping by ZIP CODE and using the count() method, we can see how many counts for each listed zip code\n\n\ncrash_zip.groupby('ZIP CODE').count()\n\n\n\n\n\n  \n    \n      \n      CRASH DATE\n    \n    \n      ZIP CODE\n      \n    \n  \n  \n    \n      10001.0\n      1\n    \n    \n      10002.0\n      1\n    \n    \n      10003.0\n      1\n    \n    \n      10004.0\n      1\n    \n    \n      10005.0\n      1\n    \n    \n      ...\n      ...\n    \n    \n      11436.0\n      1\n    \n    \n      11691.0\n      1\n    \n    \n      11692.0\n      1\n    \n    \n      11693.0\n      1\n    \n    \n      11694.0\n      1\n    \n  \n\n180 rows × 1 columns"
  },
  {
    "objectID": "pandas.html#pivot-tables",
    "href": "pandas.html#pivot-tables",
    "title": "5  Data Manipulation with Pandas",
    "section": "5.5 Pivot Tables",
    "text": "5.5 Pivot Tables\n\nCreates a two dimensional table using column data\nEasy way to visualize data to see patterns and summarize data\nUse ‘groupby( )’ to create a pivot table\n\n\n5.5.1 Ex: Pivot Table using Titanic Data Set\n\nWe will again use the Titanic data set, but now we will create a pivot table\n\nWe can use groupby() to help create a pivot table - Group the data by ‘sex’ and ‘class’ to select survival. - Then use the aggregate function mean() to show within the table\n\ntitanic.groupby(['sex', 'class'])['survived'].aggregate('mean').unstack()\n\n\n\n\n\n  \n    \n      class\n      First\n      Second\n      Third\n    \n    \n      sex\n      \n      \n      \n    \n  \n  \n    \n      female\n      0.968085\n      0.921053\n      0.500000\n    \n    \n      male\n      0.368852\n      0.157407\n      0.135447\n    \n  \n\n\n\n\n\n\n5.5.2 Ex: Pivot Table of Crash Data\n\nCreate a pivot table that shows the Number of Persons Injured for every Crash Date per Zip Code\n\n\njan23.pivot_table('NUMBER OF PERSONS INJURED', index = 'ZIP CODE', columns = 'CRASH DATE')\n\n\n\n\n\n  \n    \n      CRASH DATE\n      01/01/2023\n      01/02/2023\n      01/03/2023\n      01/04/2023\n      01/05/2023\n      01/06/2023\n      01/07/2023\n      01/08/2023\n      01/09/2023\n      01/10/2023\n      ...\n      01/22/2023\n      01/23/2023\n      01/24/2023\n      01/25/2023\n      01/26/2023\n      01/27/2023\n      01/28/2023\n      01/29/2023\n      01/30/2023\n      01/31/2023\n    \n    \n      ZIP CODE\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      10001.0\n      0.0\n      NaN\n      0.000000\n      NaN\n      0.0\n      NaN\n      NaN\n      0.0\n      0.0\n      NaN\n      ...\n      NaN\n      NaN\n      0.0\n      0.500000\n      0.0\n      0.0\n      1.00\n      1.0\n      0.500000\n      0.5\n    \n    \n      10002.0\n      0.0\n      0.5\n      0.666667\n      0.0\n      NaN\n      NaN\n      NaN\n      0.0\n      NaN\n      0.0\n      ...\n      0.5\n      NaN\n      0.0\n      1.333333\n      0.0\n      0.0\n      0.75\n      0.0\n      0.333333\n      1.0\n    \n    \n      10003.0\n      0.5\n      0.0\n      1.000000\n      1.0\n      NaN\n      NaN\n      0.5\n      0.0\n      0.0\n      NaN\n      ...\n      0.0\n      NaN\n      NaN\n      NaN\n      0.0\n      0.0\n      0.50\n      0.0\n      0.000000\n      1.0\n    \n    \n      10004.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      0.0\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      0.00\n      NaN\n      NaN\n      NaN\n    \n    \n      10005.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      1.0\n      NaN\n      NaN\n      NaN\n      0.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      11436.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      0.000000\n      NaN\n      NaN\n      0.00\n      NaN\n      0.000000\n      NaN\n    \n    \n      11691.0\n      0.0\n      0.5\n      1.000000\n      1.0\n      1.0\n      NaN\n      NaN\n      0.0\n      NaN\n      0.0\n      ...\n      0.5\n      1.0\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.50\n      NaN\n      0.000000\n      0.0\n    \n    \n      11692.0\n      NaN\n      NaN\n      1.000000\n      1.0\n      0.0\n      1.0\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      0.0\n      NaN\n      NaN\n      NaN\n      NaN\n      0.0\n    \n    \n      11693.0\n      1.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      0.0\n      0.000000\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      11694.0\n      0.0\n      NaN\n      NaN\n      0.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      2.0\n      1.0\n      NaN\n      1.000000\n      0.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n180 rows × 31 columns"
  },
  {
    "objectID": "pandas.html#vectorized-string-operations",
    "href": "pandas.html#vectorized-string-operations",
    "title": "5  Data Manipulation with Pandas",
    "section": "5.6 Vectorized String Operations",
    "text": "5.6 Vectorized String Operations\n\nMust use the ‘str’ attribute of a Pandas Series and Index objects to access operations\nSome examples of operations:\n\nlen()\nlower()\n\ntranslate()\n\nslower()\nljust()\n\nupper()\n\nstartswith()\n\nisupper()\nrjust()\n\nfind()\n\nendswith()\n\nisnumeric()\ncapitalize()\n\nswapcase()\n\nistitle()\nrpartition()\n\n\n\n5.6.1 Ex: Create a Panda Series of ‘BOROUGH’ names and swap cases.\n\nbname = pd.Series(jan23['BOROUGH'])\nbname.head(15).dropna() # original \n\n5      BROOKLYN\n6        QUEENS\n7     MANHATTAN\n11       QUEENS\n12     BROOKLYN\n13       QUEENS\n14       QUEENS\nName: BOROUGH, dtype: object\n\n\n\nbname.str.capitalize().head(15).dropna()# to make first letter capital\n\n5      Brooklyn\n6        Queens\n7     Manhattan\n11       Queens\n12     Brooklyn\n13       Queens\n14       Queens\nName: BOROUGH, dtype: object\n\n\n\nbname.str.swapcase().head(15).dropna() # to make all lower case\n\n5      brooklyn\n6        queens\n7     manhattan\n11       queens\n12     brooklyn\n13       queens\n14       queens\nName: BOROUGH, dtype: object\n\n\n\nbname.str.len().head(15).dropna() # to return the length of the name and data type\n\n5     8.0\n6     6.0\n7     9.0\n11    6.0\n12    8.0\n13    6.0\n14    6.0\nName: BOROUGH, dtype: float64\n\n\n\nbname.str.startswith('B').head(15).dropna() # to see if starts with a letter B\n\n5      True\n6     False\n7     False\n11    False\n12     True\n13    False\n14    False\nName: BOROUGH, dtype: object"
  },
  {
    "objectID": "pandas.html#time-series",
    "href": "pandas.html#time-series",
    "title": "5  Data Manipulation with Pandas",
    "section": "5.7 Time Series",
    "text": "5.7 Time Series\n\nTime Stamps : Moments in time\n\nEx: July 4th, 2023 at 8:00 AM\nPandas provides the Timestamp type\n\nTime Intervals: Reference a length of time with a beginning and end\n\nEx: The year of 2022\nPandas provides the Period type\n\nTime Deltas/ Durations: Reference an exact length of time\n\nEx: 0.3 seconds\nPandas provides the Timedelta type\n\n\n\n5.7.1 Can create a Timestamp object\n\ncombines ‘datetime’ and ‘dateutil’ to be used as a Series or DataFrame\n\n\ndate = pd.to_datetime(\"2nd of February, 2023\")\ndate\ntype(date)\n\npandas._libs.tslibs.timestamps.Timestamp\n\n\n\n\n5.7.2 Can create Series that has time indexed data\n\nind = pd.DatetimeIndex(['2022-07-04', '2022-08-04',\n                          '2022-07-04', '2022-08-04'])\ninddata = pd.Series([0,1,2,3], index = ind)\ninddata\n\n2022-07-04    0\n2022-08-04    1\n2022-07-04    2\n2022-08-04    3\ndtype: int64\n\n\n\n\n5.7.3 Frequencies and Offsets\nThe following are the main codes avaiable:\n- D Calendar day    \n- B Business day\n- W Weekly      \n- M Month end   \n- BM   Business month end\n- Q Quarter end \n- BQ   Business quarter end\n- A Year end    \n- BA   Business year end\n- H Hours   \n- BH   Business hours\n- T Minutes     \n- S Seconds     \n- L Milliseonds     \n- U Microseconds        \n- N nanoseconds \n\n\n5.7.4 Ex: TimeDelta\n\ncreate a TimeDelta data type starting at 00:00:00 using frequency of 2 hours and 30 minutes (2H30T) over 5 periods.\n\n\npd.timedelta_range(0, periods = 5, freq = \"2H30T\")\n\nTimedeltaIndex(['0 days 00:00:00', '0 days 02:30:00', '0 days 05:00:00',\n                '0 days 07:30:00', '0 days 10:00:00'],\n               dtype='timedelta64[ns]', freq='150T')"
  },
  {
    "objectID": "pandas.html#high-performance-pandas-eval",
    "href": "pandas.html#high-performance-pandas-eval",
    "title": "5  Data Manipulation with Pandas",
    "section": "5.8 High Performance Pandas: eval()",
    "text": "5.8 High Performance Pandas: eval()\n\neval() uses string expressions to compute operations using DataFrames\n\nsupports all arithmetic operations, comparison operators, bitwise operators (& and |), and the use of and and or in Boolean expressions\n\n\n\nnrows, ncols = 10, 5 # creating 2 DF of 5 rows and 10 columns\nrand = np.random.RandomState(7)\ndfa, dfb = (pd.DataFrame(rand.rand(nrows, ncols))\n            for i in range (2))\n\n# to compute sum of dfa and dfb and place into one table\n\nprint(\"dfa\", dfa)\nprint(\"dfb\", dfb)\npd.eval('dfa + dfb')\n\ndfa           0         1         2         3         4\n0  0.076308  0.779919  0.438409  0.723465  0.977990\n1  0.538496  0.501120  0.072051  0.268439  0.499883\n2  0.679230  0.803739  0.380941  0.065936  0.288146\n3  0.909594  0.213385  0.452124  0.931206  0.024899\n4  0.600549  0.950130  0.230303  0.548490  0.909128\n5  0.133169  0.523413  0.750410  0.669013  0.467753\n6  0.204849  0.490766  0.372385  0.477401  0.365890\n7  0.837918  0.768648  0.313995  0.572625  0.276049\n8  0.452843  0.352978  0.657399  0.370351  0.459093\n9  0.719324  0.412992  0.906423  0.180452  0.741119\ndfb           0         1         2         3         4\n0  0.422374  0.426454  0.634380  0.522906  0.414886\n1  0.001427  0.092262  0.709394  0.524346  0.696160\n2  0.955468  0.682914  0.053129  0.308853  0.592595\n3  0.235120  0.964971  0.945048  0.848401  0.472324\n4  0.841477  0.131111  0.308734  0.462996  0.741847\n5  0.485825  0.136876  0.343537  0.324426  0.300419\n6  0.165501  0.414902  0.448121  0.774900  0.796391\n7  0.522390  0.460630  0.778214  0.887289  0.674919\n8  0.800479  0.939111  0.040656  0.875672  0.276563\n9  0.475764  0.796761  0.717242  0.147148  0.658748\n\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n    \n  \n  \n    \n      0\n      0.498682\n      1.206372\n      1.072789\n      1.246371\n      1.392875\n    \n    \n      1\n      0.539923\n      0.593383\n      0.781446\n      0.792785\n      1.196043\n    \n    \n      2\n      1.634698\n      1.486653\n      0.434070\n      0.374789\n      0.880740\n    \n    \n      3\n      1.144714\n      1.178356\n      1.397172\n      1.779607\n      0.497223\n    \n    \n      4\n      1.442026\n      1.081240\n      0.539037\n      1.011486\n      1.650976\n    \n    \n      5\n      0.618995\n      0.660289\n      1.093946\n      0.993439\n      0.768172\n    \n    \n      6\n      0.370350\n      0.905668\n      0.820505\n      1.252302\n      1.162281\n    \n    \n      7\n      1.360308\n      1.229278\n      1.092208\n      1.459914\n      0.950968\n    \n    \n      8\n      1.253322\n      1.292090\n      0.698055\n      1.246023\n      0.735656\n    \n    \n      9\n      1.195089\n      1.209753\n      1.623666\n      0.327599\n      1.399867"
  },
  {
    "objectID": "geo.html#handling-spatial-data-with-geopandas",
    "href": "geo.html#handling-spatial-data-with-geopandas",
    "title": "6  Geospatial Data",
    "section": "6.1 “Handling Spatial Data with GeoPandas”",
    "text": "6.1 “Handling Spatial Data with GeoPandas”\nKaitlyn Bedard\nGeoPandas is a python library created as an extension of pandas to offer support for geographic data. Like pandas, GeoPandas has a series type and a dataframe type: GeoSeries and GeoDataFrame. It allows users to do work that would otherwise need a GIS database. Note that since GeoPandas is an extension of Pandas, it inherits all its attributes and methods. Please review the pandas presentations for information on these tools, if needed.\n\n6.1.1 Installation\nYou can install GeoPandas using the below commands in terminal. The documentation recommends the first method.\nconda install -c conda-forge geopandas\nconda install geopandas\npip install geopandas\n\n\n6.1.2 Basic Concepts\nThe GeoPandas GeoDataFrame is essentially a pandas dataframe that supports typical data, however, it also supports geometries. Though the dataframe can have multiple geometry columns, there is one “active” column on which all operations are applied to.\nThe types of geometries are:\n\nPoints: coordinates\nLines: set of two coordinates\nPolygons: list of coordinate tuples, first and last must be the same (closed shape)\n\nThese geometries are often represented by shapely.geometry objects. Note, we can also have multi-points, multi-lines, and multi-polygons. Below are examples of creating these geometries using shapely. Each GeoSeries has a specified CRS (Coordinate Reference System) that stores information about the data.\n\nfrom shapely.geometry import LineString, Point, Polygon\nimport geopandas as gpd\n\n# point example\npoint = Point(0.5, 0.5)\ngdf1 = gpd.GeoDataFrame(geometry=[point])\n\n# line example\nline = LineString([(0, 0), (1, 1)])\ngdf2 = gpd.GeoDataFrame(geometry=[line])\n\n# polygon example\npolygon = Polygon([(0, 0), (0, 1), (1, 1), (1, 0), (0, 0)])\ngdf3 = gpd.GeoDataFrame(geometry=[polygon])\n\nThe following are some examples of basic attributes of a GeoSeries:\n\nlength: returns the length of a line\n\n\ngdf2.length\n\n0    1.414214\ndtype: float64\n\n\n\narea: returns the area of the shape\n\n\ngdf3.area\n\n0    1.0\ndtype: float64\n\n\n\nbounds: gives the bounds of each row in a geometry column\ntotal_bounds: gives the total bounds of a geometry series\ngeom_type: gives the geometry type\n\n\ngdf3.geom_type\n\n0    Polygon\ndtype: object\n\n\n\nis_valid: returns True for valid geometries and False otherwise\n\nBelow are some examples of basic methods that can be applied to a GeoSeries:\n\ndistance(): returns the (minimum) distance of each row of a geometry to a specified paramater\n\nparameter other: can be a single geometry, or an entire geometry series\nparameter align: True if you want to align GeoSeries by index, false otherwise\n\n\n\ngdf2.distance(Point((1,0)))\n\n0    0.707107\ndtype: float64\n\n\n\ncentroid: returns a new GeoSeries with the centers of each row in the geometry\n\n\ngdf3.centroid\n\n0    POINT (0.50000 0.50000)\ndtype: geometry\n\n\nBelow are examples of some relationship tests that can be applied to a GeoSeries:\n\ncontains(): returns true if shape contains a specified other\n\nparameter other: can be a single geometry, or an entire geometry series\nparameter align: True if you want to align GeoSeries by index, false otherwise\n\n\n\ngdf3.contains(gdf1)\n\n0    True\ndtype: bool\n\n\n\nintersects(): returns true if shape intersects a specified other\n\nparameter other: can be a single geometry, or an entire geometry series\nparameter align: True if you want to align GeoSeries by index, false otherwise\n\n\n\ngdf2.intersects(gdf3)\n\n0    True\ndtype: bool\n\n\n\n\n6.1.3 Reading Files\nIf you have a file that contains data and geometry information, you can read it directly with geopandas using the geopandas.read_file() command. Examples of these files are GeoPackage, GeoJSON, Shapefile. However, we can convert other types of files to a GeoDataFrame. For example, we can transform the NYC crash data. The below code creates a point geometry. The points are the coordinates of the crashes.\n\n# Reading csv file \nimport pandas as pd \nimport numpy as np\n# Shapely for converting latitude/longtitude to geometry\nfrom shapely.geometry import Point \n# To create GeodataFrame\nimport geopandas as gpd \n\njan23 = pd.read_csv('data/nyc_crashes_202301_cleaned.csv')\n\n# creating geometry using shapely (removing empty points)\ngeometry = [Point(xy) for xy in zip(jan23[\"LONGITUDE\"], \\\n            jan23[\"LATITUDE\"]) if not Point(xy).is_empty]\n\n# creating geometry column to be used by geopandas\ngeometry2 = gpd.points_from_xy(jan23[\"LONGITUDE\"], jan23[\"LATITUDE\"])\n\n# coordinate reference system (epsg:4326 implies geographic coordinates)\ncrs = {'init': 'epsg:4326'}\n\n# create Geographic data frame (removing rows with missing coordinates)\njan23_gdf = gpd.GeoDataFrame(jan23.loc[~pd.isna(\\\n            jan23[\"LATITUDE\"]) & ~pd.isna(\\\n            jan23[\"LONGITUDE\"])],crs=crs, geometry=geometry)\n\njan23_gdf.head()\n\n/usr/local/lib/python3.11/site-packages/pyproj/crs/crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n  in_crs_string = _prepare_from_proj_string(in_crs_string)\n\n\n\n\n\n\n  \n    \n      \n      CRASH DATE\n      CRASH TIME\n      BOROUGH\n      ZIP CODE\n      LATITUDE\n      LONGITUDE\n      LOCATION\n      ON STREET NAME\n      CROSS STREET NAME\n      OFF STREET NAME\n      ...\n      Unnamed: 31\n      Unnamed: 32\n      Unnamed: 33\n      Unnamed: 34\n      Unnamed: 35\n      Unnamed: 36\n      Unnamed: 37\n      Unnamed: 38\n      Unnamed: 39\n      geometry\n    \n  \n  \n    \n      0\n      1/1/23\n      14:38\n      BROOKLYN\n      11211.0\n      40.719094\n      -73.946108\n      (40.7190938,-73.9461082)\n      BROOKLYN QUEENS EXPRESSWAY RAMP\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      POINT (-73.94611 40.71909)\n    \n    \n      1\n      1/1/23\n      8:04\n      QUEENS\n      11430.0\n      40.659508\n      -73.773687\n      (40.6595077,-73.7736867)\n      NASSAU EXPRESSWAY\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      POINT (-73.77369 40.65951)\n    \n    \n      2\n      1/1/23\n      18:05\n      MANHATTAN\n      10011.0\n      40.742454\n      -74.008686\n      (40.7424543,-74.008686)\n      10 AVENUE\n      11 AVENUE\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      POINT (-74.00869 40.74245)\n    \n    \n      3\n      1/1/23\n      23:45\n      QUEENS\n      11103.0\n      40.769737\n      -73.912440\n      (40.769737, -73.91244)\n      ASTORIA BOULEVARD\n      37 STREET\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      POINT (-73.91244 40.76974)\n    \n    \n      4\n      1/1/23\n      4:50\n      BRONX\n      10462.0\n      40.830555\n      -73.850720\n      (40.830555, -73.85072)\n      CASTLE HILL AVENUE\n      EAST 177 STREET\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      POINT (-73.85072 40.83055)\n    \n  \n\n5 rows × 41 columns\n\n\n\n\n\n6.1.4 Plotting\nWe can easily plot our data now that has been transformed to a geometric data frame.\n\n# Basic Plot\njan23_gdf.plot()\n# Color the plot by borough\njan23_gdf.plot(column = 'BOROUGH',legend=True)\n\n# Color the plot by number persons injuried\njan23_gdf.plot(column = 'NUMBER OF PERSONS INJURED',legend=True, \\\n               cmap= \"OrRd\")\n\n# Plotting missing information \njan23_gdf.plot(column='BOROUGH', missing_kwds={'color': 'lightgrey'})\n\n<AxesSubplot: >\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.1.5 Interactive Maps\nWe can also easily create an interactive plot, using the .explore() method.\n\n# interactive map of just the latitude and longitude points\njan23_gdf.explore()\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n# interactive map where points are colored by borough\njan23_gdf.explore(column='BOROUGH',legend=True)\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n# interative map that plots the crashes where 1+ persons are killed\njan23_gdf_edit = jan23_gdf.copy()\njan23_gdf_edit = jan23_gdf[jan23_gdf[\"NUMBER OF PERSONS KILLED\"] > 0]\njan23_gdf_edit.explore(column='NUMBER OF PERSONS KILLED', \\\n                        style_kwds={'radius': 7})\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\n6.1.6 Setting and Changing Projections\nEarlier, we showed how to set a CRS using crs = {'init': 'epsg:4326'}. However, the CRS can also be set using the .set_crs function on GeoDataFrame that does not yet have a defined CRS. Going back to our first example, gdf1, we can set the CRS as follows.\n\ngdf1 = gdf1.set_crs(\"EPSG:4326\")\ngdf1.plot()\n\n<AxesSubplot: >\n\n\n\n\n\nWe can also change the CRS of a geometry using the .to_crs() function. Some options are:\n\nEPSG:3395 - World Mercator system\nESPG:4326 - Standard Coordinates\nEPSG:2163 - NAD83, a system for the US and Canada\n\nNote that 4326 is the most common.\n\n\n6.1.7 Merging Data and Demonstrations\nThe below code imports the NYC borough and zip code level spatial data.\n\nimport geopandas as gpd\n\n# import NYC Borough Data\nboros = gpd.read_file(\"data/nyc_boroughs.geojson\")\nboros.set_crs(\"EPSG:4326\")\nboros.head()\n\n\n\n\n\n  \n    \n      \n      boro_code\n      boro_name\n      shape_area\n      shape_leng\n      geometry\n    \n  \n  \n    \n      0\n      5\n      Staten Island\n      1623620725.06\n      325917.353702\n      MULTIPOLYGON (((-74.05051 40.56642, -74.05047 ...\n    \n    \n      1\n      2\n      Bronx\n      1187182350.92\n      463176.004334\n      MULTIPOLYGON (((-73.89681 40.79581, -73.89694 ...\n    \n    \n      2\n      3\n      Brooklyn\n      1934229471.99\n      728263.543413\n      MULTIPOLYGON (((-73.86327 40.58388, -73.86381 ...\n    \n    \n      3\n      1\n      Manhattan\n      636520830.696\n      357564.317228\n      MULTIPOLYGON (((-74.01093 40.68449, -74.01193 ...\n    \n    \n      4\n      4\n      Queens\n      3041418543.49\n      888199.780587\n      MULTIPOLYGON (((-73.82645 40.59053, -73.82642 ...\n    \n  \n\n\n\n\n\n# import NYC Zip Code Data\nzipcodes = gpd.read_file(\"data/nyc_zipcodes.geojson\")\nzipcodes.set_crs(\"EPSG:4326\")\nzipcodes.head()\n\n\n\n\n\n  \n    \n      \n      ZIPCODE\n      BLDGZIP\n      PO_NAME\n      POPULATION\n      AREA\n      STATE\n      COUNTY\n      ST_FIPS\n      CTY_FIPS\n      URL\n      SHAPE_AREA\n      SHAPE_LEN\n      geometry\n    \n  \n  \n    \n      0\n      11436\n      0\n      Jamaica\n      18681.0\n      2.269930e+07\n      NY\n      Queens\n      36\n      081\n      http://www.usps.com/\n      0.0\n      0.0\n      POLYGON ((-73.80585 40.68291, -73.80569 40.682...\n    \n    \n      1\n      11213\n      0\n      Brooklyn\n      62426.0\n      2.963100e+07\n      NY\n      Kings\n      36\n      047\n      http://www.usps.com/\n      0.0\n      0.0\n      POLYGON ((-73.93740 40.67973, -73.93487 40.679...\n    \n    \n      2\n      11212\n      0\n      Brooklyn\n      83866.0\n      4.197210e+07\n      NY\n      Kings\n      36\n      047\n      http://www.usps.com/\n      0.0\n      0.0\n      POLYGON ((-73.90294 40.67084, -73.90223 40.668...\n    \n    \n      3\n      11225\n      0\n      Brooklyn\n      56527.0\n      2.369863e+07\n      NY\n      Kings\n      36\n      047\n      http://www.usps.com/\n      0.0\n      0.0\n      POLYGON ((-73.95797 40.67066, -73.95576 40.670...\n    \n    \n      4\n      11218\n      0\n      Brooklyn\n      72280.0\n      3.686880e+07\n      NY\n      Kings\n      36\n      047\n      http://www.usps.com/\n      0.0\n      0.0\n      POLYGON ((-73.97208 40.65060, -73.97192 40.650...\n    \n  \n\n\n\n\nI will demonstrate some more tools using the NYC Borough data, NYC Zip Code data, NYC Crash Data, and the merged data sets.\nWe can plot the borough data based on the number of people killed, for example. First, compute the average number of deaths per borough. Then merge the averages into the borough data frame, and plot accordingly.\n\n# change input to match\nboros['boro_name'] = boros['boro_name'].apply(lambda x: x.upper())\n\n# change name of column to match\njan23_gdf = jan23_gdf.rename(columns={\"BOROUGH\":\"boro_name\"})\n\n# Compute the average number of deaths per borough\navg_deaths_per_boro = jan23_gdf.groupby('boro_name')['NUMBER OF PERSONS KILLED'].mean()\n\n# Merge the average deaths per borough back into the borough GeoDataFrame\nboros = boros.merge(avg_deaths_per_boro, on='boro_name', suffixes=('', '_mean'))\nboros.head()\n\n\n\n\n\n  \n    \n      \n      boro_code\n      boro_name\n      shape_area\n      shape_leng\n      geometry\n      NUMBER OF PERSONS KILLED\n    \n  \n  \n    \n      0\n      5\n      STATEN ISLAND\n      1623620725.06\n      325917.353702\n      MULTIPOLYGON (((-74.05051 40.56642, -74.05047 ...\n      0.007812\n    \n    \n      1\n      2\n      BRONX\n      1187182350.92\n      463176.004334\n      MULTIPOLYGON (((-73.89681 40.79581, -73.89694 ...\n      0.000848\n    \n    \n      2\n      3\n      BROOKLYN\n      1934229471.99\n      728263.543413\n      MULTIPOLYGON (((-73.86327 40.58388, -73.86381 ...\n      0.001676\n    \n    \n      3\n      1\n      MANHATTAN\n      636520830.696\n      357564.317228\n      MULTIPOLYGON (((-74.01093 40.68449, -74.01193 ...\n      0.003101\n    \n    \n      4\n      4\n      QUEENS\n      3041418543.49\n      888199.780587\n      MULTIPOLYGON (((-73.82645 40.59053, -73.82642 ...\n      0.002525\n    \n  \n\n\n\n\n\n# plot \nboros.plot(column = \"NUMBER OF PERSONS KILLED\", legend = True)\n\n<AxesSubplot: >\n\n\n\n\n\nWe can follow this same process to plot the average number of injuries on the zip code level.\n\n# format changes\njan23_gdf = jan23_gdf.rename(columns={\"ZIP CODE\":\"ZIPCODE\"})\njan23_gdf[\"ZIPCODE\"] = jan23_gdf[\"ZIPCODE\"].replace(np.nan, 0)\njan23_gdf[\"ZIPCODE\"] = jan23_gdf[\"ZIPCODE\"].astype(int)\njan23_gdf[\"ZIPCODE\"] = jan23_gdf[\"ZIPCODE\"].astype(str)\njan23_gdf[\"ZIPCODE\"] = jan23_gdf[\"ZIPCODE\"].replace('0', np.nan)\n\n# Compute the average number of injuries per zipcode\navg_injuries_per_zip = jan23_gdf.groupby('ZIPCODE')['NUMBER OF PERSONS INJURED'].mean()\n\n# Merge the average injuries per zip back into the zipcodes GeoDataFrame\nzipcodes = zipcodes.merge(avg_injuries_per_zip, on='ZIPCODE', suffixes=('', '_mean'))\nzipcodes.head()\n\n\n\n\n\n  \n    \n      \n      ZIPCODE\n      BLDGZIP\n      PO_NAME\n      POPULATION\n      AREA\n      STATE\n      COUNTY\n      ST_FIPS\n      CTY_FIPS\n      URL\n      SHAPE_AREA\n      SHAPE_LEN\n      geometry\n      NUMBER OF PERSONS INJURED\n    \n  \n  \n    \n      0\n      11436\n      0\n      Jamaica\n      18681.0\n      2.269930e+07\n      NY\n      Queens\n      36\n      081\n      http://www.usps.com/\n      0.0\n      0.0\n      POLYGON ((-73.80585 40.68291, -73.80569 40.682...\n      0.535714\n    \n    \n      1\n      11213\n      0\n      Brooklyn\n      62426.0\n      2.963100e+07\n      NY\n      Kings\n      36\n      047\n      http://www.usps.com/\n      0.0\n      0.0\n      POLYGON ((-73.93740 40.67973, -73.93487 40.679...\n      0.573034\n    \n    \n      2\n      11212\n      0\n      Brooklyn\n      83866.0\n      4.197210e+07\n      NY\n      Kings\n      36\n      047\n      http://www.usps.com/\n      0.0\n      0.0\n      POLYGON ((-73.90294 40.67084, -73.90223 40.668...\n      0.627907\n    \n    \n      3\n      11225\n      0\n      Brooklyn\n      56527.0\n      2.369863e+07\n      NY\n      Kings\n      36\n      047\n      http://www.usps.com/\n      0.0\n      0.0\n      POLYGON ((-73.95797 40.67066, -73.95576 40.670...\n      0.970588\n    \n    \n      4\n      11218\n      0\n      Brooklyn\n      72280.0\n      3.686880e+07\n      NY\n      Kings\n      36\n      047\n      http://www.usps.com/\n      0.0\n      0.0\n      POLYGON ((-73.97208 40.65060, -73.97192 40.650...\n      0.660377\n    \n  \n\n\n\n\n\n# plot \nzipcodes.explore(column = \"NUMBER OF PERSONS INJURED\", legend = True)\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\nWe can also plot the number of crashes by zip code (or borough) as well. See the below code:\n\n# count the number of crashes per zipcode\ncrash_count_by_zipcode = jan23_gdf.groupby('ZIPCODE')['CRASH DATE'].count().reset_index()\n\n# merge the count with the zipcodes data frame\nzipcodes_with_crash_count = zipcodes.merge(crash_count_by_zipcode, on='ZIPCODE')\n\n# plot \nzipcodes_with_crash_count.plot(column='CRASH DATE', cmap='OrRd', legend=True)\n\n<AxesSubplot: >\n\n\n\n\n\n\n\n6.1.8 Resources\nFor more information see the following:\n\nGeoPandas Documentation\n\nhttps://geopandas.org/en/stable/docs.html\n\nNYC Borough Data\n\nhttps://data.cityofnewyork.us/City-Government/Borough-Boundaries/tqmj-j8zm\n\nNYC Zip Code Data\n\nhttps://data.beta.nyc/en/dataset/nyc-zip-code-tabulation-areas/resource/894e9162-871c-4552-a09c-c6915d8783fb"
  },
  {
    "objectID": "descr.html#different-python-methods-and-which-to-use",
    "href": "descr.html#different-python-methods-and-which-to-use",
    "title": "7  Descriptive Statistics",
    "section": "7.1 Different Python methods and which to use",
    "text": "7.1 Different Python methods and which to use\n\n7.1.1 Explanation of Methods\nThere are many methods to perform descriptive statistics operations. After briefly describing them, we will perform example operations to put into context how they work.\nPython’s built-in functions: These built-in operations are in the Python library, where we would not have to import any packages. There are not many operations already built-in, and it cannot compute large datasets well.\nStatistics package: Includes some additional functions for computation. NumPy is more compatible for using opertions than this package.\nNumPy: NumPy is a very common package to import. It is beneficial when working with single and multi dimensional arrays.\nPandas: Pandas is based off of the same numerical computing as NumPy and works with series and dataframes.\n\n\n7.1.2 Example: mean\nBelow is an example of computing mean with all of the above methods to express their differences.\nJust for this example, I will create my own datasets (as the NYC data does not portray the differences as easily).\n\nimport math\nimport numpy as np\nimport pandas as pd\n\nx = [2, 3.5, 7, 4]\nxnan = [2, 3.5, 7, 4, math.nan]\ny = np.array(xnan)\nz = pd.Series(xnan)\n\nPython’s built-in functions\nHere, we create the formula for mean by only using the built-in Python operations.\n\nmean = sum(x) / len(x) # sum and length are built-in, whereas mean is not\nmean # uses just the list\n\n4.125\n\n\n\nmean_xnan = sum(xnan) / len(xnan)\nmean_xnan\n\nnan\n\n\nThis method cannot skip NaN’s in the list, so the user would have to find a way to eliminate all NaN’s from their list before computing.\nStatistics Package\n\nimport statistics\nstatistics.mean(x) # uses just the list, x, rather than the array or series\n\n4.125\n\n\n\nstatistics.mean(xnan)\n\nnan\n\n\nSimilarly, will just output “nan” if there are any nan’s in the list.\nNumPy\n\nimport numpy as np\nnp.mean(y) # uses the array y = np.array(xnan)\n\nnan\n\n\nNotice that nan occurs. To avoid this, we can use nanmean() instead.\n\nnp.nanmean(y)\n\n4.125\n\n\nPandas\n\nimport pandas as pd\nz.mean() # uses the series z = pd.Series(xnan)\n\n4.125\n\n\nnan does not occur due to the default parameter in the pandas mean skipna = True.\n\nz.mean(skipna = False)\n\nnan\n\n\n\n\n7.1.3 So what do we use?\nAs shown above, pandas is nice as it automatically ignores nan by default when computing numeric operations, rather than just outputting nan. This is faster, cleaner, and preferrable to me when I am calculating operations. So, outside of the context of this class, I prefer using pandas if I had the choice.\nMoreover, in the context of this class, the data we will be analyzing is typically in the form of a dataframe. Pandas will typically be the best option when working with a dataframe, so it is best to continue using pandas."
  },
  {
    "objectID": "descr.html#data",
    "href": "descr.html#data",
    "title": "7  Descriptive Statistics",
    "section": "7.2 Data",
    "text": "7.2 Data\nThe data I will pull from is the January 2023 NYC Crash Data (cleaned).\n\njan23 = pd.read_csv(\"data/nyc_crashes_202301_cleaned.csv\")\n\njan23 = jan23.loc[:,['CRASH DATE', 'CRASH TIME', 'BOROUGH', 'ZIP CODE', 'LATITUDE',\n       'LONGITUDE', 'LOCATION', 'ON STREET NAME', 'CROSS STREET NAME',\n       'OFF STREET NAME', 'NUMBER OF PERSONS INJURED',\n       'NUMBER OF PERSONS KILLED', 'NUMBER OF PEDESTRIANS INJURED',\n       'NUMBER OF PEDESTRIANS KILLED', 'NUMBER OF CYCLIST INJURED',\n       'NUMBER OF CYCLIST KILLED', 'NUMBER OF MOTORIST INJURED',\n       'NUMBER OF MOTORIST KILLED', 'CONTRIBUTING FACTOR VEHICLE 1',\n       'CONTRIBUTING FACTOR VEHICLE 2', 'CONTRIBUTING FACTOR VEHICLE 3',\n       'CONTRIBUTING FACTOR VEHICLE 4', 'CONTRIBUTING FACTOR VEHICLE 5',\n       'COLLISION_ID', 'VEHICLE TYPE CODE 1', 'VEHICLE TYPE CODE 2',\n       'VEHICLE TYPE CODE 3', 'VEHICLE TYPE CODE 4', 'VEHICLE TYPE CODE 5']]\n\n\n7.2.1 Isolating Parts of the Dataframe\nDescriptive statistics do not make sense in context with all aspects of the dataframe we will be using. Most of the descriptive statistics shown below will only make sense with continuous variables. Thus, I will briefly show how to isolate certain aspects of the dataframe, so that we can do so later.\n\n7.2.1.1 Columns\n\njan23[\"BOROUGH\"] # isolating BOROUGH column\n\n0        BROOKLYN\n1          QUEENS\n2       MANHATTAN\n3          QUEENS\n4           BRONX\n          ...    \n7239     BROOKLYN\n7240     BROOKLYN\n7241     BROOKLYN\n7242    MANHATTAN\n7243       QUEENS\nName: BOROUGH, Length: 7244, dtype: object\n\n\n\ntype(jan23[\"BOROUGH\"])\n\npandas.core.series.Series\n\n\nNotice that the individual columns are classified as series. Pandas can be used on dataframes and series.\n\njan23[\"BOROUGH\"].value_counts(dropna = False) # categorical / discrete\n# \"dropna = True\" is the default and drops the missing (NaN) values\n\nBROOKLYN         2386\nQUEENS           1980\nMANHATTAN        1290\nBRONX            1179\nSTATEN ISLAND     384\nNaN                25\nName: BOROUGH, dtype: int64\n\n\nvalue_counts() does not work on dataframes, as it is a series operation. Moreover, it allows us to explore individual columns in more detail.\n\njan23[\"NUMBER OF PEDESTRIANS KILLED\"].value_counts(dropna = False) # numeric / continous\n# works with both categorical and numeric values\n\n0    7239\n1       5\nName: NUMBER OF PEDESTRIANS KILLED, dtype: int64\n\n\n\njan23[[\"BOROUGH\", \"NUMBER OF PEDESTRIANS KILLED\"]] # isolating multiple columns\n\n\n\n\n\n  \n    \n      \n      BOROUGH\n      NUMBER OF PEDESTRIANS KILLED\n    \n  \n  \n    \n      0\n      BROOKLYN\n      0\n    \n    \n      1\n      QUEENS\n      0\n    \n    \n      2\n      MANHATTAN\n      0\n    \n    \n      3\n      QUEENS\n      0\n    \n    \n      4\n      BRONX\n      0\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      7239\n      BROOKLYN\n      0\n    \n    \n      7240\n      BROOKLYN\n      0\n    \n    \n      7241\n      BROOKLYN\n      0\n    \n    \n      7242\n      MANHATTAN\n      0\n    \n    \n      7243\n      QUEENS\n      0\n    \n  \n\n7244 rows × 2 columns\n\n\n\n\n\n7.2.1.2 Rows\nDescriptive Statistics on rows are not very beneficial, as comparing the variables in rows of this NYC dataframe do not make much sense. Often, looking at rows is not very ideal, and the outputs are not always useful. However, here are a few ways that rows can be isolated from the dataframe if necessary.\n\njan23.iloc[6543:6547]\n\n\n\n\n\n  \n    \n      \n      CRASH DATE\n      CRASH TIME\n      BOROUGH\n      ZIP CODE\n      LATITUDE\n      LONGITUDE\n      LOCATION\n      ON STREET NAME\n      CROSS STREET NAME\n      OFF STREET NAME\n      ...\n      CONTRIBUTING FACTOR VEHICLE 2\n      CONTRIBUTING FACTOR VEHICLE 3\n      CONTRIBUTING FACTOR VEHICLE 4\n      CONTRIBUTING FACTOR VEHICLE 5\n      COLLISION_ID\n      VEHICLE TYPE CODE 1\n      VEHICLE TYPE CODE 2\n      VEHICLE TYPE CODE 3\n      VEHICLE TYPE CODE 4\n      VEHICLE TYPE CODE 5\n    \n  \n  \n    \n      6543\n      1/28/23\n      5:25\n      BROOKLYN\n      11206.0\n      40.701077\n      -73.94043\n      (40.701077, -73.94043)\n      HUMBOLDT STREET\n      FLUSHING AVENUE\n      NaN\n      ...\n      Other Vehicular\n      NaN\n      NaN\n      NaN\n      4602244\n      Station Wagon/Sport Utility Vehicle\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      6544\n      1/28/23\n      10:55\n      STATEN ISLAND\n      10301.0\n      40.640907\n      -74.08134\n      (40.640907, -74.08134)\n      NaN\n      NaN\n      25        SHERMAN AVENUE\n      ...\n      Unspecified\n      NaN\n      NaN\n      NaN\n      4602219\n      Sedan\n      Sedan\n      NaN\n      NaN\n      NaN\n    \n    \n      6545\n      1/28/23\n      0:09\n      QUEENS\n      11372.0\n      40.755030\n      -73.88242\n      (40.75503, -73.88242)\n      NaN\n      NaN\n      33-11     85 STREET\n      ...\n      Unspecified\n      NaN\n      NaN\n      NaN\n      4602365\n      Sedan\n      Box Truck\n      NaN\n      NaN\n      NaN\n    \n    \n      6546\n      1/28/23\n      13:00\n      BROOKLYN\n      11220.0\n      40.644955\n      -74.01611\n      (40.644955, -74.01611)\n      NaN\n      NaN\n      325       54 STREET\n      ...\n      Unspecified\n      NaN\n      NaN\n      NaN\n      4602449\n      Sedan\n      Station Wagon/Sport Utility Vehicle\n      NaN\n      NaN\n      NaN\n    \n  \n\n4 rows × 29 columns\n\n\n\n\njan23[jan23[\"CRASH DATE\"] == \"01/01/2023\"]\n\n\n\n\n\n  \n    \n      \n      CRASH DATE\n      CRASH TIME\n      BOROUGH\n      ZIP CODE\n      LATITUDE\n      LONGITUDE\n      LOCATION\n      ON STREET NAME\n      CROSS STREET NAME\n      OFF STREET NAME\n      ...\n      CONTRIBUTING FACTOR VEHICLE 2\n      CONTRIBUTING FACTOR VEHICLE 3\n      CONTRIBUTING FACTOR VEHICLE 4\n      CONTRIBUTING FACTOR VEHICLE 5\n      COLLISION_ID\n      VEHICLE TYPE CODE 1\n      VEHICLE TYPE CODE 2\n      VEHICLE TYPE CODE 3\n      VEHICLE TYPE CODE 4\n      VEHICLE TYPE CODE 5\n    \n  \n  \n  \n\n0 rows × 29 columns\n\n\n\n\njan23[jan23[\"COLLISION_ID\"] == 4594599]\n\n\n\n\n\n  \n    \n      \n      CRASH DATE\n      CRASH TIME\n      BOROUGH\n      ZIP CODE\n      LATITUDE\n      LONGITUDE\n      LOCATION\n      ON STREET NAME\n      CROSS STREET NAME\n      OFF STREET NAME\n      ...\n      CONTRIBUTING FACTOR VEHICLE 2\n      CONTRIBUTING FACTOR VEHICLE 3\n      CONTRIBUTING FACTOR VEHICLE 4\n      CONTRIBUTING FACTOR VEHICLE 5\n      COLLISION_ID\n      VEHICLE TYPE CODE 1\n      VEHICLE TYPE CODE 2\n      VEHICLE TYPE CODE 3\n      VEHICLE TYPE CODE 4\n      VEHICLE TYPE CODE 5\n    \n  \n  \n    \n      1\n      1/1/23\n      8:04\n      QUEENS\n      11430.0\n      40.659508\n      -73.773687\n      (40.6595077,-73.7736867)\n      NASSAU EXPRESSWAY\n      NaN\n      NaN\n      ...\n      Unspecified\n      NaN\n      NaN\n      NaN\n      4594599\n      Sedan\n      Sedan\n      NaN\n      NaN\n      NaN\n    \n  \n\n1 rows × 29 columns\n\n\n\n\ntype(jan23[jan23[\"COLLISION_ID\"] == 4594599])\n\npandas.core.frame.DataFrame\n\n\n\n\n\n7.2.2 Data Isolated\nOnly the continuous variables will make sense for most of the descriptive statistics below, so we will use the following dataframe of just the continous variables, when applicable.\n\ncjan23 = jan23[[\"NUMBER OF PEDESTRIANS INJURED\", \"NUMBER OF PEDESTRIANS KILLED\", \"NUMBER OF CYCLIST INJURED\", \n                \"NUMBER OF CYCLIST KILLED\", \"NUMBER OF MOTORIST INJURED\", \"NUMBER OF MOTORIST KILLED\"]]"
  },
  {
    "objectID": "descr.html#common-operations",
    "href": "descr.html#common-operations",
    "title": "7  Descriptive Statistics",
    "section": "7.3 Common Operations",
    "text": "7.3 Common Operations\n\n7.3.1 Descriptive Statistics with Pandas\n\n\n7.3.2 center\n\nmean(): mean\nmedian(): median\nmode(): mode\n\n\n\n7.3.3 spread\n\nmin(): minimum\nmax(): maximum\nstd(): standard deviation\nvar(): variance\nquantile(): quantiles\n\n\n\n7.3.4 shape\n\nskew(): adjusted Fisher-Pearson standardized moment\n\n\n\n7.3.5 correlation (deals with two variables)\n\ncorr(): correlation coefficient\ncov(): covariance\n\n\n\n7.3.6 other important operations\n\ncount(): total count\nsum(): summation\nvalue_counts(): individual counts\ndescribe(): describe the data with many descriptive statistics\n\nBelow I worked on a few specific descriptive statistics operators to give a general idea of how the operators work. If an operator is not used below, it is listed under where it would be used similarly.\n\n\n7.3.7 Important operators\nSum\nsum(axis = None, skipna = False). Below we focus on the usage of axis.\n\ncjan23.sum() # or cjan23.sum(0) or cjan23.sum(None)\n# takes the indvidual sums of the numeric columns\n\nNUMBER OF PEDESTRIANS INJURED     843\nNUMBER OF PEDESTRIANS KILLED        5\nNUMBER OF CYCLIST INJURED         241\nNUMBER OF CYCLIST KILLED            3\nNUMBER OF MOTORIST INJURED       2413\nNUMBER OF MOTORIST KILLED           9\ndtype: int64\n\n\n\n# compute \"axis = 1\", rows\ncjan23.sum(1)\n\n0       1\n1       1\n2       0\n3       2\n4       0\n       ..\n7239    0\n7240    1\n7241    0\n7242    0\n7243    2\nLength: 7244, dtype: int64\n\n\nThese functions: mean(), median(), mode(), min(), max() std(), var(), quantile(), skew(), and count() are used similarly, where their default will operate on the columns, and a specification of axis = 1 will operate on the rows. Any of these operators not shown below, give a similar looking output as sum() does above.\n\n\n7.3.8 Center\nMode\n\njan23.mode() # lists the most frequent value\n# mode is relevant for discrete and continuous variables\n\n\n\n\n\n  \n    \n      \n      CRASH DATE\n      CRASH TIME\n      BOROUGH\n      ZIP CODE\n      LATITUDE\n      LONGITUDE\n      LOCATION\n      ON STREET NAME\n      CROSS STREET NAME\n      OFF STREET NAME\n      ...\n      CONTRIBUTING FACTOR VEHICLE 2\n      CONTRIBUTING FACTOR VEHICLE 3\n      CONTRIBUTING FACTOR VEHICLE 4\n      CONTRIBUTING FACTOR VEHICLE 5\n      COLLISION_ID\n      VEHICLE TYPE CODE 1\n      VEHICLE TYPE CODE 2\n      VEHICLE TYPE CODE 3\n      VEHICLE TYPE CODE 4\n      VEHICLE TYPE CODE 5\n    \n  \n  \n    \n      0\n      1/13/23\n      0:00\n      BROOKLYN\n      11207.0\n      40.606566\n      -74.044983\n      (0.0, 0.0)\n      BELT PARKWAY\n      3 AVENUE\n      49-21     METROPOLITAN AVENUE\n      ...\n      Unspecified\n      Unspecified\n      Unspecified\n      Unspecified\n      4594332\n      Sedan\n      Sedan\n      Sedan\n      Sedan\n      Sedan\n    \n    \n      1\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      BROADWAY\n      560       WINTHROP STREET\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      4594347\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      985       RICHMOND AVENUE\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      4594350\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ATLANTIC AVENUE\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      4594351\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      4594359\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      7239\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      4605213\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      7240\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      4605214\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      7241\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      4605246\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      7242\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      4605289\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      7243\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      4605324\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n7244 rows × 29 columns\n\n\n\nThe mode outputs the most frequent value. If there are multiple values that are the most frequent, then all of those values will be outputted. For example, “OFF STREET NAME” has four values that are the most frequent. Thus, four values are outputted. Since mode() was outputted in the format of a dataframe, the NaN values just represent empty spaces, where other columns have a value in that row. See: “COLLISION_ID”. There are 7244 rows because there are 7244 unique collision ID’s, so they all are the most frequent value (one occurrence of each). This explains all the empty spaces with all of the other variables, since the dataframe format needed a filler to still output a dataframe.\n\n\n7.3.9 Spread\nQuantile\n\ncjan23.quantile([.65, .9]) # can specify specific quantiles\n\n\n\n\n\n  \n    \n      \n      NUMBER OF PEDESTRIANS INJURED\n      NUMBER OF PEDESTRIANS KILLED\n      NUMBER OF CYCLIST INJURED\n      NUMBER OF CYCLIST KILLED\n      NUMBER OF MOTORIST INJURED\n      NUMBER OF MOTORIST KILLED\n    \n  \n  \n    \n      0.65\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      0.90\n      1.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n    \n  \n\n\n\n\n\n\n7.3.10 Shape\nSkew\n\ncjan23.skew() # negative skew means left skewness, positive means right\n\nNUMBER OF PEDESTRIANS INJURED    16.357461\nNUMBER OF PEDESTRIANS KILLED     38.031561\nNUMBER OF CYCLIST INJURED         5.276883\nNUMBER OF CYCLIST KILLED         49.118899\nNUMBER OF MOTORIST INJURED        3.230788\nNUMBER OF MOTORIST KILLED        34.958763\ndtype: float64\n\n\n\n\n7.3.11 Correlation\nCorrelation Coefficient\n\njan23[\"NUMBER OF PEDESTRIANS INJURED\"].corr(jan23[\"NUMBER OF PEDESTRIANS KILLED\"])  \n# the correlation coefficient of these two variables\n\n-0.007686375916216244\n\n\ncov() would be calculated in the same way.\n\n\n7.3.12 Describe\nAbove calculates each chosen operation indivudally. Is there one operation that can show multiple descriptive statistics at once?\nJust for the purpose of showing how to make changes to the default function where the character values are needed to portray, I will be using all variables (dataframe jan23). Later, I will use cjan23 when delving more into editing the describe function, as the descriptie statistics automatically count numeric values as continuous (which is not true for many of these numeric variables).\n\njan23.describe() # default omits character and string values\n\n\n\n\n\n  \n    \n      \n      ZIP CODE\n      LATITUDE\n      LONGITUDE\n      NUMBER OF PERSONS INJURED\n      NUMBER OF PERSONS KILLED\n      NUMBER OF PEDESTRIANS INJURED\n      NUMBER OF PEDESTRIANS KILLED\n      NUMBER OF CYCLIST INJURED\n      NUMBER OF CYCLIST KILLED\n      NUMBER OF MOTORIST INJURED\n      NUMBER OF MOTORIST KILLED\n      COLLISION_ID\n    \n  \n  \n    \n      count\n      7240.000000\n      7240.000000\n      7240.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7.244000e+03\n    \n    \n      mean\n      10876.268785\n      40.723872\n      -73.917446\n      0.502761\n      0.002347\n      0.116372\n      0.000690\n      0.033269\n      0.000414\n      0.333103\n      0.001242\n      4.599022e+06\n    \n    \n      std\n      532.816111\n      0.087734\n      0.088494\n      0.813641\n      0.051164\n      0.397927\n      0.026265\n      0.180118\n      0.020348\n      0.749174\n      0.038951\n      2.365885e+03\n    \n    \n      min\n      10001.000000\n      40.504658\n      -74.250150\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      4.594332e+06\n    \n    \n      25%\n      10453.000000\n      40.665374\n      -73.966253\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      4.597113e+06\n    \n    \n      50%\n      11208.000000\n      40.714790\n      -73.922485\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      4.599058e+06\n    \n    \n      75%\n      11239.000000\n      40.784210\n      -73.865596\n      1.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      4.600953e+06\n    \n    \n      max\n      12134.000000\n      43.299428\n      -73.051978\n      21.000000\n      2.000000\n      19.000000\n      1.000000\n      2.000000\n      1.000000\n      8.000000\n      2.000000\n      4.605324e+06\n    \n  \n\n\n\n\nIt may be useful to edit the describe() feature to show moreso the values that we wish to see. The default .describe() output is shown above.\nThe default describe() input:\nDataFrame.describe(percentiles = None, include = None, exclude = None, datetime_is_numeric = False)\n\n7.3.12.1 Changing the default\n\n# changing percentile default\njan23.describe([.2, .45, .9])\n\n\n\n\n\n  \n    \n      \n      ZIP CODE\n      LATITUDE\n      LONGITUDE\n      NUMBER OF PERSONS INJURED\n      NUMBER OF PERSONS KILLED\n      NUMBER OF PEDESTRIANS INJURED\n      NUMBER OF PEDESTRIANS KILLED\n      NUMBER OF CYCLIST INJURED\n      NUMBER OF CYCLIST KILLED\n      NUMBER OF MOTORIST INJURED\n      NUMBER OF MOTORIST KILLED\n      COLLISION_ID\n    \n  \n  \n    \n      count\n      7240.000000\n      7240.000000\n      7240.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7.244000e+03\n    \n    \n      mean\n      10876.268785\n      40.723872\n      -73.917446\n      0.502761\n      0.002347\n      0.116372\n      0.000690\n      0.033269\n      0.000414\n      0.333103\n      0.001242\n      4.599022e+06\n    \n    \n      std\n      532.816111\n      0.087734\n      0.088494\n      0.813641\n      0.051164\n      0.397927\n      0.026265\n      0.180118\n      0.020348\n      0.749174\n      0.038951\n      2.365885e+03\n    \n    \n      min\n      10001.000000\n      40.504658\n      -74.250150\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      4.594332e+06\n    \n    \n      20%\n      10305.000000\n      40.651722\n      -73.978789\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      4.596668e+06\n    \n    \n      45%\n      11204.000000\n      40.704138\n      -73.931070\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      4.598665e+06\n    \n    \n      50%\n      11208.000000\n      40.714790\n      -73.922485\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      4.599058e+06\n    \n    \n      90%\n      11415.000000\n      40.843662\n      -73.802865\n      1.000000\n      0.000000\n      1.000000\n      0.000000\n      0.000000\n      0.000000\n      1.000000\n      0.000000\n      4.602072e+06\n    \n    \n      max\n      12134.000000\n      43.299428\n      -73.051978\n      21.000000\n      2.000000\n      19.000000\n      1.000000\n      2.000000\n      1.000000\n      8.000000\n      2.000000\n      4.605324e+06\n    \n  \n\n\n\n\nReplaces the default .25 and .75, but keeps the median (.5).\n\n# including all columns, rather than just \"number\" default\njan23.describe(include = 'all')\n\n\n\n\n\n  \n    \n      \n      CRASH DATE\n      CRASH TIME\n      BOROUGH\n      ZIP CODE\n      LATITUDE\n      LONGITUDE\n      LOCATION\n      ON STREET NAME\n      CROSS STREET NAME\n      OFF STREET NAME\n      ...\n      CONTRIBUTING FACTOR VEHICLE 2\n      CONTRIBUTING FACTOR VEHICLE 3\n      CONTRIBUTING FACTOR VEHICLE 4\n      CONTRIBUTING FACTOR VEHICLE 5\n      COLLISION_ID\n      VEHICLE TYPE CODE 1\n      VEHICLE TYPE CODE 2\n      VEHICLE TYPE CODE 3\n      VEHICLE TYPE CODE 4\n      VEHICLE TYPE CODE 5\n    \n  \n  \n    \n      count\n      7244\n      7244\n      7219\n      7240.000000\n      7240.000000\n      7240.000000\n      7244\n      5341\n      3453\n      1903\n      ...\n      5378\n      689\n      191\n      62\n      7.244000e+03\n      7108\n      4553\n      634\n      179\n      59\n    \n    \n      unique\n      31\n      1245\n      5\n      NaN\n      NaN\n      NaN\n      6140\n      1580\n      1562\n      1877\n      ...\n      30\n      13\n      5\n      3\n      NaN\n      67\n      81\n      17\n      11\n      5\n    \n    \n      top\n      1/13/23\n      0:00\n      BROOKLYN\n      NaN\n      NaN\n      NaN\n      (0.0, 0.0)\n      BELT PARKWAY\n      BROADWAY\n      560       WINTHROP STREET\n      ...\n      Unspecified\n      Unspecified\n      Unspecified\n      Unspecified\n      NaN\n      Sedan\n      Sedan\n      Sedan\n      Sedan\n      Sedan\n    \n    \n      freq\n      294\n      116\n      2386\n      NaN\n      NaN\n      NaN\n      81\n      124\n      37\n      3\n      ...\n      4550\n      637\n      183\n      60\n      NaN\n      3478\n      1969\n      327\n      93\n      29\n    \n    \n      mean\n      NaN\n      NaN\n      NaN\n      10876.268785\n      40.723872\n      -73.917446\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      4.599022e+06\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      std\n      NaN\n      NaN\n      NaN\n      532.816111\n      0.087734\n      0.088494\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      2.365885e+03\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      min\n      NaN\n      NaN\n      NaN\n      10001.000000\n      40.504658\n      -74.250150\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      4.594332e+06\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      25%\n      NaN\n      NaN\n      NaN\n      10453.000000\n      40.665374\n      -73.966253\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      4.597113e+06\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      50%\n      NaN\n      NaN\n      NaN\n      11208.000000\n      40.714790\n      -73.922485\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      4.599058e+06\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      75%\n      NaN\n      NaN\n      NaN\n      11239.000000\n      40.784210\n      -73.865596\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      4.600953e+06\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      max\n      NaN\n      NaN\n      NaN\n      12134.000000\n      43.299428\n      -73.051978\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      4.605324e+06\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n11 rows × 29 columns\n\n\n\nMy only con with this, is that most of these numerical values are not included in the “unique”, “top”, and “frequency” rows, even though in context they are discrete, and would make sense to be included in these.\n\n# excluding numerical columns\n# gives just \"object\" i.e. categorical\njan23.describe(exclude = 'number')\n\n\n\n\n\n  \n    \n      \n      CRASH DATE\n      CRASH TIME\n      BOROUGH\n      LOCATION\n      ON STREET NAME\n      CROSS STREET NAME\n      OFF STREET NAME\n      CONTRIBUTING FACTOR VEHICLE 1\n      CONTRIBUTING FACTOR VEHICLE 2\n      CONTRIBUTING FACTOR VEHICLE 3\n      CONTRIBUTING FACTOR VEHICLE 4\n      CONTRIBUTING FACTOR VEHICLE 5\n      VEHICLE TYPE CODE 1\n      VEHICLE TYPE CODE 2\n      VEHICLE TYPE CODE 3\n      VEHICLE TYPE CODE 4\n      VEHICLE TYPE CODE 5\n    \n  \n  \n    \n      count\n      7244\n      7244\n      7219\n      7244\n      5341\n      3453\n      1903\n      7189\n      5378\n      689\n      191\n      62\n      7108\n      4553\n      634\n      179\n      59\n    \n    \n      unique\n      31\n      1245\n      5\n      6140\n      1580\n      1562\n      1877\n      48\n      30\n      13\n      5\n      3\n      67\n      81\n      17\n      11\n      5\n    \n    \n      top\n      1/13/23\n      0:00\n      BROOKLYN\n      (0.0, 0.0)\n      BELT PARKWAY\n      BROADWAY\n      560       WINTHROP STREET\n      Driver Inattention/Distraction\n      Unspecified\n      Unspecified\n      Unspecified\n      Unspecified\n      Sedan\n      Sedan\n      Sedan\n      Sedan\n      Sedan\n    \n    \n      freq\n      294\n      116\n      2386\n      81\n      124\n      37\n      3\n      1776\n      4550\n      637\n      183\n      60\n      3478\n      1969\n      327\n      93\n      29\n    \n  \n\n\n\n\nInteresting note: rather than outputting as an empty set, the function decided to use the columns that are typically omitted instead, which is the same as jan23.describe(\"include = object\").\n\n# making datetime numeric\njan23.describe(include = 'all', datetime_is_numeric = True)\n\n\n\n\n\n  \n    \n      \n      CRASH DATE\n      CRASH TIME\n      BOROUGH\n      ZIP CODE\n      LATITUDE\n      LONGITUDE\n      LOCATION\n      ON STREET NAME\n      CROSS STREET NAME\n      OFF STREET NAME\n      ...\n      CONTRIBUTING FACTOR VEHICLE 2\n      CONTRIBUTING FACTOR VEHICLE 3\n      CONTRIBUTING FACTOR VEHICLE 4\n      CONTRIBUTING FACTOR VEHICLE 5\n      COLLISION_ID\n      VEHICLE TYPE CODE 1\n      VEHICLE TYPE CODE 2\n      VEHICLE TYPE CODE 3\n      VEHICLE TYPE CODE 4\n      VEHICLE TYPE CODE 5\n    \n  \n  \n    \n      count\n      7244\n      7244\n      7219\n      7240.000000\n      7240.000000\n      7240.000000\n      7244\n      5341\n      3453\n      1903\n      ...\n      5378\n      689\n      191\n      62\n      7.244000e+03\n      7108\n      4553\n      634\n      179\n      59\n    \n    \n      unique\n      31\n      1245\n      5\n      NaN\n      NaN\n      NaN\n      6140\n      1580\n      1562\n      1877\n      ...\n      30\n      13\n      5\n      3\n      NaN\n      67\n      81\n      17\n      11\n      5\n    \n    \n      top\n      1/13/23\n      0:00\n      BROOKLYN\n      NaN\n      NaN\n      NaN\n      (0.0, 0.0)\n      BELT PARKWAY\n      BROADWAY\n      560       WINTHROP STREET\n      ...\n      Unspecified\n      Unspecified\n      Unspecified\n      Unspecified\n      NaN\n      Sedan\n      Sedan\n      Sedan\n      Sedan\n      Sedan\n    \n    \n      freq\n      294\n      116\n      2386\n      NaN\n      NaN\n      NaN\n      81\n      124\n      37\n      3\n      ...\n      4550\n      637\n      183\n      60\n      NaN\n      3478\n      1969\n      327\n      93\n      29\n    \n    \n      mean\n      NaN\n      NaN\n      NaN\n      10876.268785\n      40.723872\n      -73.917446\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      4.599022e+06\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      std\n      NaN\n      NaN\n      NaN\n      532.816111\n      0.087734\n      0.088494\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      2.365885e+03\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      min\n      NaN\n      NaN\n      NaN\n      10001.000000\n      40.504658\n      -74.250150\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      4.594332e+06\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      25%\n      NaN\n      NaN\n      NaN\n      10453.000000\n      40.665374\n      -73.966253\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      4.597113e+06\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      50%\n      NaN\n      NaN\n      NaN\n      11208.000000\n      40.714790\n      -73.922485\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      4.599058e+06\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      75%\n      NaN\n      NaN\n      NaN\n      11239.000000\n      40.784210\n      -73.865596\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      4.600953e+06\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      max\n      NaN\n      NaN\n      NaN\n      12134.000000\n      43.299428\n      -73.051978\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      4.605324e+06\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n11 rows × 29 columns\n\n\n\nIncluding datetime as numeric works if the date times are inputted in a different style (typically, YYYY-MM-DD 00:00:00.000000), which our data is not. Thus, as we see, the date and time is still treated as an object.\n\n\n7.3.12.2 Changing rows with describe()\nAbove were specific ways to change the function that were already built into the function itself. What if we want to add more rows describing another descriptive statistic? I will be using just the discrete values for the following examples.\n\n# adding sum to the dataframe\ncjan23.describe().append(pd.Series(cjan23.sum(), name = 'sum'))\n\n/var/folders/cq/5ysgnwfn7c3g0h46xyzvpj800000gn/T/ipykernel_25286/3080308836.py:2: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n  cjan23.describe().append(pd.Series(cjan23.sum(), name = 'sum'))\n\n\n\n\n\n\n  \n    \n      \n      NUMBER OF PEDESTRIANS INJURED\n      NUMBER OF PEDESTRIANS KILLED\n      NUMBER OF CYCLIST INJURED\n      NUMBER OF CYCLIST KILLED\n      NUMBER OF MOTORIST INJURED\n      NUMBER OF MOTORIST KILLED\n    \n  \n  \n    \n      count\n      7244.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7244.000000\n    \n    \n      mean\n      0.116372\n      0.000690\n      0.033269\n      0.000414\n      0.333103\n      0.001242\n    \n    \n      std\n      0.397927\n      0.026265\n      0.180118\n      0.020348\n      0.749174\n      0.038951\n    \n    \n      min\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      25%\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      50%\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      75%\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      max\n      19.000000\n      1.000000\n      2.000000\n      1.000000\n      8.000000\n      2.000000\n    \n    \n      sum\n      843.000000\n      5.000000\n      241.000000\n      3.000000\n      2413.000000\n      9.000000\n    \n  \n\n\n\n\n\n# adding a row counting nan's\ncjan23.describe().append(pd.Series(cjan23.isna().sum(), name = 'nans'))\n\n/var/folders/cq/5ysgnwfn7c3g0h46xyzvpj800000gn/T/ipykernel_25286/4231082202.py:2: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n  cjan23.describe().append(pd.Series(cjan23.isna().sum(), name = 'nans'))\n\n\n\n\n\n\n  \n    \n      \n      NUMBER OF PEDESTRIANS INJURED\n      NUMBER OF PEDESTRIANS KILLED\n      NUMBER OF CYCLIST INJURED\n      NUMBER OF CYCLIST KILLED\n      NUMBER OF MOTORIST INJURED\n      NUMBER OF MOTORIST KILLED\n    \n  \n  \n    \n      count\n      7244.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7244.000000\n    \n    \n      mean\n      0.116372\n      0.000690\n      0.033269\n      0.000414\n      0.333103\n      0.001242\n    \n    \n      std\n      0.397927\n      0.026265\n      0.180118\n      0.020348\n      0.749174\n      0.038951\n    \n    \n      min\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      25%\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      50%\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      75%\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      max\n      19.000000\n      1.000000\n      2.000000\n      1.000000\n      8.000000\n      2.000000\n    \n    \n      nans\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n  \n\n\n\n\n\n# removing a row\ncjan23.describe().drop(labels = \"max\", axis = 0)\n\n\n\n\n\n  \n    \n      \n      NUMBER OF PEDESTRIANS INJURED\n      NUMBER OF PEDESTRIANS KILLED\n      NUMBER OF CYCLIST INJURED\n      NUMBER OF CYCLIST KILLED\n      NUMBER OF MOTORIST INJURED\n      NUMBER OF MOTORIST KILLED\n    \n  \n  \n    \n      count\n      7244.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7244.000000\n    \n    \n      mean\n      0.116372\n      0.000690\n      0.033269\n      0.000414\n      0.333103\n      0.001242\n    \n    \n      std\n      0.397927\n      0.026265\n      0.180118\n      0.020348\n      0.749174\n      0.038951\n    \n    \n      min\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      25%\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      50%\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      75%\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n  \n\n\n\n\n\n\n7.3.12.3 Changing columns with describe()\n\n# removing a column\ncjan23.describe().drop(columns = \"NUMBER OF CYCLIST INJURED\")\n\n\n\n\n\n  \n    \n      \n      NUMBER OF PEDESTRIANS INJURED\n      NUMBER OF PEDESTRIANS KILLED\n      NUMBER OF CYCLIST KILLED\n      NUMBER OF MOTORIST INJURED\n      NUMBER OF MOTORIST KILLED\n    \n  \n  \n    \n      count\n      7244.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7244.000000\n    \n    \n      mean\n      0.116372\n      0.000690\n      0.000414\n      0.333103\n      0.001242\n    \n    \n      std\n      0.397927\n      0.026265\n      0.020348\n      0.749174\n      0.038951\n    \n    \n      min\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      25%\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      50%\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      75%\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      max\n      19.000000\n      1.000000\n      1.000000\n      8.000000\n      2.000000\n    \n  \n\n\n\n\n\n# note that the manual changes made above are not permanent unless the variable is reassigned\ncjan23.describe()\n\n\n\n\n\n  \n    \n      \n      NUMBER OF PEDESTRIANS INJURED\n      NUMBER OF PEDESTRIANS KILLED\n      NUMBER OF CYCLIST INJURED\n      NUMBER OF CYCLIST KILLED\n      NUMBER OF MOTORIST INJURED\n      NUMBER OF MOTORIST KILLED\n    \n  \n  \n    \n      count\n      7244.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7244.000000\n    \n    \n      mean\n      0.116372\n      0.000690\n      0.033269\n      0.000414\n      0.333103\n      0.001242\n    \n    \n      std\n      0.397927\n      0.026265\n      0.180118\n      0.020348\n      0.749174\n      0.038951\n    \n    \n      min\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      25%\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      50%\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      75%\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      max\n      19.000000\n      1.000000\n      2.000000\n      1.000000\n      8.000000\n      2.000000\n    \n  \n\n\n\n\nAll of the above ways were manipulating the describe() operator to potentially make visualizing descriptive statistics easier, by putting certain desirable traits in or out of the table.\nDescribe on Individual Columns\n\njan23[\"BOROUGH\"].describe() # character and discrete\n\ncount         7219\nunique           5\ntop       BROOKLYN\nfreq          2386\nName: BOROUGH, dtype: object\n\n\n\njan23[\"NUMBER OF PEDESTRIANS KILLED\"].describe() # numeric and continuous\n\ncount    7244.000000\nmean        0.000690\nstd         0.026265\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%         0.000000\nmax         1.000000\nName: NUMBER OF PEDESTRIANS KILLED, dtype: float64\n\n\nNote that numeric and discrete would still be treated as continuous, so descriptive statistics are not very beneficial fot these variables. Regardless, descriptive statistics are typically more of interest to us if they are continuous."
  },
  {
    "objectID": "descr.html#conclusion",
    "href": "descr.html#conclusion",
    "title": "7  Descriptive Statistics",
    "section": "7.4 Conclusion",
    "text": "7.4 Conclusion\nIn this presentation we looked into different methods of performing descriptive statistics, and saw how to use many of these operators. There are many ways to compute descriptive statistics, and we explored how to do so with pandas. We then focused on how to maniputlate the describe() function in many ways that may help us to visualize the data much easier. Afterwards, we looked at isolating columns and rows to perform descriptive statistics on. Analyzing the descriptive statistics is extremely important to understaning data. Another way to possibly put data into a more digestible form is to visualize it, which other presentations touch on."
  },
  {
    "objectID": "stats.html#tests-for-exploratory-data-analysis",
    "href": "stats.html#tests-for-exploratory-data-analysis",
    "title": "8  Statistical Tests and Models",
    "section": "8.1 Tests for Exploratory Data Analysis",
    "text": "8.1 Tests for Exploratory Data Analysis\nA collection of functions are available from scipy.stats.\n\nComparing the locations of two samples\n\nttest_ind: t-test for two independent samples\nttest_rel: t-test for paired samples\nranksums: Wilcoxon rank-sum test for two independent samples\nwilcoxon: Wilcoxon signed-rank test for paired samples\n\nComparing the locations of multiple samples\n\nf_oneway: one-way ANOVA\nkruskal: Kruskal-Wallis H-test\n\nTests for associations in contigency tables\n\nchi2_contingency: Chi-square test of independence of variables\nfisher_exact: Fisher exact test on a 2x2 contingency table\n\nGoodness of fit\n\ngoodness_of_fit: distribution could contain unspecified parameters\nanderson: Anderson-Darling test\nkstest: Kolmogorov-Smirnov test\nchisquare: one-way chi-square test\nnormaltest: test for normality\n\n\nSince R has a richer collections of statistical functions, we can call R function from Python with rpy2. See, for example, a blog on this subject.\nFor example, fisher_exact can only handle 2x2 contingency tables. For contingency tables larger than 2x2, we can call fisher.text() from R through rpy2. See this StackOverflow post. Note that the . in function names and arguments are replaced with _.\n\nimport pandas as pd\nimport numpy as np\nimport rpy2.robjects.numpy2ri\nfrom rpy2.robjects.packages import importr\nrpy2.robjects.numpy2ri.activate()\n\nstats = importr('stats')\n\njan23 = pd.read_csv(\"data/nyc_crashes_202301.csv\")\njan23[\"injury\"] = np.where(jan23[\"NUMBER OF PERSONS INJURED\"] > 0, 1, 0)\nm = pd.crosstab(jan23[\"injury\"], jan23[\"BOROUGH\"])\nprint(m)\n\nres = stats.fisher_test(m.to_numpy(), simulate_p_value = True)\nres[0][0]\n\nLoading custom .Rprofile\n\n\nBOROUGH  BRONX  BROOKLYN  MANHATTAN  QUEENS  STATEN ISLAND\ninjury                                                    \n0          526       999        528     801            126\n1          295       654        283     515             69\n\n\n0.11094452773613193"
  },
  {
    "objectID": "stats.html#linear-model",
    "href": "stats.html#linear-model",
    "title": "8  Statistical Tests and Models",
    "section": "8.2 Linear Model",
    "text": "8.2 Linear Model\nLet’s simulate some data for illustrations.\n\nimport numpy as np\n\nnobs = 200\nncov = 5\nnp.random.seed(123)\nx = np.random.random((nobs, ncov)) # Uniform over [0, 1)\nbeta = np.repeat(1, ncov)\ny = 2 + np.dot(x, beta) + np.random.normal(size = nobs)\n\nCheck the shape of y:\n\ny.shape\n\n(200,)\n\n\nCheck the shape of x:\n\nx.shape\n\n(200, 5)\n\n\nThat is, the true linear regression model is \\[\ny = 2 + x_1 + x_2 + x_3 + x_4 + x_5 + \\epsilon.\n\\]\nA regression model for the observed data can be fitted as\n\nimport statsmodels.api as sma\nxmat = sma.add_constant(x)\nmymod = sma.OLS(y, xmat)\nmyfit = mymod.fit()\nmyfit.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:            y          R-squared:             0.309\n\n\n  Model:                   OLS         Adj. R-squared:        0.292\n\n\n  Method:             Least Squares    F-statistic:           17.38\n\n\n  Date:             Wed, 01 Mar 2023   Prob (F-statistic): 3.31e-14\n\n\n  Time:                 14:10:56       Log-Likelihood:      -272.91\n\n\n  No. Observations:         200        AIC:                   557.8\n\n\n  Df Residuals:             194        BIC:                   577.6\n\n\n  Df Model:                   5                                    \n\n\n  Covariance Type:      nonrobust                                  \n\n\n\n\n           coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  const     1.8754     0.282     6.656  0.000     1.320     2.431\n\n\n  x1        1.1703     0.248     4.723  0.000     0.682     1.659\n\n\n  x2        0.8988     0.235     3.825  0.000     0.435     1.362\n\n\n  x3        0.9784     0.238     4.114  0.000     0.509     1.448\n\n\n  x4        1.3418     0.250     5.367  0.000     0.849     1.835\n\n\n  x5        0.6027     0.239     2.519  0.013     0.131     1.075\n\n\n\n\n  Omnibus:        0.810   Durbin-Watson:         1.978\n\n\n  Prob(Omnibus):  0.667   Jarque-Bera (JB):      0.903\n\n\n  Skew:          -0.144   Prob(JB):              0.637\n\n\n  Kurtosis:       2.839   Cond. No.               8.31\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nQuestions to review:\n\nHow are the regression coefficients interpreted? Intercept?\nWhy does it make sense to center the covariates?\n\nNow we form a data frame with the variables\n\nimport pandas as pd\ndf = np.concatenate((y.reshape((nobs, 1)), x), axis = 1)\ndf = pd.DataFrame(data = df,\n                  columns = [\"y\"] + [\"x\" + str(i) for i in range(1,\n                  ncov + 1)])\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 200 entries, 0 to 199\nData columns (total 6 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   y       200 non-null    float64\n 1   x1      200 non-null    float64\n 2   x2      200 non-null    float64\n 3   x3      200 non-null    float64\n 4   x4      200 non-null    float64\n 5   x5      200 non-null    float64\ndtypes: float64(6)\nmemory usage: 9.5 KB\n\n\nLet’s use a formula to specify the regression model as in R, and fit a robust linear model (rlm) instead of OLS. Note that the model specification and the function interface is similar to R.\n\nimport statsmodels.formula.api as smf\nmymod = smf.rlm(formula = \"y ~ x1 + x2 + x3 + x4 + x5\", data = df)\nmyfit = mymod.fit()\nmyfit.summary()\n\n\n\nRobust linear Model Regression Results\n\n  Dep. Variable:          y          No. Observations:      200\n\n\n  Model:                 RLM         Df Residuals:          194\n\n\n  Method:               IRLS         Df Model:                5\n\n\n  Norm:                HuberT                                  \n\n\n  Scale Est.:            mad                                   \n\n\n  Cov Type:              H1                                    \n\n\n  Date:           Wed, 01 Mar 2023                             \n\n\n  Time:               14:10:56                                 \n\n\n  No. Iterations:        16                                    \n\n\n\n\n               coef     std err      z      P>|z|  [0.025    0.975]  \n\n\n  Intercept     1.8353     0.294     6.246  0.000     1.259     2.411\n\n\n  x1            1.1254     0.258     4.355  0.000     0.619     1.632\n\n\n  x2            0.9664     0.245     3.944  0.000     0.486     1.447\n\n\n  x3            0.9995     0.248     4.029  0.000     0.513     1.486\n\n\n  x4            1.3275     0.261     5.091  0.000     0.816     1.839\n\n\n  x5            0.6768     0.250     2.712  0.007     0.188     1.166\n\nIf the model instance has been used for another fit with different fit parameters, then the fit options might not be the correct ones anymore .\n\n\nFor model diagnostics, one can check residual plots.\n\nimport matplotlib.pyplot as plt\n\nmyOlsFit = smf.ols(formula = \"y ~ x1 + x2 + x3 + x4 + x5\", data = df).fit()\nfig = plt.figure(figsize = (6, 6))\n## residual versus x1; can do the same for other covariates\nfig = sma.graphics.plot_regress_exog(myOlsFit, 'x1', fig=fig)\n\neval_env: 1\n\n\n\n\n\nSee more on residual diagnostics and specification tests."
  },
  {
    "objectID": "stats.html#generalized-linear-regression",
    "href": "stats.html#generalized-linear-regression",
    "title": "8  Statistical Tests and Models",
    "section": "8.3 Generalized Linear Regression",
    "text": "8.3 Generalized Linear Regression\nA linear regression model cannot be applied to presence/absence or count data.\nBinary or count data need to be modeled under a generlized framework. Consider a binary or count variable \\(Y\\) with possible covariates \\(X\\). A generalized model describes a transformation \\(g\\) of the conditional mean \\(E[Y | X]\\) by a linear predictor \\(X^{\\top}\\beta\\). That is \\[\ng( E[Y | X] ) = X^{\\top} \\beta.\n\\] The transformation \\(g\\) is known as the link function.\nFor logistic regression with binary outcomes, the link function is the logit function \\[\ng(u) = \\log \\frac{u}{1 - u}, \\quad u \\in (0, 1).\n\\]\nWhat is the interpretation of the regression coefficients in a logistic regression? Intercept?\nA logistic regression can be fit with statsmodels.api.glm.\nLet’s generate some binary data first by dichotomizing existing variables.\n\nimport statsmodels.genmod as smg\neta = x.dot([2, 2, 2, 2, 2]) - 5\np = smg.families.links.Logit().inverse(eta)\ndf[\"yb\"] = np.random.binomial(1, p, p.size)\n\nFit a logistic regression for y1b.\n\nmylogistic = smf.glm(formula = 'yb ~ x1 + x2 + x3 + x4 + x5', data = df,\n                     family = smg.families.Binomial())\nmylfit = mylogistic.fit()\nmylfit.summary()\n\n\n\nGeneralized Linear Model Regression Results\n\n  Dep. Variable:          yb          No. Observations:       200 \n\n\n  Model:                  GLM         Df Residuals:           194 \n\n\n  Model Family:        Binomial       Df Model:                 5 \n\n\n  Link Function:         Logit        Scale:                1.0000\n\n\n  Method:                IRLS         Log-Likelihood:      -110.80\n\n\n  Date:            Wed, 01 Mar 2023   Deviance:             221.61\n\n\n  Time:                14:10:57       Pearson chi2:          196. \n\n\n  No. Iterations:          4          Pseudo R-squ. (CS):  0.2410 \n\n\n  Covariance Type:     nonrobust                                  \n\n\n\n\n               coef     std err      z      P>|z|  [0.025    0.975]  \n\n\n  Intercept    -4.6982     0.820    -5.728  0.000    -6.306    -3.091\n\n\n  x1            2.1654     0.618     3.505  0.000     0.955     3.376\n\n\n  x2            1.1788     0.579     2.035  0.042     0.044     2.314\n\n\n  x3            1.5921     0.587     2.713  0.007     0.442     2.742\n\n\n  x4            2.2310     0.630     3.539  0.000     0.995     3.466\n\n\n  x5            2.5496     0.598     4.263  0.000     1.377     3.722\n\n\n\n\nIf we treat y1b as count data, a Poisson regression can be fitted.\n\nmyPois = smf.glm(formula = 'yb ~ x1 + x2 + x3 + x4 + x5', data = df,\n                 family = smg.families.Poisson())\nmypfit = myPois.fit()\nmypfit.summary()\n\n\n\nGeneralized Linear Model Regression Results\n\n  Dep. Variable:          yb          No. Observations:       200 \n\n\n  Model:                  GLM         Df Residuals:           194 \n\n\n  Model Family:         Poisson       Df Model:                 5 \n\n\n  Link Function:          Log         Scale:                1.0000\n\n\n  Method:                IRLS         Log-Likelihood:      -161.33\n\n\n  Date:            Wed, 01 Mar 2023   Deviance:             112.66\n\n\n  Time:                14:10:57       Pearson chi2:          89.6 \n\n\n  No. Iterations:          5          Pseudo R-squ. (CS):  0.1071 \n\n\n  Covariance Type:     nonrobust                                  \n\n\n\n\n               coef     std err      z      P>|z|  [0.025    0.975]  \n\n\n  Intercept    -2.4529     0.435    -5.644  0.000    -3.305    -1.601\n\n\n  x1            0.7638     0.361     2.117  0.034     0.057     1.471\n\n\n  x2            0.3698     0.330     1.121  0.262    -0.277     1.017\n\n\n  x3            0.5396     0.345     1.562  0.118    -0.137     1.217\n\n\n  x4            0.7895     0.368     2.144  0.032     0.068     1.511\n\n\n  x5            0.9477     0.352     2.691  0.007     0.257     1.638"
  },
  {
    "objectID": "visual.html#matplotlib",
    "href": "visual.html#matplotlib",
    "title": "9  Visualization",
    "section": "9.1 Matplotlib",
    "text": "9.1 Matplotlib\nThe matplotlib library can provide methods in plotting and arranging data visually in order to help viewers understand the main concepts of the data analysis. In this chapter, a progression of graphs will be shown to demonstrate some of the capabilities the library has to graph and plot data.\nThere are several types of graphs that can be used, such as:\n\nScatterplot\nLine plot\n3D plot\n\nThe library can be installed using either pip or conda. For example:\n\n# pip install matplotlib\n\n\n9.1.1 Usage\nLet’s start with a simple scatter plot. We would need to import the libraries as shown. For this example, we use the pyplot submodule, abbreviated to plt. We will use randomly generated data in 3 dimensions (x,y,z).\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(8465);\n\nx = np.random.uniform(0, 3, 10);\ny = np.random.uniform(0, 3, 10);\nz = np.random.uniform(0, 3, 10);\n\nplt.scatter(x, y)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.show()\n\n\n\n\nWe could start plotting another plot, but we have not saved our scatterplot as an object. Thus, it will get overridden by whatever we plot next. If we want to keep a plot, we can save as a figure object. In addition, if we need multiple plots together, we can use a subplot shown as follows.\n\nfigure, (fig1, fig2) = plt.subplots(1, 2, figsize = (8, 6))\n\nfig1.scatter(y, z, marker = '^')\nfig2.scatter(x, y, color = 'red')\n\nplt.show()\n\n\n\n\nWe can also do 3d plots alongside 2d, but we need a different function in order to do so. The following uses 3d axes to plot the scatterplot.\n\nfigure = plt.figure()\n\n# Make 3D axes for fig1\n\nfig2 = figure.add_subplot(122, projection = '3d')\nfig1 = figure.add_subplot(121)\n\n# Plot\n\nfig1.plot(x, z, label = \"Line Graph\")\nfig2.scatter(x, y, z, c = z, cmap = 'cool', label = \"Scatter in 3D\")\nfig1.legend()\nfig2.legend()\n\nplt.show()\n\n\n\n\n\n\n9.1.2 Animation (to be completed)\nAnimations can also be done through matplotlib. This requires the use of the animation submodule which has a variety functions that can be used to plot animations. Inputs required include the frames and other functions needed to update the plots per frame.\n\nimport matplotlib.animation as animation\n\ndef updatept(self):\n    z = 10;\n\nWe can use the FuncAnimation(args, updatept(), frames) to update.\n\n\n9.1.3 Conclusion\nWe have demonstrated some capabilities of the matplotlib library but more complex methods of plotting and arranging visual elements can be found in the documentation."
  },
  {
    "objectID": "visual.html#gg-plot-with-plotnine",
    "href": "visual.html#gg-plot-with-plotnine",
    "title": "9  Visualization",
    "section": "9.2 GG-Plot with Plotnine",
    "text": "9.2 GG-Plot with Plotnine\nThe plotnine package facilitates the creation of highly-informative plots of structured data based on the R implementation of ggplot2. The plotnine package is built on the top of Matplotlib and interacts well with Pandas.\n\n9.2.1 Installation\nWe need to install the package from our command before we start to use it.\nUsing pip:\npip install plotnine        \npip install plotnine[all]  # For the whole package of Plotnine\nOr using conda:\nconda install -c conda-forge plotnine`\n\n\n9.2.2 Import\nNow we can call plotnine in our python code\n\nimport plotnine as p9\nfrom plotnine import *\nfrom plotnine.data import *\n\n\n\n9.2.3 Some fundimental plots via plotnine\nActually there are plenty plots that Plotnine can make, but because of the time limitation we will only introduce these four\n\nBar Chart\nScatter Plot\nHistogram\nBox Plot\n\nExamples will be illustrated with the new york crash dataset, and since the dataset is too large, I will extract the first 50 crashes to do the illustration:\n\nimport pandas as pd\nimport numpy as np\ndf = pd.read_csv('data/nyc_crashes_202301.csv')\ndf1 = df.head(50)\ndf1.head()\n\n\n\n\n\n  \n    \n      \n      CRASH DATE\n      CRASH TIME\n      BOROUGH\n      ZIP CODE\n      LATITUDE\n      LONGITUDE\n      LOCATION\n      ON STREET NAME\n      CROSS STREET NAME\n      OFF STREET NAME\n      ...\n      CONTRIBUTING FACTOR VEHICLE 2\n      CONTRIBUTING FACTOR VEHICLE 3\n      CONTRIBUTING FACTOR VEHICLE 4\n      CONTRIBUTING FACTOR VEHICLE 5\n      COLLISION_ID\n      VEHICLE TYPE CODE 1\n      VEHICLE TYPE CODE 2\n      VEHICLE TYPE CODE 3\n      VEHICLE TYPE CODE 4\n      VEHICLE TYPE CODE 5\n    \n  \n  \n    \n      0\n      01/01/2023\n      14:38\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      BROOKLYN QUEENS EXPRESSWAY RAMP\n      NaN\n      NaN\n      ...\n      Driver Inattention/Distraction\n      Driver Inattention/Distraction\n      NaN\n      NaN\n      4594563\n      Sedan\n      Sedan\n      Sedan\n      NaN\n      NaN\n    \n    \n      1\n      01/01/2023\n      8:04\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NASSAU EXPRESSWAY\n      NaN\n      NaN\n      ...\n      Unspecified\n      NaN\n      NaN\n      NaN\n      4594599\n      Sedan\n      Sedan\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      01/01/2023\n      18:05\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      10 AVENUE\n      11 AVENUE\n      NaN\n      ...\n      Unspecified\n      NaN\n      NaN\n      NaN\n      4594810\n      Sedan\n      Taxi\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      01/01/2023\n      23:45\n      NaN\n      NaN\n      40.769737\n      -73.91244\n      (40.769737, -73.91244)\n      ASTORIA BOULEVARD\n      37 STREET\n      NaN\n      ...\n      Driver Inattention/Distraction\n      NaN\n      NaN\n      NaN\n      4594595\n      Taxi\n      Taxi\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      01/01/2023\n      4:50\n      NaN\n      NaN\n      40.830555\n      -73.85072\n      (40.830555, -73.85072)\n      CASTLE HILL AVENUE\n      EAST 177 STREET\n      NaN\n      ...\n      Unspecified\n      NaN\n      NaN\n      NaN\n      4594761\n      Station Wagon/Sport Utility Vehicle\n      Sedan\n      NaN\n      NaN\n      NaN\n    \n  \n\n5 rows × 29 columns\n\n\n\n\n9.2.3.1 Bar Chart\ngeom_bar(mapping=None, data=None, stat=‘count’, position=‘stack’, na_rm=False, inherit_aes=True, show_legend=None, raster=False, width=None, **kwargs)\nSuppose we are curious about the types of vehicle in the crash, we can make a bar chart to illlustrate that\n\n( # The brackets means print   \n    ggplot(df1)  # The data we are using\n    + geom_bar(aes(x = 'VEHICLE TYPE CODE 1') ) # The plot we want to make\n)\n\n\n\n\n<ggplot: (303974937)>\n\n\nSome improvement of the chart:\n\nBlack is too dreary! We want to make this graph more vivid and fancy(maybe by adding color)\nIn here the words in x axis are really hard to see, so we might make some arrangement for the angle of these words\nAnd also, we want to have a title for the graph, and maybe change the label for axis\nSometimes we may want the spesific counts for the bars – by adding a label\nSuppose we want to fliped the data to verticle – we can do that too\n\n\n(\n    ggplot(df1, # The dataset we are using\n           aes(x = 'VEHICLE TYPE CODE 1', fill='VEHICLE TYPE CODE 1'))  # x is the specific column in the dataset we are using, 'fill' color the columns of Vehicle Type Code 1\"\n    + geom_bar() # The plot we want to make\n    + theme(axis_text_x=element_text(angle=75)) #We want the text to have an angle\n    + ggtitle('Vehicle Counts') # Make a title for the chart\n    + xlab(\"Vehicle_Type\") # Change x lable of the graph\n    + ylab(\"Count\") # Change y lable of the graph\n   #+ coord_flip() # Flipped the data to verticle\n)\n\n\n\n\n<ggplot: (303541745)>\n\n\n\n\n9.2.3.2 Scatter Plot\ngeom_point(mapping=None, data=None, stat=‘identity’, position=‘identity’, na_rm=False, inherit_aes=True, show_legend=None, raster=False, **kwargs)\nSuppose we are curious about the place where Crashes happend, we may do a scatter plot for the longitude and latitude\n\n(\n    ggplot(df1, #The dataset we are using\n       aes(x = 'LONGITUDE', y='LATITUDE')) # Make x and y axis\n        + geom_point() # Fill the points inside the graph\n       #+ geom_smooth(method = 'lm') # It is senseless to do this in here but this is the way we fit a line for scatter plots\n)\n\n/usr/local/lib/python3.11/site-packages/plotnine/layer.py:411: PlotnineWarning: geom_point : Removed 7 rows containing missing values.\n\n\n\n\n\n<ggplot: (304061305)>\n\n\nSome Improvements: 1. Sometimes we might want to change the shape of the dot to something else\n\nWe might find the points are uniform, we may want to change the size of the points too\n\n\n(\n    ggplot(df1, # The dataset we are using\n        aes(x = 'LONGITUDE', y='LATITUDE', size = 'LATITUDE')) # Make x and y axis, and make point size by latitude\n        + geom_point( # Fill the point inside the graph\n         aes(shape='VEHICLE TYPE CODE 1')) # Change the shape of the dots according to Vehicle Type\n)\n\n/usr/local/lib/python3.11/site-packages/plotnine/layer.py:411: PlotnineWarning: geom_point : Removed 8 rows containing missing values.\n/usr/local/lib/python3.11/site-packages/plotnine/guides/guides.py:253: PlotnineWarning: geom_point legend : Removed 1 rows containing missing values.\n\n\n\n\n\n<ggplot: (304056145)>\n\n\nThe Dataset might be too small to see the clustering, we might need to have a bigger one– with some clean up\nAnd also, we can anticipate that a lump of black dots is not beautiful– we might want to change its color to build something fancy!\n\ndf2 = df.head(10000) # A little bit data cleaning process\ndf2[\"LATITUDE\"] = df2[\"LATITUDE\"].replace([0.0], np.nan)\ndf2[\"LONGITUDE\"] = df2[\"LONGITUDE\"].replace([0.0], np.nan)\n\n(\n    ggplot(df2, # The dataset we are using\n        aes(x = 'LONGITUDE', y='LATITUDE', color = 'LATITUDE')) # We have our x as Longitude, y as latitude, and we colored the clusters by its latitude\n        + geom_point()\n        + scale_color_gradient(low='#10098f', high='#0ABAB5',guide='colorbar') # From low lattitude to high lattitude colors -- according to colorbar(p.s. Ultramarine and Tiffany blue, my favorites blue colors)         \n)\n\n/usr/local/lib/python3.11/site-packages/plotnine/layer.py:411: PlotnineWarning: geom_point : Removed 561 rows containing missing values.\n\n\n\n\n\n<ggplot: (303984633)>\n\n\n\n\n9.2.3.3 Histogram\ngeom_histogram(mapping=None, data=None, stat=‘bin’, position=‘stack’, na_rm=False, inherit_aes=True, show_legend=None, raster=False, **kwargs)\nI can not find a continuous variable in the NYC Car Crach dataset, so it might be better to import other dataset to do that\nIn here I will use a dataset plant in Python called diamonds\n\ndiamonds.head()\n\n\n\n\n\n  \n    \n      \n      carat\n      cut\n      color\n      clarity\n      depth\n      table\n      price\n      x\n      y\n      z\n    \n  \n  \n    \n      0\n      0.23\n      Ideal\n      E\n      SI2\n      61.5\n      55.0\n      326\n      3.95\n      3.98\n      2.43\n    \n    \n      1\n      0.21\n      Premium\n      E\n      SI1\n      59.8\n      61.0\n      326\n      3.89\n      3.84\n      2.31\n    \n    \n      2\n      0.23\n      Good\n      E\n      VS1\n      56.9\n      65.0\n      327\n      4.05\n      4.07\n      2.31\n    \n    \n      3\n      0.29\n      Premium\n      I\n      VS2\n      62.4\n      58.0\n      334\n      4.20\n      4.23\n      2.63\n    \n    \n      4\n      0.31\n      Good\n      J\n      SI2\n      63.3\n      58.0\n      335\n      4.34\n      4.35\n      2.75\n    \n  \n\n\n\n\nSuppose we are curious about the carats of these diamonds, we can make a histogram for that\n\n(\n    ggplot(diamonds, # The dataset we are using\n           aes(x='carat')) # The data column we are using\n    + geom_histogram() # We want to do a histogram\n)\n\n/usr/local/lib/python3.11/site-packages/plotnine/stats/stat_bin.py:95: PlotnineWarning: 'stat_bin()' using 'bins = 142'. Pick better value with 'binwidth'.\n\n\n\n\n\n<ggplot: (304258913)>\n\n\nSome Improvements:\n\nWe can make the graph look nicer by defining the number of bins and bins’ width, this graph waste too much places\nWhen we dealing with this data, we might find out that the count is way too large, so we might want to do some normalization to a number that closer to the number of carat(1 maybe)\nSometimes we might want to see the proportion of the graph, we can handle that by some improvements\nWe can also filled the color of the gram with some other variables to see other characristics of these variables, for example, we might curious about the quality of cut of each diamonds\n\n\n(\n    ggplot(diamonds, aes(x = 'carat',\n                        #y = after_stat('count'), # Specify each bin is a count\n                        #y = after_stat('ncount'), # Normalise the count to 1\n                        #y = after_stat('density'), # Density\n                        #y = after_stat('width*density'), # Do some little calculation\n                      fill = 'cut'))  # Filled color by variable'cut'\n    + geom_histogram(binwidth= 0.5) # Change the width of the bin\n)\n\n\n\n\n<ggplot: (304357769)>\n\n\nWe can even make the plot more fancy by its own theme!\n\n(\n    ggplot(diamonds, aes(x = 'carat',\n                      y = after_stat('count'), # Specify each bin is a count\n                     #y = after_stat('ncount'), # Normalise the count to 1\n                     #y = after_stat('density'), # Density\n                     #y = after_stat('width*density')), # Show proportion \n                      fill = 'cut'))  # Filled color by variable'cut'\n    + geom_histogram(binwidth= 0.50) # Change the width of the bin\n   #+ theme_xkcd() # Add a theme to makes it better!\n   #+ theme(rect=element_rect(color='black', size=3, fill='#EEBB0050')) # An example of customize a theme\n    + theme(\n    panel_grid=element_line(color='purple'),\n    panel_grid_major=element_line(size=1.4, alpha=1),\n    panel_grid_major_x=element_line(linetype='dashed'),\n    panel_grid_major_y=element_line(linetype='dashdot'),\n    panel_grid_minor=element_line(alpha=.25),\n    panel_grid_minor_x=element_line(color='red'),\n    panel_grid_minor_y=element_line(color='green'),\n    panel_ontop=False  # Put the points behind the grid\n )\n)\n\n\n\n\n<ggplot: (304281633)>\n\n\n\n\n9.2.3.4 Boxplot\nBack to the NYC Crash Data, suppose we want to analysis the relationship among numbers of persons injured and borough, we might build a boxplot to see that\n\n(\n    ggplot(df1, # The data we are using\n             aes(\"BOROUGH\" , \"NUMBER OF PERSONS INJURED\")) # We define our axis\n    + geom_boxplot() # The plot we are using\n)\n\n\n\n\n<ggplot: (304229633)>\n\n\nSome Improvements:\n\nAdd a title to the plot, change the title of x and y axis\nWe may want to change the color of the boxes..? Sometimes?\nWe can change the theme of the plot\nSometimes we may want to see all the points of the boxplot, we can do that with plotnine\n\n\n(\n    ggplot(df1, # The data we are using\n             aes(\"BOROUGH\" , \"NUMBER OF PERSONS INJURED\")) \n    + geom_boxplot(color = \"#0437F2\") # The plot we are using, and change the color in here\n    + xlab(\"Borough\") # Change the title of x axis\n    + ylab(\"Number of persons injured\") # Change the title of y axis\n    + ggtitle(\"Person Injured within each borough\") # Add a title for the graph\n    + theme_bw() # Maybe we can add a theme sometimes?\n   #+ geom_jitter() # This function can add all the points of the boxplot\n)\n\n\n\n\n<ggplot: (301913469)>\n\n\n\n\n\n9.2.4 Sub Graphs\nAs any other library supporting the Grammar of Graphics, plotnine has a special technique called facet that allows to split one plot into multiple plots based on a factor variable included in the dataset\nFor the sub graphs plotnine we are going to talk about two important grammar– facet_wrap and facet_grid\nThe examples will be illustrated via diamonds dataset\n\ndiamonds.head()\n\n\n\n\n\n  \n    \n      \n      carat\n      cut\n      color\n      clarity\n      depth\n      table\n      price\n      x\n      y\n      z\n    \n  \n  \n    \n      0\n      0.23\n      Ideal\n      E\n      SI2\n      61.5\n      55.0\n      326\n      3.95\n      3.98\n      2.43\n    \n    \n      1\n      0.21\n      Premium\n      E\n      SI1\n      59.8\n      61.0\n      326\n      3.89\n      3.84\n      2.31\n    \n    \n      2\n      0.23\n      Good\n      E\n      VS1\n      56.9\n      65.0\n      327\n      4.05\n      4.07\n      2.31\n    \n    \n      3\n      0.29\n      Premium\n      I\n      VS2\n      62.4\n      58.0\n      334\n      4.20\n      4.23\n      2.63\n    \n    \n      4\n      0.31\n      Good\n      J\n      SI2\n      63.3\n      58.0\n      335\n      4.34\n      4.35\n      2.75\n    \n  \n\n\n\n\n\n9.2.4.1 facet_wrap\nplotnine.facets.facet_wrap(facets=None, nrow=None, ncol=None, scales=‘fixed’, shrink=True, labeller=‘label_value’, as_table=True, drop=True, dir=‘h’)\nSometimes we might want to see a lot of charts inside one large one, we can also do this within Facet_wrap\nFor example, in the diamond dataset, Suppose we are curious about the carat vs. price graphs for each levels of cut, we can do a plot like that\n\n(\n    ggplot(diamonds, aes(x = 'carat', y = 'price'))\n    + geom_point(color = '#4EE2EC') # Diamond blue!\n    + labs(x='carat', y='price')\n   #+ facet_wrap('cut', # Distinguish the levels of cut within the plot of carat vs. price\n                 #ncol = 2) # Change the number of columns\n)\n\n\n\n\n<ggplot: (304464869)>\n\n\n\n\n9.2.4.2 Facet_grid\nplotnine.facets.facet_grid(facets, margins=False, scales=‘fixed’, space=‘fixed’, shrink=True, labeller=‘label_value’, as_table=True, drop=True)\nSometimes we may want to see the facets with more than one variables, we can use Facet_grid\nIn this case, suppose we are curious about the graphs of carat vs. price for each levels of cut and clarity\n\n(\n    ggplot(diamonds, aes(x='carat', y='price', \n                         color = 'depth' # If we want to see another dimension of data, we might use color to illustrate that\n                        ))\n    + geom_point() \n    + labs(x='carat', y='price')\n   #+ facet_grid('cut ~ clarity') # Cut levels at right and clarities at top\n   #+ facet_grid('cut ~ .') # Cut levels only, at top\n   #+ facet_grid('. ~ clarity') # Clarities only, at right\n    + scale_color_gradient(low='#10098f', high='#0ABAB5',guide='colorbar') #The color will represent depth, from low to high by light to dense of the color\n)\n\n\n\n\n<ggplot: (304096745)>\n\n\nWe can also seperate this two-dimensional plot to one dimensional by list all the posible combinations of these characters on the side\nIn this case we can use facet_grid to generate those plots\nAnd also, we might be interested in the trend of these variables, so we may estiamte a linear regression for them\n\n(\n    ggplot(diamonds, aes(x='carat', y='price')) # The plot we want to make\n    + geom_point()\n   #+ geom_smooth() # Estimate Linear Regression\n    + facet_grid('cut+clarity ~ .') # We want to see the carat vs. price data seperated by cut+clarity\n    + theme(strip_text_y = element_text(angle = 0,              # Change facet text angle\n                                        ha = 'left'             # Change text alignment\n                                       ),\n            strip_background_y = element_text(color = '#cfe4ee' # Change background colour of facet background, in this case-- diamond blue!\n                                              , width = 0.2     # Adjust width of facet background to fit facet text\n                                             ),\n            figure_size=(12, 30)                                 # Adjust width & height of figure to fit y-axis\n           )\n)\n\n\n\n\n<ggplot: (304162821)>\n\n\n\n\n\n9.2.5 Some useful resources\nLearn all you need to know about Plotnine via its own website: https://plotnine.readthedocs.io/en/stable/index.html.\nIn case you are interested in data visualization via python, check out this website! https://pythonplot.com/.\nAnd finally there is a useful data visualization Github I found, read it if you are interested! https://github.com/pmaji/practical-python-data-viz-guide."
  },
  {
    "objectID": "supervised.html#introduction",
    "href": "supervised.html#introduction",
    "title": "10  Supervised Learning",
    "section": "10.1 Introduction",
    "text": "10.1 Introduction\nSupervised learning uses labeled datasets to train algorithms that to classify data or predict outcomes accurately. As input data is fed into the model, it adjusts its weights until the model has been fitted appropriately, which occurs as part of the cross validation process.\nIn contrast, unsupervised learning uses unlabeled data to discover patterns that help solve for clustering or association problems. This is particularly useful when subject matter experts are unsure of common properties within a data set."
  },
  {
    "objectID": "supervised.html#classification-vs-regression",
    "href": "supervised.html#classification-vs-regression",
    "title": "10  Supervised Learning",
    "section": "10.2 Classification vs Regression",
    "text": "10.2 Classification vs Regression\n\nClassificaiton: outcome variable is categorical\nRegression: outcome variable is continuous\nBoth problems can have many covariates (predictors/features)\n\n\n10.2.1 Regression metrics\n\nMean squared error (MSE)\nMean absolute error (MAE)\n\n\n\n10.2.2 Classification metrics\n\n10.2.2.1 Confusion matrix\nhttps://en.wikipedia.org/wiki/Confusion_matrix\nFour entries in the confusion matrix:\n\nTP: number of true positives\nFN: number of false negatives\nFP: number of false positives\nTN: number of true negatives\n\nFour rates from the confusion matrix with actual (row) margins:\n\nTPR: TP / (TP + FN). Also known as sensitivity.\nFNR: TN / (TP + FN). Also known as miss rate.\nFPR: FP / (FP + TN). Also known as false alarm, fall-out.\nTNR: TN / (FP + TN). Also known as specificity.\n\nNote that TPR and FPR do not add up to one. Neither do FNR and FPR.\nFour rates from the confusion matrix with predicted (column) margins:\n\nPPV: TP / (TP + FP). Also known as precision.\nFDR: FP / (TP + FP).\nFOR: FN / (FN + TN).\nNPV: TN / (FN + TN).\n\n\n\n10.2.2.2 Measure of classification performance\nMeasures for a given confusion matrix:\n\nAccuracy: (TP + TN) / (P + N). The proportion of all corrected predictions. Not good for highly imbalanced data.\nRecall (sensitivity/TPR): TP / (TP + FN). Intuitively, the ability of the classifier to find all the positive samples.\nPrecision: TP / (TP + FP). Intuitively, the ability of the classifier not to label as positive a sample that is negative.\nF-beta score: Harmonic mean of precision and recall with \\(\\beta\\) chosen such that recall is considered \\(\\beta\\) times as important as precision, \\[\n(1 + \\beta^2) \\frac{\\text{precision} \\cdot \\text{recall}}\n{\\beta^2 \\text{precision} + \\text{recall}}\n\\] See stackexchange post for the motivation of \\(\\beta^2\\).\n\nWhen classification is obtained by dichotomizing a continuous score, the receiver operating characteristic (ROC) curve gives a graphical summary of the FPR and TPR for all thresholds. The ROC curve plots the TPR against the FPR at all thresholds.\n\nIncreasing from \\((0, 0)\\) to \\((1, 1)\\).\nBest classification passes \\((0, 1)\\).\nClassification by random guess gives the 45-degree line.\nArea between the ROC and the 45-degree line is the Gini coefficient, a measure of inequality.\nArea under the curve (AUC) of ROC thus provides an important metric of classification results.\n\n\n\n\n10.2.3 Cross-validation\n\nGoal: strike a bias-variance tradeoff.\nK-fold: hold out each fold as testing data.\nScores: minimized to train a model"
  },
  {
    "objectID": "supervised.html#support-vector-machines",
    "href": "supervised.html#support-vector-machines",
    "title": "10  Supervised Learning",
    "section": "10.3 Support Vector Machines",
    "text": "10.3 Support Vector Machines\n\n10.3.1 Introduction\nSupport Vector Machine (SVM) is a type of suppervised learning models that can be used to analyze classification and regression. In this section will develop the intuition behind support vector machines and provide some examples.\n\n\n10.3.2 Package that need to install\nBefore we begin ensure that these this package are installed in your python\npip install scikit-learn\nScikit-learn is a python package that provides efficient versions of a large number of common algorithms It constist of all type of machine learning model which is wildly known such as:\n\nLinear Regression\nLogistic Regression\nDecision Trees\nGaussian Process\n\nFurthermore, it also provide function that can be used anytime and use it on the provided machine learning algorithm. There are two type of functions:\n\nAvalable dataset functions such as Iris dataset load_iris\nRandomly generated datasets function such as make_moon , make_circle etc.\n\n\n\n10.3.3 Support Vector Classifier\nBefore we get into SVM , let us take a look at this simple classification problem. Consider a distinguishable datasets\n\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nseed = 220\n\nfrom sklearn.datasets import make_blobs\nX, y = make_blobs(n_samples=50, centers=2,\n                  random_state= seed, cluster_std=1)\nplt.scatter(X[:, 0], X[:, 1], c=y, s=50);\n\n\n\n\nOne of the solution we can do is to draw lines as a way to seperate these two classes.\n\ndef xfit(m,b):\n    t = np.linspace(-5,5,50)\n    y = m*t + b\n\n    return y\n\nX, y = make_blobs(n_samples=50, centers=2,\n                  random_state= 220, cluster_std=1)\n    \nax = plt.gca()\nax.scatter(X[:, 0], X[:, 1], c=y, s=50)\nt = np.linspace(-5,5,50)\ny1 = xfit(7,-5)\ny2 = xfit(15,9)\ny3 = xfit(-5,-4)\nax.plot(t,y1,label = 'Line 1')\nax.plot(t,y2,label = 'Line 2')\nax.plot(t,y3,label = 'Line 3')\nax.set_xlim(-5, 5)\nax.set_ylim(-7, 0)\nax.legend();\n\n\n\n\nHow do we find the best line that divide them both? In other word we need to find the optimal line or best decision boundary.\nLets import Support Vector Machine module for now to help us find the best line to classify the data set.\n\nfrom sklearn.svm import SVC # \"Support vector classifier\"\nmodel = SVC(kernel='linear', C=1E10)\n# For now lets not think about the purpose of C\nmodel.fit(X, y)\n\nax = plt.gca()\nax.set_xlim(-5, 5)\nax.set_ylim(-7, 0)\n\nxlim = ax.get_xlim()\nylim = ax.get_ylim()\n\n# Create a mesh grid\nx_grid = np.linspace(xlim[0], xlim[1], 30)\ny_grid = np.linspace(ylim[0], ylim[1], 30)\nY_mesh, X_mesh = np.meshgrid(y_grid,x_grid)\nxy = np.vstack([X_mesh.ravel(),Y_mesh.ravel()]).T\n\nP = model.decision_function(xy).reshape(X_mesh.shape)\n\nax.contour(X_mesh, Y_mesh, P, colors='k',\n               levels=[-1, 0 ,1], alpha=0.5,\n               linestyles= ['--','-','--']);\n\nax.scatter(X[:, 0], X[:, 1], c=y, s=50);\n\nax.scatter(model.support_vectors_[:, 0],\n                   model.support_vectors_[:, 1],\n                   s=300, linewidth=2, facecolor ='none', edgecolor = 'black');\n\n\n\n\nThere is a name for this line. Is called margin, it is the shortest distance between the selected observation and the line. In this case we are using the largest margin to seperate the observation. We called it Maximal Margin Classifier.\nThe selected observation (circled points) are called Support Vectors. For simple explaination, it is the points that used to create the margin.\nWhat if we have a weird observation as shown below? What happend if we try to use Maximal Margin Classifier? Lets add a point on an interesting location.\n\n# Addiing a point near yellow side and name it blue\nfrom sklearn.datasets import make_blobs\nX, y = make_blobs(n_samples=50, centers=2,\n                  random_state= 220, cluster_std=1)\n\nX_new = [2, -4]\n\nX = np.vstack([X,X_new])\n\ny_new = np.array([1]).reshape(1)\n\ny = np.append(y, [0], axis=0)\n\nax = plt.subplot()\nax.scatter(X[:, 0], X[:, 1], c=y, s=51);\n\n\n\n\nUsing Maximum Margin Classifier\n\nmodel = SVC(kernel='linear', C=1E10)\nmodel.fit(X, y)\n\nax = plt.gca()\nax.set_xlim(-5, 5)\nax.set_ylim(-7, 0)\n\nxlim = ax.get_xlim()\nylim = ax.get_ylim()\n\n# Create a mesh grid\nx_grid = np.linspace(xlim[0], xlim[1], 30)\ny_grid = np.linspace(ylim[0], ylim[1], 30)\nY_mesh, X_mesh = np.meshgrid(y_grid,x_grid)\nxy = np.vstack([X_mesh.ravel(),Y_mesh.ravel()]).T\n\nP = model.decision_function(xy).reshape(X_mesh.shape)\n\nax.contour(X_mesh, Y_mesh, P, colors='k',\n               levels=[-1, 0 ,1], alpha=0.5,\n               linestyles= ['--','-','--']);\n\nax.scatter(X[:, 0], X[:, 1], c=y, s=50);\n\n\n\n\nAs you can see Maximal Margin Classifier might not be a useful in this case. We must make the margin that is not sensitve to outliers and allow a few misclassifications. So we need to implement Soft Margin to get a better prediction. This is where parameter C comes in.\n\n# New fit with modifiying the C\n\nmodel = SVC(kernel='linear', C=0.1)\nmodel.fit(X, y)\n\nax = plt.gca()\nax.set_xlim(-5, 5)\nax.set_ylim(-7, 0)\n\nxlim = ax.get_xlim()\nylim = ax.get_ylim()\n\n# Create a mesh grid\nx_grid = np.linspace(xlim[0], xlim[1], 30)\ny_grid = np.linspace(ylim[0], ylim[1], 30)\nY_mesh, X_mesh = np.meshgrid(y_grid,x_grid)\nxy = np.vstack([X_mesh.ravel(),Y_mesh.ravel()]).T\n\nP = model.decision_function(xy).reshape(X_mesh.shape)\n\nax.contour(X_mesh, Y_mesh, P, colors='k',\n               levels=0, alpha=0.5, linestyles= '-');\n\nax.scatter(X[:, 0], X[:, 1], c=y, s=50);\n\n\n\n\nIncreasing the parameter C will greatly influence the classification line location\n\nX, y = make_blobs(n_samples=100, centers=2,\n                  random_state=0, cluster_std=1.2)\n\nfig, ax = plt.subplots(1, 2)\nfig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n\nfor axi, C in zip(ax, [100.0, 0.1]):\n    model = SVC(kernel='linear', C=C).fit(X, y)\n    \n\n    axi.set_xlim(-3, 6)\n    axi.set_ylim(-2, 7)\n\n    xlim = axi.get_xlim()\n    ylim = axi.get_ylim()\n\n    # Create a mesh grid\n    x_grid = np.linspace(xlim[0], xlim[1], 30)\n    y_grid = np.linspace(ylim[0], ylim[1], 30)\n    Y_mesh, X_mesh = np.meshgrid(y_grid,x_grid)\n    xy = np.vstack([X_mesh.ravel(),Y_mesh.ravel()]).T\n\n    P = model.decision_function(xy).reshape(X_mesh.shape)\n\n    axi.contour(X_mesh, Y_mesh, P, colors='k',\n               levels=[-1,0,1], alpha=0.5, linestyles=['--','-','--']);\n\n    axi.scatter(X[:, 0], X[:, 1], c=y, s=50)\n    axi.set_title('C = {0:.1f}'.format(C), size=14)\n\n\n\n\n\n10.3.3.1 Support Vector Machine\nNow we have some basic understanding on classifiying thing, lets take a look at the sample problem below.\n\nfrom sklearn.datasets import make_circles\nX, y = make_circles(100, factor=.1, noise=.1)\n\nplt.scatter(X[:, 0], X[:, 1], c=y, s=50);\n\n\n\n\nIf we apply a standard Support Vector Classifier the result will be like this.\n\nclf = SVC(kernel='linear').fit(X, y)\n\nax = plt.gca()\nax.set_xlim(-1.5, 1.5)\nax.set_ylim(-1.5, 1.5)\n\nxlim = ax.get_xlim()\nylim = ax.get_ylim()\n\n# Create a mesh grid\nx_grid = np.linspace(xlim[0], xlim[1], 30)\ny_grid = np.linspace(ylim[0], ylim[1], 30)\nY_mesh, X_mesh = np.meshgrid(y_grid,x_grid)\nxy = np.vstack([X_mesh.ravel(),Y_mesh.ravel()]).T\n\nP = clf.decision_function(xy).reshape(X_mesh.shape)\n\nax.contour(X_mesh, Y_mesh, P, colors='k',\n               levels=0, alpha=0.5, linestyles= '-');\n\nax.scatter(X[:, 0], X[:, 1], c=y, s=50);\n\n\n\n\nThis is not a good classifier. We need a way to make it better. Instead of just using the available data, let us try to convert a data to a better dimension space.\n\nr = np.exp(-(X ** 2).sum(1))\n\nIn this case we will implement a kernel that will translate our data to a new diemension. This is one of the way to fit a nonlinear relationship with a linear classifier.\n\nax = plt.subplot(projection='3d')\nax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=50);\n#ax.view_init(elev=-90, azim=30)\nax.set_xlabel('x');\nax.set_ylabel('y');\nax.set_zlabel('r');\n\n\n\n\nNow you can see that it is seperated. We can apply the Support Vector Classifier to the dataset\n\nr = r.reshape(100,1)\n\nb = np.concatenate((X,r),1)\n\nfrom sklearn.svm import SVC \n\nclf = SVC(kernel='linear').fit(b, y)\n\nax = plt.gca()\nax.set_xlim(-1.5, 1.5)\nax.set_ylim(-1.5, 1.5)\n\nxlim = ax.get_xlim()\nylim = ax.get_ylim()\n\n# Create a mesh grid\nx_grid = np.linspace(xlim[0], xlim[1], 30)\ny_grid = np.linspace(ylim[0], ylim[1], 30)\nY_mesh, X_mesh = np.meshgrid(y_grid,x_grid)\nxy = np.vstack([X_mesh.ravel(),Y_mesh.ravel()]).T\n\nr_1 = np.exp(-(xy ** 2).sum(1))\n\nr_1 = r_1.reshape(900,1)\n\nb_1 = np.concatenate((xy,r_1),1)\n\nP = clf.decision_function(b_1).reshape(X_mesh.shape)\n\nax.contour(X_mesh, Y_mesh, P, colors='k',\n                levels=0, alpha=0.5, linestyles= '-');\n\nax.scatter(X[:, 0], X[:, 1], c=y, s=50);\n\n\n\n\nOr you can just use SVC radial basis fucntion kernel to automatically create a decision boundary for you.\n\nclf = SVC(kernel='rbf').fit(X, y)\n\nax = plt.gca()\nax.set_xlim(-1.5, 1.5)\nax.set_ylim(-1.5, 1.5)\n\nxlim = ax.get_xlim()\nylim = ax.get_ylim()\n\n# Create a mesh grid\nx_grid = np.linspace(xlim[0], xlim[1], 30)\ny_grid = np.linspace(ylim[0], ylim[1], 30)\nY_mesh, X_mesh = np.meshgrid(y_grid,x_grid)\nxy = np.vstack([X_mesh.ravel(),Y_mesh.ravel()]).T\n\nP = clf.decision_function(xy).reshape(X_mesh.shape)\n\nax.contour(X_mesh, Y_mesh, P, colors='k',\n               levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-','--']);\n\nax.scatter(X[:, 0], X[:, 1], c=y, s=50);\n\nax.scatter(clf.support_vectors_[:, 0],\n                   clf.support_vectors_[:, 1],\n                   s=300, linewidth=2, facecolor ='none', edgecolor = 'black');\n\n\n\n\nAs for summary, Support Vector Machine follow these steps:\n\nStart with a data in low dimension.\nUse kernel to move the data to a higher dimension.\nFind a Support Vector Classifier that seperate the data into two groups.\n\n\n\n10.3.3.2 Kernel\nLet talk more about the kernel. There are mutiple type of kernel. We will go through a few of them. Generaly, they call as a kernel trick or kernel method or kernel function. For simple explanation, these kernel can be view as a method on how we transform the data points into. It may need to transform to a higher dimension it may not.\n\nLinear Kernel The linear kernel is a kernel that uses the dot product of the input vectors to measure their similarity: \\[k(x,x')= (x\\cdot x')\\]\nPolynomial Kernel\n\nFor homogeneous case: \\[k(x,x')= (x\\cdot x')^d\\] where if \\(d = 1\\) it wil be act as linear kernel.\nFor inhomogeneous case: \\[k(x,x')= (x\\cdot x' + r )^d\\] where r is a coefficient.\n\nRadial Basis Function Kernel (or rbf) is a well know kernel that can transform the data to a infinite dimension space.\n\nThe function is known as:\n\\(k(x,x') = \\exp\\left(-\\gamma\\left\\Vert x-x' \\right\\Vert^2\\right)\\)\n\\(\\gamma >0\\). Sometimes parametrized using \\(\\gamma = \\frac{1}{2\\sigma^2}\\)\n\n\n10.3.3.3 Regression Problem\nWe will talk a little on Regression Problem and how it works on Support Vector Machine.\nLets consider a data output as shown below.\n\nfrom sklearn.datasets import make_regression\nimport matplotlib.pyplot as plt\n\nX, y = make_regression(n_samples=100, n_features=1, noise=10, random_state = 2220)\n\nplt.scatter(X, y, marker='o')\nplt.show()\n\n\n\n\nSo how Support Vector Machine works for regression problem? Instead of giving some math formulas. Let do a fit and show the output of the graph.\n\nfrom sklearn.svm import SVR\n\nmodel = SVR(kernel='linear', C = 100, epsilon = 10)\n\nmodel.fit(X, y)\n\nX_new = np.linspace(-3, 3, 100).reshape(-1, 1)\ny_pred = model.predict(X_new)\n\nplt.scatter(X, y, marker='o')\nplt.plot(X_new, y_pred, color='red')\nplt.plot(X_new, y_pred + model.epsilon, color='black', linestyle='--')\nplt.plot(X_new, y_pred - model.epsilon, color='black', linestyle='--')\nplt.show()\n\n\n\n\nAs you can see for regression problem Support Vector Machine for Regression or SVR create a two black lines as the decision boundary and the red line as the hyperplane. Our objective is to ensure points are within the boundary. The best fit line is the hyperplane that has a maximum number of points.\nYou can control the model by adjust the C value and epsilon value. C value change the slope of the line, lower the value will reduce the slope of the fit line. epsilon change the distance of the decision boundary, lower the epsilon reduce the distance of the dicision boundary.\n\n\n10.3.3.4 Example: Classification\nLet take a look at our NYC database. We would like to create a machine learning model with SVM.\n\nimport pandas as pd\n\njan23 = pd.read_csv(\"data/nyc_crashes_202301_cleaned.csv\")\njan23.head()\n\n\n\n\n\n  \n    \n      \n      CRASH DATE\n      CRASH TIME\n      BOROUGH\n      ZIP CODE\n      LATITUDE\n      LONGITUDE\n      LOCATION\n      ON STREET NAME\n      CROSS STREET NAME\n      OFF STREET NAME\n      ...\n      Unnamed: 30\n      Unnamed: 31\n      Unnamed: 32\n      Unnamed: 33\n      Unnamed: 34\n      Unnamed: 35\n      Unnamed: 36\n      Unnamed: 37\n      Unnamed: 38\n      Unnamed: 39\n    \n  \n  \n    \n      0\n      1/1/23\n      14:38\n      BROOKLYN\n      11211.0\n      40.719094\n      -73.946108\n      (40.7190938,-73.9461082)\n      BROOKLYN QUEENS EXPRESSWAY RAMP\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      1/1/23\n      8:04\n      QUEENS\n      11430.0\n      40.659508\n      -73.773687\n      (40.6595077,-73.7736867)\n      NASSAU EXPRESSWAY\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      1/1/23\n      18:05\n      MANHATTAN\n      10011.0\n      40.742454\n      -74.008686\n      (40.7424543,-74.008686)\n      10 AVENUE\n      11 AVENUE\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      1/1/23\n      23:45\n      QUEENS\n      11103.0\n      40.769737\n      -73.912440\n      (40.769737, -73.91244)\n      ASTORIA BOULEVARD\n      37 STREET\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      1/1/23\n      4:50\n      BRONX\n      10462.0\n      40.830555\n      -73.850720\n      (40.830555, -73.85072)\n      CASTLE HILL AVENUE\n      EAST 177 STREET\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n5 rows × 40 columns\n\n\n\nLet us merge with uszipcode database to increase the number of input value to predict injury.\n\n#Calculate the sum\njan23['sum'] = jan23['NUMBER OF PERSONS INJURED'] + jan23['NUMBER OF PEDESTRIANS INJURED']+ jan23['NUMBER OF CYCLIST INJURED'] + jan23['NUMBER OF MOTORIST INJURED']\n\nfor index in jan23.index:\n    if jan23['sum'][index] > 0:\n        jan23.loc[index,['injured']] = 1\n    else:\n        jan23.loc[index,['injured']] = 0\n        \nfrom uszipcode import SearchEngine\n\nsearch = SearchEngine()\n\nresultlist = []\n\nfor index in jan23.index:\n    checkZip = jan23['ZIP CODE'][index]\n    if np.isnan(checkZip) == False:\n        zipcode = int(checkZip)\n        result = search.by_zipcode(zipcode)\n        resultlist.append(result.to_dict())\n    else:\n        resultlist.append({})\n\nZipcode_data = pd.DataFrame.from_records(resultlist)\n\nmerge = pd.concat([jan23, Zipcode_data], axis=1)\n\n# Drop the repeated zipcode\nmerge = merge.drop(['zipcode','lat','lng'],axis = 1)\n\nmerge = merge[merge['population'].notnull()]\n\nFocus_data = merge[['radius_in_miles', 'population', 'population_density',\n'land_area_in_sqmi', 'water_area_in_sqmi', 'housing_units',\n'occupied_housing_units','median_home_value','median_household_income','injured']]\n\nThese are the focus data that we will apply SVM to.\n\nFocus_data.head()\n\n\n\n\n\n  \n    \n      \n      radius_in_miles\n      population\n      population_density\n      land_area_in_sqmi\n      water_area_in_sqmi\n      housing_units\n      occupied_housing_units\n      median_home_value\n      median_household_income\n      injured\n    \n  \n  \n    \n      0\n      2.000000\n      90117.0\n      39209.0\n      2.30\n      0.07\n      37180.0\n      33489.0\n      655500.0\n      46848.0\n      1.0\n    \n    \n      2\n      0.909091\n      50984.0\n      77436.0\n      0.66\n      0.00\n      33252.0\n      30294.0\n      914500.0\n      104238.0\n      0.0\n    \n    \n      3\n      0.852273\n      38780.0\n      54537.0\n      0.71\n      0.00\n      18518.0\n      16890.0\n      648900.0\n      55129.0\n      1.0\n    \n    \n      4\n      1.000000\n      75784.0\n      51207.0\n      1.48\n      0.00\n      31331.0\n      29855.0\n      271300.0\n      45864.0\n      0.0\n    \n    \n      5\n      2.000000\n      80018.0\n      36934.0\n      2.17\n      0.05\n      34885.0\n      30601.0\n      524100.0\n      51725.0\n      0.0\n    \n  \n\n\n\n\nTo reduce the complexity, we will get 1000 sample from the dataset and import train_test_split to split up our data to measure the performance.\n\nrandom_sample = Focus_data.sample(n=1000, random_state=220)\n\n#Create X input\nX = random_sample[['radius_in_miles', 'population', 'population_density',\n'land_area_in_sqmi', 'water_area_in_sqmi', 'housing_units',\n'occupied_housing_units','median_home_value','median_household_income']].values\n\n#Create Y for output\ny  = random_sample['injured'].values\n\nfrom sklearn.model_selection import train_test_split \n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n\nApply SVM to our dataset and make a prediction on X_test\n\nfrom sklearn.svm import SVC \n\nclf = SVC(kernel='rbf').fit(X_train, y_train)\n\n#Make prediction using X_test\ny_pred = clf.predict(X_test)\n\nCheck our accuracy of our model by importing accuracy_score from sklearn.metrics\n\nfrom sklearn.metrics import accuracy_score\n\naccuracy = accuracy_score(y_test, y_pred)\n\naccuracy\n\n0.7\n\n\n\n\n\n10.3.4 Conclusion\nSupport Vector Machines is one of the powerful tool mainly for classifications.\n\nTheir dependence on relatively few support vectors means that they are very compact models, and take up very little memory.\nOnce the model is trained, the prediction phase is very fast.\nBecause they are affected only by points near the margin, they work well with high-dimensional data—even data with more dimensions than samples, which is a challenging regime for other algorithms.\nTheir integration with kernel methods makes them very versatile, able to adapt to many types of data.\n\nHowever, SVMs have several disadvantages as well:\n\nThe scaling with the number of samples \\(N\\) is \\(O[N^3]\\) at worst, or \\(O[N^2]\\) for efficient implementations. For large numbers of training samples, this computational cost can be prohibitive.\nThe results are strongly dependent on a suitable choice for the softening parameter \\(C\\).This must be carefully chosen via cross-validation, which can be expensive as datasets grow in size.\nThe results do not have a direct probabilistic interpretation. This can be estimated via an internal cross-validation (see the probability parameter of SVC), but this extra estimation is costly.\n\n\n\n10.3.5 References\n\nIn-Depth: Support Vector Machines\nSupport Vector Machines Video"
  },
  {
    "objectID": "supervised.html#decision-trees",
    "href": "supervised.html#decision-trees",
    "title": "10  Supervised Learning",
    "section": "10.4 Decision Trees",
    "text": "10.4 Decision Trees\n\n10.4.1 Introduction\nDecision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. A tree can be seen as a piecewise constant approximation.\nFor instance, in the example below, decision trees learn from data to approximate a sine curve with a set of if-then-else decision rules. The deeper the tree, the more complex the decision rules and the fitter the model.\n\n\n\n1.png\n\n\npicture source: https://scikit-learn.org/stable/modules/tree.html#\nHere is an simple example of what the “tree” looks like.\n\n\n\n2.png\n\n\nI will introduce the basics of the decision tree package in scikit-learn through this spam email classification example, using a simple mock dataset.\n\nimport pandas as pd\n\nmock_spam = pd.read_csv('data/mock_spam.csv')\nmock_spam\n\n\n\n\n\n  \n    \n      \n      is_spam\n      unknown_sender\n      sales_words\n      scam_words\n    \n  \n  \n    \n      0\n      1\n      1\n      0\n      0\n    \n    \n      1\n      1\n      1\n      1\n      1\n    \n    \n      2\n      1\n      1\n      1\n      0\n    \n    \n      3\n      1\n      1\n      0\n      1\n    \n    \n      4\n      1\n      1\n      1\n      0\n    \n    \n      5\n      0\n      0\n      0\n      0\n    \n    \n      6\n      0\n      0\n      1\n      0\n    \n    \n      7\n      0\n      1\n      0\n      0\n    \n    \n      8\n      0\n      0\n      0\n      1\n    \n    \n      9\n      0\n      1\n      1\n      0\n    \n  \n\n\n\n\nLet’s construct and visualize the model (tree version)\n\nfrom sklearn import tree\nimport graphviz\n\nemail_features = mock_spam[['unknown_sender', 'sales_words', 'scam_words']].values\nis_spam = mock_spam[['is_spam']].values\nclf = tree.DecisionTreeClassifier(criterion='gini') # Create a default classifier\nclf = clf.fit(email_features, is_spam)\nfeat_names = ['is unknown sender', 'contain sales words', 'contain scam words']\nclass_names = ['normal', 'spam']\ndot_data = tree.export_graphviz(clf, out_file=None, feature_names = feat_names, \n                                class_names=class_names, filled=True)\nclf_graph = graphviz.Source(dot_data)\nclf_graph\n\n\n\n\nBoth root and internal nodes have child nodes that branch out from them based on the value of a feature. For instance, the root node splits the unknown_sender feature space, and the threshold is 0.5. Its left subtree represents all the data with unknown_sender <= 0.5, whereas its right subtree represents all the subset of data with unknown_sender > 0.5. Each leaf node has an predicted value which will be used as the output from the decision tree. For example, the leftmost leaf node (left child of the root node) will lead to output is_spam = 0 (i.e. “normal”).\nWe can use this model to make some prediction.\n\nnew_email_feat = [[0, 1, 0], # Known sender, contains sales word, no scam word\n                  [1, 1, 0]] # Unknown sender, contains sales word, no scam word\nclf.predict(new_email_feat) # expected result: 0 (normal), 1 (spam)\n\narray([0, 1])\n\n\nGiven an input, the predicted outcome is obtained by traversing the decision tree. The traversal starts from the root node, and chooses left or right subtree based on the node’s splitting rule recursively, until it reaches a leaf node.\nFor the example input [1, 1, 0], its unknown_sender feature is 1, so we follow the right subtree based on the root node’s splitting rule. The next node splits on the scam_words feature, and since its value is 0, we follow the left subtree. The next node uses the sales_words feature, and its value is 1, so we should go down to the right subtree, where we reach a leaf node. Thus the predicted outcome is the value 1 (class “spam”).\n\n\n10.4.2 Tree algorithms\nAs many other supervised learning approaches, the decision trees are constructed in a way that minimizes a chosen cost function. It is computationally infeasible to find the optimal decision tree that minimizes the cost function. Thus, a greedy approach known as recursive binary splitting is often used.\nPackage scikit-learn uses an optimized version of the CART algorithm; however, the scikit-learn implementation does not support categorical variables for now. Minimal cost-complexity pruning is an algorithm used to prune a tree to avoid over-fitting.\n\n10.4.2.1 Cost function\nGini and entropy are classification criteria. Mean Squared Error (MSE or L2 error), Poisson deviance and Mean Absolute Error (MAE or L1 error) are Regression criteria. Here shows the mathematical formulations to get gini and entropy.\nAs we see from the above example, in a decision tree, each tree node \\(m\\) is associated with a subset of the training data set. Assume there are \\(n_m\\) data points associated with \\(m\\), and the class values of the data points are in the set \\(Q_m\\).\nFurther assume that there are K classes, and let \\[\np_{mk}=\\frac{1}{n_m}\\sum_{y\\in Q_m}I(y=k) (k=1,...,K)\n\\] represent the proportion of class \\(k\\) observations in node \\(m\\). Then the cost functions (referred to as classification criteria in sklearn) available in sklearn are: * Gini: \\[\nH(Q_m)=\\sum_{k}p_{mk}(1-p_{mk})\n\\] * Log loss or entropy: \\[\nH(Q_m)=-\\sum_{k}p_{mk}log(p_{mk})\n\\]\nIn sklearn.tree.DecisionTreeClassifierhe, the default criterion is gini. One advantage of using Gini impurity over entropy is that it can be faster to compute, since it involves only a simple sum of squares rather than logarithmic functions. Additionally, Gini impurity tends to be more robust to small changes in the data, while entropy can be sensitive to noise.\n\n\n10.4.2.2 How to choose what feature and threshold to split on at each node?\nThe decision tree algorithm iterates over all possible features and thresholds and chooses the one that maximize purity or minimize impurity or maximize information gain.\nLet’s use the spam email example to calulate the impurity reduction.\nThe impurity reduction based on Gini is calculated as the difference between the Gini index of the parent node and the weighted average of the Gini of the child nodes. The split that results in the highest impurity reduction based on Gini is chosen as the best split.\n\nclf_graph\n\n\n\n\n\n\n10.4.2.3 When to stop splitting?\nThere are several stopping criteria that can be used to decide when to stop splitting in a decision tree algorithm. Here are some common ones:\nWhen a node is 100% one class\nMaximum depth: Stop splitting when the tree reaches a maximum depth, i.e., when the number of levels in the tree exceeds a predefined threshold.\nMinimum number of samples: Stop splitting when the number of samples in a node falls below a certain threshold. This can help avoid overfitting by preventing the tree from making very specific rules for very few samples.\nMinimum decrease in impurity: Stop splitting when the impurity measure (e.g., Gini impurity or entropy) does not decrease by a certain threshold after a split. This can help avoid overfitting by preventing the tree from making splits that do not significantly improve the purity of the resulting child nodes.\nMaximum number of leaf nodes: Stop splitting when the number of leaf nodes reaches a predefined maximum.\n\n\n\n10.4.3 Demo\n\n10.4.3.1 Preparation\n\n10.4.3.1.1 Step 1: install scikit-learn\nUse pip\npip install -U scikit-learn\nUse Conda\nconda create -n sklearn-env -c conda-forge scikit-learn\n\nconda activate sklearn-env\n\n\n10.4.3.1.2 Step 2: Import Required Libraries\n\nfrom sklearn import tree\nimport pandas as pd\nimport numpy as np\n\n\n\n10.4.3.1.3 Step 3: Preparing the Data\n\nNYC = pd.read_csv(\"data/merged.csv\")\n\n\n# drop rows with missing data in some columns\nNYC = NYC.dropna(subset=['BOROUGH', 'hour', 'median_home_value', 'occupied_housing_units'])\n# Select the features\nnyc_subset = NYC[['BOROUGH', 'hour', 'median_home_value', 'occupied_housing_units']].copy()\n\n\n# One hot encode categorical features\nnyc_encoded = pd.get_dummies(nyc_subset, columns=['BOROUGH', 'hour'])\nnyc_encoded\n\n\n\n\n\n  \n    \n      \n      median_home_value\n      occupied_housing_units\n      BOROUGH_BRONX\n      BOROUGH_BROOKLYN\n      BOROUGH_MANHATTAN\n      BOROUGH_QUEENS\n      BOROUGH_STATEN ISLAND\n      hour_0\n      hour_1\n      hour_2\n      ...\n      hour_14\n      hour_15\n      hour_16\n      hour_17\n      hour_18\n      hour_19\n      hour_20\n      hour_21\n      hour_22\n      hour_23\n    \n  \n  \n    \n      3\n      648900.0\n      16890.0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n    \n    \n      4\n      271300.0\n      29855.0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      5\n      524100.0\n      30601.0\n      0\n      1\n      0\n      0\n      0\n      1\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      6\n      654900.0\n      10429.0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      7\n      602400.0\n      14199.0\n      0\n      0\n      1\n      0\n      0\n      0\n      1\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      7183\n      445900.0\n      12775.0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n    \n    \n      7185\n      445900.0\n      12775.0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      7186\n      397500.0\n      22873.0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      7187\n      655500.0\n      33489.0\n      0\n      1\n      0\n      0\n      0\n      1\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      7188\n      426100.0\n      26420.0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n  \n\n6663 rows × 31 columns\n\n\n\n\n# Train test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = \\\n    train_test_split(nyc_encoded.values, NYC[['injury']].values, test_size = 0.20)\n\n\n\n\n10.4.3.2 Building the Decision Tree Model\n\n# Fit the model and plot the tree (using default parameters)\ninjury_clf = tree.DecisionTreeClassifier(\n    criterion='gini', splitter='best', max_depth=None, \n    min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n    max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, \n    class_weight=None, ccp_alpha=0.0)\ninjury_clf = injury_clf.fit(X_train, y_train)\n\n\ninjury_clf.tree_.node_count\n\n3685\n\n\nArguments related to stopping criteria: * max_depth * min_samples_split * min_samples_leaf * min_weight_fraction_leaf * max_features * max_leaf_nodes * min_impurity_decrease\nOther important arguments: * criterion: cost function to use * splitter: node splitting strategy * ccp_alpha: pruning parameter\n\nfrom sklearn.model_selection import GridSearchCV\n\n# define the hyperparameter grid for logistic regression\nparam_grid = {'criterion': ['gini', 'entropy'],\n              'max_depth': [10, 15, 20],\n              'min_impurity_decrease': [1e-4, 1e-3, 1e-2],\n              'ccp_alpha': [0.0, 1e-5, 1e-4, 1e-3]}\n\n# perform cross-validation with GridSearchCV\ntree_clf = tree.DecisionTreeClassifier()\ngrid_search = GridSearchCV(tree_clf, param_grid, cv=5, scoring='f1')\n\n# fit the GridSearchCV object to the training data\ngrid_search.fit(X_train, y_train)\n\n# print the best hyperparameters found\ngrid_search.best_params_\n\n{'ccp_alpha': 0.0,\n 'criterion': 'entropy',\n 'max_depth': 20,\n 'min_impurity_decrease': 0.0001}\n\n\n\n# Use parameters from cross-validation to train another model\ninjury_clf2 = tree.DecisionTreeClassifier(\n    criterion='gini', splitter='best', max_depth=20, \n    min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n    max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0001, \n    class_weight=None, ccp_alpha=0.0001)\ninjury_clf2 = injury_clf2.fit(X_train, y_train)\ninjury_clf2.tree_.node_count\n\n839\n\n\n\n# Prune the tree more aggressively\ninjury_clf3 = tree.DecisionTreeClassifier(\n    criterion='gini', splitter='best', max_depth=None, \n    min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n    max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0001, \n    class_weight=None, ccp_alpha=8e-4)\ninjury_clf3 = injury_clf3.fit(X_train, y_train)\ninjury_clf3.tree_.node_count\n\n11\n\n\n\ninjury_dot_data3 = tree.export_graphviz(injury_clf3, out_file=None, filled=True)\ninjury_clf_graph = graphviz.Source(injury_dot_data3)\ninjury_clf_graph\n\n\n\n\n\n\n10.4.3.3 Evaluation\n\n# caculate the predicted values\nclf_pred = injury_clf.predict(X_test)\nclf2_pred = injury_clf2.predict(X_test)\nclf3_pred = injury_clf3.predict(X_test)\n\n\n# evaluate the model\nfrom sklearn.metrics import confusion_matrix, \\\naccuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\n# Confusion matrix\nclf_cm = confusion_matrix(y_test, clf_pred)\nclf2_cm = confusion_matrix(y_test, clf2_pred)\nclf3_cm = confusion_matrix(y_test, clf3_pred)\n\n# Accuracy\nclf_acc = accuracy_score(y_test, clf_pred)\nclf2_acc = accuracy_score(y_test, clf2_pred)\nclf3_acc = accuracy_score(y_test, clf3_pred)\n\n# Precision\nclf_precision = precision_score(y_test, clf_pred)\nclf2_precision = precision_score(y_test, clf2_pred)\nclf3_precision = precision_score(y_test, clf3_pred)\n\n# Recall\nclf_recall = recall_score(y_test, clf_pred)\nclf2_recall = recall_score(y_test, clf2_pred)\nclf3_recall = recall_score(y_test, clf3_pred)\n\n# F1-score\nclf_f1 = f1_score(y_test, clf_pred)\nclf2_f1 = f1_score(y_test, clf2_pred)\nclf3_f1 = f1_score(y_test, clf3_pred)\n\n# AUC\nclf_auc = roc_auc_score(y_test, clf_pred)\nclf2_auc = roc_auc_score(y_test, clf2_pred)\nclf3_auc = roc_auc_score(y_test, clf3_pred)\n\n\nprint(\"Default parameter results:\")\nprint(\"Confusion matrix:\")\nprint(clf_cm)\nprint(\"Accuracy:\", clf_acc)\nprint(\"Precision:\", clf_precision)\nprint(\"Recall:\", clf_recall)\nprint(\"F1-score:\", clf_f1)\nprint(\"AUC:\", clf_auc)\nprint(\"\\n\")\nprint(\"Cross-valiation parameter results:\")\nprint(\"Confusion matrix:\")\nprint(clf2_cm)\nprint(\"Accuracy:\", clf2_acc)\nprint(\"Precision:\", clf2_precision)\nprint(\"Recall:\", clf2_recall)\nprint(\"F1-score:\", clf2_f1)\nprint(\"AUC:\", clf2_auc)\nprint(\"\\n\")\nprint(\"More aggressive pruning results:\")\nprint(\"Confusion matrix:\")\nprint(clf3_cm)\nprint(\"Accuracy:\", clf3_acc)\nprint(\"Precision:\", clf3_precision)\nprint(\"Recall:\", clf3_recall)\nprint(\"F1-score:\", clf3_f1)\nprint(\"AUC:\", clf3_auc)\n\nDefault parameter results:\nConfusion matrix:\n[[608 218]\n [360 147]]\nAccuracy: 0.5663915978994749\nPrecision: 0.40273972602739727\nRecall: 0.28994082840236685\nF1-score: 0.3371559633027523\nAUC: 0.5130091551212803\n\n\nCross-valiation parameter results:\nConfusion matrix:\n[[662 164]\n [383 124]]\nAccuracy: 0.5896474118529632\nPrecision: 0.4305555555555556\nRecall: 0.2445759368836292\nF1-score: 0.3119496855345912\nAUC: 0.5230143606936306\n\n\nMore aggressive pruning results:\nConfusion matrix:\n[[761  65]\n [457  50]]\nAccuracy: 0.6084021005251313\nPrecision: 0.43478260869565216\nRecall: 0.09861932938856016\nF1-score: 0.1607717041800643\nAUC: 0.5099634177209145\n\n\n\n\n\n10.4.4 Conclusion\nIn conclusion, decision trees are a widely used supervised learning algorithm for classification and regression tasks. They are easy to understand and interpret. The algorithm works by recursively splitting the dataset based on the attribute that provides the most information gain or the impurity reduction. The tree structure is built from the root node to the leaf nodes, where each node represents a decision based on a feature of the data.\nOne advantage of decision trees is their interpretability, which allows us to easily understand the decision-making process. They can also model complex problems with multiple outcomes. They are not affected by missing values or outliers.\nHowever, decision trees can be prone to overfitting and may not perform well on complex datasets. They can also be sensitive to small variations in the training data and may require pruning to prevent overfitting. Random forest would be a better choice in this situation. Furthermore, decision trees may not perform well on imbalanced datasets, and their performance can be affected by the selection of splitting criteria.\nOverall, decision trees are a useful and versatile tool in machine learning, but it is important to carefully consider their advantages and disadvantages before applying them to a specific problem.\n\n\n10.4.5 References\n\nhttps://scikit-learn.org/stable/modules/tree.html#\n\nhttps://www.coursera.org/learn/advanced-learning-algorithms/home/week/4"
  },
  {
    "objectID": "supervised.html#random-forest",
    "href": "supervised.html#random-forest",
    "title": "10  Supervised Learning",
    "section": "10.5 Random forest",
    "text": "10.5 Random forest\nRandom forest (RF) is a commonly-used ensemble machine learning algorithm. It is a bagging, also known as bootstrap aggregation, method, which combines the output of multiple decision trees to reach a single result.\n\nRegression: mean\nClassification: majority vote\n\n\n10.5.1 Algorithm\nRF baggs on both data (rows) and features (columns).\n\nA random sample of the training data in a training set is selected with replacement (bootstrap)\nA random subset of the features is selected as features (which ensures low correlation among the decision trees)\nHyperparameters\n\nnode size\nnumber of trees\nnumber of features\n\n\nUse cross-valudation to select the hyperparameters.\nAdvantages:\n\nReduced risk of overfitting since averaging of uncorrelated trees lowers overall variance and prediction error.\nProvides flexibility in handeling missing data.\nEasy to evaluate feature importance\n\nMean decrease in impurity (MDI): when a feature is excluded\nMean decrease accuracy: when the values of a feature is randomly permuted\n\n\nDisadvantages:\n\nComputing intensive\nResource hungery\nInterpretation"
  },
  {
    "objectID": "supervised.html#bagging-vs.-boosting",
    "href": "supervised.html#bagging-vs.-boosting",
    "title": "10  Supervised Learning",
    "section": "10.6 Bagging vs. Boosting",
    "text": "10.6 Bagging vs. Boosting\nBy Nathan Nhan\n\n10.6.1 Introduction\nBefore we talk about Bagging and Boosting we first must talk about ensemble learning. Ensemble learning is a technique used in machine-learning where we have multiple models (often times called “weak learners”) that are trained to solve the same problem and then combined to obtain better results that they could individually. With this, we can obtain more accurate and more robust models for our data using this technique.\nBagging and boosting are two types of ensemble learning techniques. They decrease the variance of a single estimate as they combine multiple estimates from different models to create a model with higher stability. Additionally, ensemble learning techniques increase the stability of the final model by reducing faactors of error in our models such as unnecessary noise, bias, and variance that we might find hurts the accuracy of our model. Specifically:\n\nBagging helps decrease the model’s variance and prevent over-fitting.\nBoosting helps decrease the model’s bias.\n\n\n\n10.6.2 Bagging\nBagging, which stands for bootrap aggregation, is a ensemble learning algorithm designed to improve the stability and accuracy of algorithms used in statistical classification and regression. In Bagging, multiple homogenous algorithms are trained independently and combined afterward to determine the model’s average. It works like this:\n\nFrom the original dataset, multiple subsets are created, selecting observations with replacement.\nOn each of these subsets, a base learner (weak learner) is created for each\nNext, all the independent models are run in parallel with one another\nThe final predictions are determined by combining the predictions from all the models.\n\n\n\n\nVisualization on how the bagging algorithm works\n\n\nThe benefits of using bagging algorithms are that * Bagging algorithms reduce bias and variance errors. * Bagging algorithms can handle overfitting (when a model works with a training dataset but fails with the actual testing dataset). * Bagging can easily be implemented and produce more robust models.\n\n10.6.2.1 Illustration\nFirst we must load the dataset. For this topic I will be generating a madeup dataset shown below. It is recommended to make sure the dataset has no missing values as datasets with missing values leads to inconsistent results and poor model performance.\n\nimport pandas as pd\nimport numpy as np\nimport random\n\nlength = 1000\nrandom.seed(0)\n\n# Generate a random dataset\ndata = {\n        'Age': [random.randint(10, 80) for x in range(length)],\n        'Weight': [random.randint(110, 250) for x in range(length)],\n        'Height': [random.randint(55, 77) for x in range(length)],\n        'Average BPM': [random.randint(70, 100) for x in range(length)],\n        'Amount of Surgeries': [random.randint(0, 3) for x in range(length)],\n        'Blood Pressure': [random.randint(100, 180) for x in range(length)],\n        }\n\ndata['Healthy?'] = np.nan\ndf = pd.DataFrame(data)\n\n\n# Generate a random response variable displaying \"1\" for healthy and \"0\" for unhealthy\nfor index, row in df.iterrows():\n    if row['Blood Pressure'] < 110 and row['Average BPM'] > 80:\n        df.at[index, 'Healthy?'] = random.choice([0, 1, 1, 1, 0, 0, 0])\n    else:\n        df.at[index, 'Healthy?'] = random.choice([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1])\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      Age\n      Weight\n      Height\n      Average BPM\n      Amount of Surgeries\n      Blood Pressure\n      Healthy?\n    \n  \n  \n    \n      0\n      59\n      125\n      61\n      92\n      1\n      177\n      1.0\n    \n    \n      1\n      63\n      198\n      71\n      72\n      0\n      149\n      1.0\n    \n    \n      2\n      15\n      218\n      72\n      74\n      0\n      117\n      1.0\n    \n    \n      3\n      43\n      144\n      66\n      96\n      3\n      132\n      1.0\n    \n    \n      4\n      75\n      165\n      69\n      84\n      3\n      173\n      1.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      995\n      15\n      113\n      73\n      74\n      2\n      147\n      1.0\n    \n    \n      996\n      11\n      184\n      63\n      74\n      3\n      102\n      1.0\n    \n    \n      997\n      38\n      139\n      73\n      72\n      1\n      110\n      1.0\n    \n    \n      998\n      51\n      152\n      68\n      99\n      0\n      159\n      1.0\n    \n    \n      999\n      18\n      180\n      56\n      80\n      1\n      162\n      1.0\n    \n  \n\n1000 rows × 7 columns\n\n\n\nNext, we need to specify the x and y variables where the x-variable will hold all the input columns containing numbers and y-variable will contain the output column.\n\nX = df.drop([\"Healthy?\"], axis=\"columns\")\ny = df['Healthy?']\n\nprint(X)\n\n     Age  Weight  Height  Average BPM  Amount of Surgeries  Blood Pressure\n0     59     125      61           92                    1             177\n1     63     198      71           72                    0             149\n2     15     218      72           74                    0             117\n3     43     144      66           96                    3             132\n4     75     165      69           84                    3             173\n..   ...     ...     ...          ...                  ...             ...\n995   15     113      73           74                    2             147\n996   11     184      63           74                    3             102\n997   38     139      73           72                    1             110\n998   51     152      68           99                    0             159\n999   18     180      56           80                    1             162\n\n[1000 rows x 6 columns]\n\n\nNext we must scale our data. Dataset scaling is transforming the dataset to fit within a specific range. This ensures that no data point is left out during model training. In the example giving we will use the StandardScaler method to scale our dataset.\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nprint(X)\nprint(X_scaled)\n\n     Age  Weight  Height  Average BPM  Amount of Surgeries  Blood Pressure\n0     59     125      61           92                    1             177\n1     63     198      71           72                    0             149\n2     15     218      72           74                    0             117\n3     43     144      66           96                    3             132\n4     75     165      69           84                    3             173\n..   ...     ...     ...          ...                  ...             ...\n995   15     113      73           74                    2             147\n996   11     184      63           74                    3             102\n997   38     139      73           72                    1             110\n998   51     152      68           99                    0             159\n999   18     180      56           80                    1             162\n\n[1000 rows x 6 columns]\n[[ 0.71876116 -1.33938121 -0.78473753  0.77849777 -0.44769866  1.59373784]\n [ 0.91291599  0.44444037  0.71600359 -1.46533498 -1.3613694   0.39947944]\n [-1.41694191  0.93315861  0.8660777  -1.2409517  -1.3613694  -0.96538731]\n ...\n [-0.30055167 -0.99727844  1.01615181 -1.46533498 -0.44769866 -1.26395191]\n [ 0.33045151 -0.67961158  0.26578125  1.56383923 -1.3613694   0.82600029]\n [-1.27132579  0.00459395 -1.53510809 -0.56780188 -0.44769866  0.95395655]]\n\n\nAfter scaling the dataset, we can split it. We will split the scaled dataset into two subsets: training and testing. To split the dataset, we will use the train_test_split method. We will be using the default splitting ratio for the train_test_split method which means that 80% of the data will be the training set and 20% the testing set.\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, stratify=y, random_state=0)\n\nNow that we have split our data, we can now perform our classification. The BaggingClassifier classifier will perform all the bagging steps and build an optimized model based on our data. The BaggingClassifier will fit the weak/base learners on the randomly sampled subsets. The listed parameters are as follows:\n\nestimator - this parameter takes the algoithm we want to use, in this example we use the DecisionTreeClassifier for our weak learners.\nn_estimators - this parameter takes the amount of weak learners we want to use.\nmax_samples - this parameter represents The maximum number of data that is sampled from the training set. We use 80% of the training dataset for resampling.\nbootstrap - this parameter allows for resampling of the training dataset without replacement when set to True.\noob_score - this parameter is used to compute the model’s accuracy score after training.\nrandom_state - Seed used by the random number generator so we can reproduce out results\n\n\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nbag_model = BaggingClassifier(\nestimator=DecisionTreeClassifier(), # DecisionTreeClassifier() if estimator is not defined\nn_estimators=100, \nbootstrap=True,\noob_score=True,\nrandom_state = 0\n)\n\nbag_model.fit(X_train, y_train)\nAccScore = bag_model.oob_score_\nprint(\"Accuracy Score for Bagging Classifier: \" + str(AccScore))\n\nAccuracy Score for Bagging Classifier: 0.8853333333333333\n\n\nWe can find if overfitting occurs when we get a lower accuracy when using the testing dataset. We can find this by running the following code:\n\nif bag_model.score(X_test, y_test) < AccScore:\n    print('Overfitting has occurred since the testing dataset has a lower accuracy than the training dataset')\nelse:\n    print(\"No overfitting has occurred\")\n\nOverfitting has occurred since the testing dataset has a lower accuracy than the training dataset\n\n\nComparing this to the non-bagging algorithm like K-fold cross-validation:\n\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(DecisionTreeClassifier(), X, y, cv=5)\nprint(\"Accuracy Score for Bagging Classifier: \" + str(scores.mean())) \n\nAccuracy Score for Bagging Classifier: 0.7870000000000001\n\n\n\n\n10.6.2.2 Example: Random Forest Classifier\nThe Random Forest Classifier algorithm is a typical example of a bagging algorithm. Random Forests uses bagging underneath to sample the dataset with replacement randomly. Random Forests samples not only data rows but also columns. It also follows the bagging steps to produce an aggregated final model.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(\nn_estimators=100, \nbootstrap = True,\noob_score=True,\nrandom_state = 0\n)\n\nclf = clf.fit(X, y)\n\nprint(\"Accuracy Score for Random Forest Regression Algoritm Classifier: \" + str(clf.oob_score_)) \n\nAccuracy Score for Random Forest Regression Algoritm Classifier: 0.869\n\n\n\n\n\n10.6.3 Boosting\nThe Boosting algorithm is a type of ensemble learning algorithm that works by training weak models sequentially. First, a model is built from the training data. Then an additional model is built which tries to correct the errors present in the first model. When an input is misclassified by a model, its weight is increased so that next model is more likely to classify it correctly. This procedure is continued and models are added until either the complete training data set is predicted correctly or the maximum number of n models are added.\nIt works step-by-step like this:\n\nA subset is created from the original dataset, where initially all the data points are given equal weights.\nA base model is created on this subset which is used to make predictions on the whole dataset.\nErrors are calculated by comparing the actual values vs. the predicted values. The observations that are incorrectly predicted, are given higher weights.\nAn additional model is created and predictions are made on the dataset, trying to correct the errors of the previous model\nThe cycle will repeat until the maximum number of n models are added.\nThe final model (strong model) is the weighted mean of all the models (weak model).\n\n\n\n\nVisualization on how the boosting algorithm works\n\n\n\n10.6.3.1 Example: AdaBoost\nThe AdaBoost algorithm, which is short for “Adaptive Boosting,” was one of the first boosting methods that saw an increase in accuracy and speed performance of models. AdaBoost focuses on enhnacement in performance in areas where the first iteration of the model fails.\nWe can see an implmentation of this algorithm below:\nThe AdaBoostClassifier method has some of the following parameters:\n\nestimator - this parameter takes the algoithm we want to use, in this example we use the DecisionTreeClassifier for our weak learners.\nn_estimators - this parameter takes the amount of weak learners we want to use.\nlearning_rate - this parameter learning rate reduces the contribution of the classifier by this value. It has a default value of 1.\nrandom_state - Seed used by the random number generator so we can reproduce out results.\n\n\nfrom sklearn.ensemble import AdaBoostClassifier\n\nadaboost = AdaBoostClassifier(\n    estimator = DecisionTreeClassifier(),\n    n_estimators = 100, \n    learning_rate = 0.2,\n    random_state = 0\n    )\n\nadaboost.fit(X_train, y_train)\nscore = adaboost.score(X_test, y_test)\nprint(\"Accuracy Score for Adaboost Classifier: \" + str(score)) \n\nAccuracy Score for Adaboost Classifier: 0.76\n\n\n\n\n10.6.3.2 Example: XGBoost\nXGBoost is widely considered as one of the most important boosting methods for its advantages over other boosting algorithms.\nOther boosting algorithms typically use gradiant descent to find the minima of the n features mapped in n dimensional space. However, XGBoost instead uses the a mathematical technique called the Newton–Raphson method which uses the second derivative of the function which provides curvature information in contract to algorithms using gradiant descent which only use the first derivative.\nXGBoost has its own python library called xgboost just to itself for which we wrote the example code below.\nThe XGBClassifer contains some of the following parameters:\n\nlearning_rate - this parameter takes the amount of weak learners we want to use.\nrandom_state - Random number seed.\nimportance_type - The feature to focus on; either gain, weight, cover, total_gain or total_cover.\n\n\nfrom xgboost import XGBClassifier\nxgboost = XGBClassifier(\n    n_estimators = 1000, \n    learning_rate = 0.05,\n    random_state = 0)\n\nxgboost.fit(X_train, y_train)\nscore_xgb = xgboost.score(X_test,y_test)\nprint(\"Accuracy Score for XGBoost Classifier: \" + str(score_xgb)) \n\nAccuracy Score for XGBoost Classifier: 0.836\n\n\n\n\n\n10.6.4 Bagging vs. Boosting\nNow with all that being said, which should you use? Bagging or Boosting?\nBoth Bagging and Boosting combine several estimates from different models so both will turn out a model with higher stability.\nIf you find that the problem when using a single model gets a high error, bagging will rarely be better but on the other hand boosting will generate a combined model with lower errors.\nConversely, if your single model is over-fitting then typically bagging is the best option and boosting will not help for over-fitting. Therefore, bagging iw what you want.\n\n10.6.4.1 Similarites\nBelow is a set of similarites between Bagging and Boosting:\n\nBoth are ensemble learning methods to get N models all from one individual model\nBoth use random sampling to generate several random subsets\nBoth make the final decision by averaging and combining the N learners (or taking the majority of them i.e Majority Voting).\nBoth reduce variance and provide higher data stability than one individual model would.\n\n\n\n10.6.4.2 Differences:\nBelow is a set of differences between Bagging and Boosting:\n\nBagging combines predictions belonging to the same type while boosting is a way of combining predictions that belong to different types\nBagging decreases variance while boosting decreases bias. If the classifier is unstable (high variance), then we using Bagging. If the classifier is stable and simple (high bias), then we should use Boosting.\nBagging each model receives equal weight where in Boosting each model is weighted based on their performance.\nBagging has models run in parallel, independent of one another while Boosting has them run sequentially so each model is dependent on the previous\nIn Bagging different training data subsets are randomly generated with replacement segemnts of the original training dataset. In Boosting each new subsets contains the elements that were misclassified by previous models.\nBagging attempts to solve over-fitting problem while Boosting does not.\n\n\n\n10.6.4.3 Conclusion:\n\nWe have explained what an ensemble method is and how Bagging and Boosting algorithms function.\nWe have demonstrated the differences and similaries between the two ensemble methods: Bagging and Boosting.\nWe showed how to implement both Bagging and Boosting algorithms into Python\n\n\n\n\n10.6.5 References\nKaggle: Bagging vs. Boosting\nScikit.learn\nBagging algorithms in Python\nBoosting Algorithms in Python\nGeeksforGeeks"
  },
  {
    "objectID": "advanced.html#text-analysis-with-nltk",
    "href": "advanced.html#text-analysis-with-nltk",
    "title": "11  Advanced Topics",
    "section": "11.1 Text Analysis with nltk",
    "text": "11.1 Text Analysis with nltk\n\n11.1.1 Introduction\nnltk, or Natural Language Toolkit, is a Python package which provides a set of tools for text analysis. nltk is used in Natural Language Processing (NLP), a field of computer science which focuses on the interaction between computers and human languages. nltk is a very powerful tool for text analysis, and is used by many researchers and data scientists. In this tutorial, we will learn how to use nltk to analyze text.\n\n\n11.1.2 Getting Started\nFirst, we must install nltk using pip.\npython -m pip install nltk\nNecessary datasets/models are needed for specific functions to work. We can download a popular subset with\npython -m nltk.downloader popular\n\n\n11.1.3 Tokenizing\nTo analyze text, it needs to be broken down into smaller pieces. This is called tokenization. nltk offers two ways to tokenize text: sentence tokenization and word tokenization.\n\nimport nltk\n\nTo demonstrate this, we will use the following text, a passage from the 1951 science fiction novel Foundation by Isaac Asimov.\n\nfd_string = \"\"\"The sum of human knowing is beyond any one man; any thousand men. With the destruction of our social fabric, science will be broken into a million pieces. Individuals will know much of exceedingly tiny facets of what there is to know. They will be helpless and useless by themselves. The bits of lore, meaningless, will not be passed on. They will be lost through the generations. But, if we now prepare a giant summary of all knowledge, it will never be lost. Coming generations will build on it, and will not have to rediscover it for themselves. One millennium will do the work of thirty thousand.\"\"\"\n\n\n11.1.3.1 Sentence Tokenization\n\nfrom nltk import sent_tokenize, word_tokenize\n# nltk.download(\"popular\") # only needs to download once\nfd_sent = sent_tokenize(fd_string)\nprint(fd_sent)\n\n['The sum of human knowing is beyond any one man; any thousand men.', 'With the destruction of our social fabric, science will be broken into a million pieces.', 'Individuals will know much of exceedingly tiny facets of what there is to know.', 'They will be helpless and useless by themselves.', 'The bits of lore, meaningless, will not be passed on.', 'They will be lost through the generations.', 'But, if we now prepare a giant summary of all knowledge, it will never be lost.', 'Coming generations will build on it, and will not have to rediscover it for themselves.', 'One millennium will do the work of thirty thousand.']\n\n\n\n\n11.1.3.2 Word Tokenization\n\nfd_word = word_tokenize(fd_string)\nprint(fd_word)\n\n['The', 'sum', 'of', 'human', 'knowing', 'is', 'beyond', 'any', 'one', 'man', ';', 'any', 'thousand', 'men', '.', 'With', 'the', 'destruction', 'of', 'our', 'social', 'fabric', ',', 'science', 'will', 'be', 'broken', 'into', 'a', 'million', 'pieces', '.', 'Individuals', 'will', 'know', 'much', 'of', 'exceedingly', 'tiny', 'facets', 'of', 'what', 'there', 'is', 'to', 'know', '.', 'They', 'will', 'be', 'helpless', 'and', 'useless', 'by', 'themselves', '.', 'The', 'bits', 'of', 'lore', ',', 'meaningless', ',', 'will', 'not', 'be', 'passed', 'on', '.', 'They', 'will', 'be', 'lost', 'through', 'the', 'generations', '.', 'But', ',', 'if', 'we', 'now', 'prepare', 'a', 'giant', 'summary', 'of', 'all', 'knowledge', ',', 'it', 'will', 'never', 'be', 'lost', '.', 'Coming', 'generations', 'will', 'build', 'on', 'it', ',', 'and', 'will', 'not', 'have', 'to', 'rediscover', 'it', 'for', 'themselves', '.', 'One', 'millennium', 'will', 'do', 'the', 'work', 'of', 'thirty', 'thousand', '.']\n\n\nBoth the sentence tokenization and word tokenization functions return a list of strings. We can use these lists to perform further analysis.\n\n\n\n11.1.4 Removing Stopwords\nThe output of the word tokenization gave us a list of words. However, some of these words are not useful for our analysis. These words are called stopwords. nltk provides a list of stopwords for several languages. We can use this list to remove stopwords from our text.\n\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words(\"english\"))\nprint(stop_words)\n\n{'doing', 'both', 'didn', 'few', 'were', 'under', 'in', 'aren', 'up', \"mustn't\", 'off', 'at', 'o', 'isn', 'below', 'own', 'very', 'nor', 'and', 'between', 'yourself', 'being', 'no', 'should', 'are', \"you'd\", 'she', 'but', 'of', \"mightn't\", 'each', 'such', 'further', 'ourselves', 'do', \"aren't\", 'hers', 'have', 'is', 'himself', 'am', 'the', 'having', 'after', 'don', 'was', 'more', 'it', 'can', 'an', 'wouldn', \"weren't\", 'on', 'y', 'once', 'only', 'why', 'its', 'now', 'those', 'a', 'because', \"don't\", 'weren', 'above', 'yours', 'his', 've', 'you', \"it's\", \"won't\", 'whom', 'be', \"wouldn't\", 'them', 'how', 'just', \"you'll\", \"that'll\", 'when', 'themselves', 'shouldn', 'wasn', 'same', 'ma', 'won', 'again', 're', \"doesn't\", 'itself', 'he', 'other', 'll', 'to', 'mustn', 'their', 'what', 'hadn', 'then', 'haven', 'doesn', 't', 'for', 'ain', \"she's\", 'your', 'any', 'most', 'so', 'theirs', 'hasn', 'some', \"haven't\", 'too', 's', \"hasn't\", 'mightn', 'has', \"shan't\", 'while', 'will', 'our', 'd', 'her', 'couldn', 'until', 'where', 'there', 'herself', 'did', 'through', 'against', \"needn't\", 'or', 'had', 'which', 'yourselves', 'that', 'down', 'with', 'm', \"you've\", 'about', 'than', 'shan', 'these', \"didn't\", 'needn', \"you're\", 'here', 'before', 'my', 'this', 'into', 'all', 'ours', 'from', 'not', 'if', 'him', 'myself', 'me', \"wasn't\", 'who', \"should've\", \"couldn't\", 'i', 'during', \"isn't\", \"shouldn't\", 'over', 'they', 'does', 'by', 'as', 'been', \"hadn't\", 'we', 'out'}\n\n\n\nfd_filtered = [w for w in fd_word if w.casefold() not in stop_words]\nprint(fd_filtered)\n\n['sum', 'human', 'knowing', 'beyond', 'one', 'man', ';', 'thousand', 'men', '.', 'destruction', 'social', 'fabric', ',', 'science', 'broken', 'million', 'pieces', '.', 'Individuals', 'know', 'much', 'exceedingly', 'tiny', 'facets', 'know', '.', 'helpless', 'useless', '.', 'bits', 'lore', ',', 'meaningless', ',', 'passed', '.', 'lost', 'generations', '.', ',', 'prepare', 'giant', 'summary', 'knowledge', ',', 'never', 'lost', '.', 'Coming', 'generations', 'build', ',', 'rediscover', '.', 'One', 'millennium', 'work', 'thirty', 'thousand', '.']\n\n\nThe resulting list is significantly shorter. There are some words that nltk considers stopwords that we may want to keep, depending on the objective of our analysis. Reducing the size of our data can help us to reduce the time it takes to perform our analysis. However, removing too many words can reduce the accuracy, which is especially important when we are trying to perform sentiment analysis.\n\n\n11.1.5 Stemming\nStemming is a method which allows us to reduce the number of variants of a word. For example, the words connecting, connected, and connection are all variants of the same word connect. nltk includes a few different stemmers based on different algorithms. We will use the Snowball stemmer, an improved version of the 1979 Porter stemmer.\n\nfrom nltk.stem.snowball import SnowballStemmer\nsnow_stem = SnowballStemmer(language='english')\nfd_stem = [snow_stem.stem(w) for w in fd_word]\nprint(fd_stem)\n\n['the', 'sum', 'of', 'human', 'know', 'is', 'beyond', 'ani', 'one', 'man', ';', 'ani', 'thousand', 'men', '.', 'with', 'the', 'destruct', 'of', 'our', 'social', 'fabric', ',', 'scienc', 'will', 'be', 'broken', 'into', 'a', 'million', 'piec', '.', 'individu', 'will', 'know', 'much', 'of', 'exceed', 'tini', 'facet', 'of', 'what', 'there', 'is', 'to', 'know', '.', 'they', 'will', 'be', 'helpless', 'and', 'useless', 'by', 'themselv', '.', 'the', 'bit', 'of', 'lore', ',', 'meaningless', ',', 'will', 'not', 'be', 'pass', 'on', '.', 'they', 'will', 'be', 'lost', 'through', 'the', 'generat', '.', 'but', ',', 'if', 'we', 'now', 'prepar', 'a', 'giant', 'summari', 'of', 'all', 'knowledg', ',', 'it', 'will', 'never', 'be', 'lost', '.', 'come', 'generat', 'will', 'build', 'on', 'it', ',', 'and', 'will', 'not', 'have', 'to', 'rediscov', 'it', 'for', 'themselv', '.', 'one', 'millennium', 'will', 'do', 'the', 'work', 'of', 'thirti', 'thousand', '.']\n\n\nStemming algorithms are susceptible to errors. Related words that should share a stem may not, which is known as understemming, which is a false negative. Unrelated words that should not share a stem may, which is known as overstemming, which is a false positive.\n\n\n11.1.6 POS Tagging\nnltk also enables us to label the parts of speech of each word in a text. This is known as part-of-speech (POS) tagging. nltk uses the Penn Treebank tagset, which is a set of tags that are used to label words in a text. The tags are as follows:\n\nnltk.help.upenn_tagset()\n\n$: dollar\n    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n'': closing quotation mark\n    ' ''\n(: opening parenthesis\n    ( [ {\n): closing parenthesis\n    ) ] }\n,: comma\n    ,\n--: dash\n    --\n.: sentence terminator\n    . ! ?\n:: colon or ellipsis\n    : ; ...\nCC: conjunction, coordinating\n    & 'n and both but either et for less minus neither nor or plus so\n    therefore times v. versus vs. whether yet\nCD: numeral, cardinal\n    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n    fifteen 271,124 dozen quintillion DM2,000 ...\nDT: determiner\n    all an another any both del each either every half la many much nary\n    neither no some such that the them these this those\nEX: existential there\n    there\nFW: foreign word\n    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n    terram fiche oui corporis ...\nIN: preposition or conjunction, subordinating\n    astride among uppon whether out inside pro despite on by throughout\n    below within for towards near behind atop around if like until below\n    next into if beside ...\nJJ: adjective or numeral, ordinal\n    third ill-mannered pre-war regrettable oiled calamitous first separable\n    ectoplasmic battery-powered participatory fourth still-to-be-named\n    multilingual multi-disciplinary ...\nJJR: adjective, comparative\n    bleaker braver breezier briefer brighter brisker broader bumper busier\n    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n    cozier creamier crunchier cuter ...\nJJS: adjective, superlative\n    calmest cheapest choicest classiest cleanest clearest closest commonest\n    corniest costliest crassest creepiest crudest cutest darkest deadliest\n    dearest deepest densest dinkiest ...\nLS: list item marker\n    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n    SP-44007 Second Third Three Two * a b c d first five four one six three\n    two\nMD: modal auxiliary\n    can cannot could couldn't dare may might must need ought shall should\n    shouldn't will would\nNN: noun, common, singular or mass\n    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n    investment slide humour falloff slick wind hyena override subhumanity\n    machinist ...\nNNP: noun, proper, singular\n    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n    Shannon A.K.C. Meltex Liverpool ...\nNNPS: noun, proper, plural\n    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n    Apache Apaches Apocrypha ...\nNNS: noun, common, plural\n    undergraduates scotches bric-a-brac products bodyguards facets coasts\n    divestitures storehouses designs clubs fragrances averages\n    subjectivists apprehensions muses factory-jobs ...\nPDT: pre-determiner\n    all both half many quite such sure this\nPOS: genitive marker\n    ' 's\nPRP: pronoun, personal\n    hers herself him himself hisself it itself me myself one oneself ours\n    ourselves ownself self she thee theirs them themselves they thou thy us\nPRP$: pronoun, possessive\n    her his mine my our ours their thy your\nRB: adverb\n    occasionally unabatingly maddeningly adventurously professedly\n    stirringly prominently technologically magisterially predominately\n    swiftly fiscally pitilessly ...\nRBR: adverb, comparative\n    further gloomier grander graver greater grimmer harder harsher\n    healthier heavier higher however larger later leaner lengthier less-\n    perfectly lesser lonelier longer louder lower more ...\nRBS: adverb, superlative\n    best biggest bluntest earliest farthest first furthest hardest\n    heartiest highest largest least less most nearest second tightest worst\nRP: particle\n    aboard about across along apart around aside at away back before behind\n    by crop down ever fast for forth from go high i.e. in into just later\n    low more off on open out over per pie raising start teeth that through\n    under unto up up-pp upon whole with you\nSYM: symbol\n    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\nTO: \"to\" as preposition or infinitive marker\n    to\nUH: interjection\n    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n    man baby diddle hush sonuvabitch ...\nVB: verb, base form\n    ask assemble assess assign assume atone attention avoid bake balkanize\n    bank begin behold believe bend benefit bevel beware bless boil bomb\n    boost brace break bring broil brush build ...\nVBD: verb, past tense\n    dipped pleaded swiped regummed soaked tidied convened halted registered\n    cushioned exacted snubbed strode aimed adopted belied figgered\n    speculated wore appreciated contemplated ...\nVBG: verb, present participle or gerund\n    telegraphing stirring focusing angering judging stalling lactating\n    hankerin' alleging veering capping approaching traveling besieging\n    encrypting interrupting erasing wincing ...\nVBN: verb, past participle\n    multihulled dilapidated aerosolized chaired languished panelized used\n    experimented flourished imitated reunifed factored condensed sheared\n    unsettled primed dubbed desired ...\nVBP: verb, present tense, not 3rd person singular\n    predominate wrap resort sue twist spill cure lengthen brush terminate\n    appear tend stray glisten obtain comprise detest tease attract\n    emphasize mold postpone sever return wag ...\nVBZ: verb, present tense, 3rd person singular\n    bases reconstructs marks mixes displeases seals carps weaves snatches\n    slumps stretches authorizes smolders pictures emerges stockpiles\n    seduces fizzes uses bolsters slaps speaks pleads ...\nWDT: WH-determiner\n    that what whatever which whichever\nWP: WH-pronoun\n    that what whatever whatsoever which who whom whosoever\nWP$: WH-pronoun, possessive\n    whose\nWRB: Wh-adverb\n    how however whence whenever where whereby whereever wherein whereof why\n``: opening quotation mark\n    ` ``\n\n\nWe can use the function nltk.pos_tag() on our list of tokenized words. This will return a list of tuples, where each tuple contains a word and its corresponding tag.\n\nfd_tag = nltk.pos_tag(fd_word)\nprint(fd_tag)\n\n[('The', 'DT'), ('sum', 'NN'), ('of', 'IN'), ('human', 'JJ'), ('knowing', 'NN'), ('is', 'VBZ'), ('beyond', 'IN'), ('any', 'DT'), ('one', 'CD'), ('man', 'NN'), (';', ':'), ('any', 'DT'), ('thousand', 'CD'), ('men', 'NNS'), ('.', '.'), ('With', 'IN'), ('the', 'DT'), ('destruction', 'NN'), ('of', 'IN'), ('our', 'PRP$'), ('social', 'JJ'), ('fabric', 'NN'), (',', ','), ('science', 'NN'), ('will', 'MD'), ('be', 'VB'), ('broken', 'VBN'), ('into', 'IN'), ('a', 'DT'), ('million', 'CD'), ('pieces', 'NNS'), ('.', '.'), ('Individuals', 'NNS'), ('will', 'MD'), ('know', 'VB'), ('much', 'RB'), ('of', 'IN'), ('exceedingly', 'RB'), ('tiny', 'JJ'), ('facets', 'NNS'), ('of', 'IN'), ('what', 'WP'), ('there', 'EX'), ('is', 'VBZ'), ('to', 'TO'), ('know', 'VB'), ('.', '.'), ('They', 'PRP'), ('will', 'MD'), ('be', 'VB'), ('helpless', 'JJ'), ('and', 'CC'), ('useless', 'JJ'), ('by', 'IN'), ('themselves', 'PRP'), ('.', '.'), ('The', 'DT'), ('bits', 'NNS'), ('of', 'IN'), ('lore', 'NN'), (',', ','), ('meaningless', 'NN'), (',', ','), ('will', 'MD'), ('not', 'RB'), ('be', 'VB'), ('passed', 'VBN'), ('on', 'IN'), ('.', '.'), ('They', 'PRP'), ('will', 'MD'), ('be', 'VB'), ('lost', 'VBN'), ('through', 'IN'), ('the', 'DT'), ('generations', 'NNS'), ('.', '.'), ('But', 'CC'), (',', ','), ('if', 'IN'), ('we', 'PRP'), ('now', 'RB'), ('prepare', 'VBP'), ('a', 'DT'), ('giant', 'JJ'), ('summary', 'NN'), ('of', 'IN'), ('all', 'DT'), ('knowledge', 'NN'), (',', ','), ('it', 'PRP'), ('will', 'MD'), ('never', 'RB'), ('be', 'VB'), ('lost', 'VBN'), ('.', '.'), ('Coming', 'VBG'), ('generations', 'NNS'), ('will', 'MD'), ('build', 'VB'), ('on', 'IN'), ('it', 'PRP'), (',', ','), ('and', 'CC'), ('will', 'MD'), ('not', 'RB'), ('have', 'VB'), ('to', 'TO'), ('rediscover', 'VB'), ('it', 'PRP'), ('for', 'IN'), ('themselves', 'PRP'), ('.', '.'), ('One', 'CD'), ('millennium', 'NN'), ('will', 'MD'), ('do', 'VB'), ('the', 'DT'), ('work', 'NN'), ('of', 'IN'), ('thirty', 'JJ'), ('thousand', 'NN'), ('.', '.')]\n\n\n\n\n\nThe tokenized words from the quote should be easy to tag correctly. The function may encounter difficulty with less conventional words (e.g. Old English), but it will attempt to tag based on context.\n\n\n11.1.7 Lemmatizing\nLemmatizing is similar to stemming, but it is more accurate. Lemmatizing is a process which reduces words to their lemma, which is the base form of a word.nltk includes a lemmatizer based on the WordNet database. We can demonstrate this using a quote from the 1868 novel Little Women by Louisa May Alcott.\n\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\nquote = \"The dim, dusty room, with the busts staring down from the tall book-cases, the cosy chairs, the globes, and, best of all, the wilderness of books, in which she could wander where she liked, made the library a region of bliss to her.\"\nquote_token = word_tokenize(quote)\nquote_lemma = [lemmatizer.lemmatize(w) for w in quote_token]\nprint(quote_lemma)\n\n['The', 'dim', ',', 'dusty', 'room', ',', 'with', 'the', 'bust', 'staring', 'down', 'from', 'the', 'tall', 'book-cases', ',', 'the', 'cosy', 'chair', ',', 'the', 'globe', ',', 'and', ',', 'best', 'of', 'all', ',', 'the', 'wilderness', 'of', 'book', ',', 'in', 'which', 'she', 'could', 'wander', 'where', 'she', 'liked', ',', 'made', 'the', 'library', 'a', 'region', 'of', 'bliss', 'to', 'her', '.']\n\n\n\n\n11.1.8 Chunking/Chinking\nWhile tokenizing allows us to distinguish individual words and sentences within a larger body of text, Chunking allows us to identify phrases based on grammar we specify.\n\n#nltk.download(\"averaged_perceptron_tagger\")\nquote_tag = nltk.pos_tag(quote_token)\n\nWe can then name grammar rules to apply to the text. These use regular expressions, which are listed below:\n\n\n\n\n\n\n\nOperator\nBehavior\n\n\n.\nWildcard, matches any character\n\n\n^abc\nMatches some pattern abc at the start of a string\n\n\nabc$\nMatches some pattern abc at the end of a string\n\n\n[abc]\nMatches one of a set of characters\n\n\n[A-Z0-9]\nMatches one of a range of characters\n\n\ned|ing|s\nMatches one of the specified strings (disjunction)\n\n\n*\nZero or more of previous item, e.g. a*, [a-z]* (also known as Kleene Closure)\n\n\n+\nOne or more of previous item, e.g. a+, [a-z]+\n\n\n?\nZero or one of the previous item (i.e. optional), e.g. a?, [a-z]?\n\n\n{n}\nExactly n repeats where n is a non-negative integer\n\n\n{n,}\nAt least n repeats\n\n\n{,n}\nNo more than n repeats\n\n\n{m,n}\nAt least m and no more than n repeats\n\n\na(b|c)+\nParentheses that indicate the scope of the operators\n\n\n\n\nimport re\nimport regex\n\n\ngrammar = r\"\"\"\n  NP: {<DT|JJ|NN.*>+}          # Chunk sequences of DT, JJ, NN\n  PP: {<IN><NP>}               # Chunk prepositions followed by NP\n  VP: {<VB.*><NP|PP|CLAUSE>+$} # Chunk verbs and their arguments\n  CLAUSE: {<NP><VP>}           # Chunk NP, VP\n  \"\"\"\n\n\nchunk_parser = nltk.RegexpParser(grammar)\ntree = chunk_parser.parse(quote_tag)\ntree.pretty_print(unicodelines=True)\n\n                                                                                                                                                                                        S                                                                                                                                                                                                                                                                 \n ┌───┬───────┬─────────┬─────┬───┬───┬────┬─────┬─────┬──────┬───┬────┬───────┬────────┬───────┬─────────┬─────────┬────────┬────────┬──────┬─────┬───────┬──────┬──────┬──────────┬────┴──────────────┬────────────────────┬──────────────────────────────────┬───────────────────────────────────┬───────────────────────┬────────────────────┬─────────────────┬───────────────────────┬───────────────────────┬────────────────────────────┐           \n │   │       │         │     │   │   │    │     │     │      │   │    │       │        │       │         │         │        │        │      │     │       │      │      │          │                   │                    PP                                 PP                                  │                       │                    PP                │                       PP                      │                            PP         \n │   │       │         │     │   │   │    │     │     │      │   │    │       │        │       │         │         │        │        │      │     │       │      │      │          │                   │             ┌──────┴─────┐               ┌────────────┴─────┐                             │                       │               ┌────┴────┐            │                  ┌────┴──────┐                │                       ┌────┴─────┐     \n │   │       │         │     │   │   │    │     │     │      │   │    │       │        │       │         │         │        │        │      │     │       │      │      │          NP                  NP            │            NP              │                  NP                            NP                      NP              │         NP           NP                 │           NP               NP                      │          NP   \n │   │       │         │     │   │   │    │     │     │      │   │    │       │        │       │         │         │        │        │      │     │       │      │      │    ┌─────┴────┐       ┌──────┴─────┐       │      ┌─────┴──────┐        │      ┌───────────┼──────────┐          ┌───────┼────────┐        ┌─────┴──────┐        │         │      ┌─────┴────────┐         │           │       ┌────────┼───────┬───────┐       │          │     \n,/, ,/, staring/VBG down/RP ,/, ,/, ,/, and/CC ,/, best/JJS ,/, ,/, in/IN which/WDT she/PRP could/MD wander/VB where/WRB she/PRP liked/VBD ,/, made/VBD to/TO her/PRP$ ./. The/DT     dim/NN dusty/JJ     room/NN with/IN the/DT     busts/NNS from/IN the/DT     tall/JJ book-cases/NNS the/DT cosy/JJ chairs/NNS the/DT     globes/NNS of/IN     all/DT the/DT     wilderness/NN of/IN     books/NNS the/DT library/NN a/DT region/NN of/IN     bliss/NN\n\n\n\nAs you can see, the generated tree shows the chunks that were identified by the grammar rules. There also is a chink operator, which is the opposite of chunk. It allows us to remove a chunk from a larger chunk.\n\n\n11.1.9 Named Entity Recognition\nPrevious methods have been able to identify the parts of speech of each word in a text. However, we may want to identify specific entities within the text. For example, we may want to identify the names of people, places, and organizations. nltk includes a named entity recognizer which can identify these entities. We can demonstrate this using a quote from The Iliad by Homer.\n\nhomer = \"In the war of Troy, the Greeks having sacked some of the neighbouring towns, and taken from thence two beautiful captives, Chryseïs and Briseïs, allotted the first to Agamemnon, and the last to Achilles.\"\nhomer_token = word_tokenize(homer)\nhomer_tag = nltk.pos_tag(homer_token)\n\n\n#nltk.download(\"maxent_ne_chunker\")\n#nltk.download(\"words\")\ntree2 = nltk.ne_chunk(homer_tag)\ntree2.pretty_print(unicodelines=True)\n\n                                                                                                                                                             S                                                                                                                                                                                 \n  ┌─────┬──────┬──────┬────┬────┬────────┬──────────┬─────────┬──────┬─────┬───────────┬────────────┬──────┬────┬────────┬────────┬────────┬───────┬─────────┼────────────┬────────┬────┬─────┬───────┬─────────┬───────┬───────┬────┬────┬──────┬───────┬──────┬────┬─────┬─────────┬───────────┬────────────┬────────────┬────────────┐       \n  │     │      │      │    │    │        │          │         │      │     │           │            │      │    │        │        │        │       │         │            │        │    │     │       │         │       │       │    │    │      │       │      │    │    GPE       GPE        PERSON        GPE          GPE          GPE     \n  │     │      │      │    │    │        │          │         │      │     │           │            │      │    │        │        │        │       │         │            │        │    │     │       │         │       │       │    │    │      │       │      │    │     │         │           │            │            │            │       \nIn/IN the/DT war/NN of/IN ,/, the/DT having/VBG sacked/VBN some/DT of/IN the/DT neighbouring/JJ towns/NNS ,/, and/CC taken/VBN from/IN thence/NN two/CD beautiful/JJ captives/NNS ,/, and/CC ,/, allotted/VBD the/DT first/JJ to/TO ,/, and/CC the/DT last/JJ to/TO ./. Troy/NNP Greeks/NNP Chryseïs/NNP Briseïs/NNP Agamemnon/NNP Achilles/NNP\n\n\n\nIn the tree, some of the words that should be tagged as PERSON are tagged as GPE, or Geo-Political Entity. In these cases, we can also generate a tree which does not specify the type of named entity.\n\ntree3 = nltk.ne_chunk(homer_tag, binary=True)\ntree3.pretty_print(unicodelines=True)\n\n                                                                                                                                                             S                                                                                                                                                                                 \n  ┌─────┬──────┬──────┬────┬────┬────────┬──────────┬─────────┬──────┬─────┬───────────┬────────────┬──────┬────┬────────┬────────┬────────┬───────┬─────────┼────────────┬────────┬────┬─────┬───────┬─────────┬───────┬───────┬────┬────┬──────┬───────┬──────┬────┬─────┬─────────┬───────────┬────────────┬────────────┬────────────┐       \n  │     │      │      │    │    │        │          │         │      │     │           │            │      │    │        │        │        │       │         │            │        │    │     │       │         │       │       │    │    │      │       │      │    │     NE        NE          NE           NE           NE           NE     \n  │     │      │      │    │    │        │          │         │      │     │           │            │      │    │        │        │        │       │         │            │        │    │     │       │         │       │       │    │    │      │       │      │    │     │         │           │            │            │            │       \nIn/IN the/DT war/NN of/IN ,/, the/DT having/VBG sacked/VBN some/DT of/IN the/DT neighbouring/JJ towns/NNS ,/, and/CC taken/VBN from/IN thence/NN two/CD beautiful/JJ captives/NNS ,/, and/CC ,/, allotted/VBD the/DT first/JJ to/TO ,/, and/CC the/DT last/JJ to/TO ./. Troy/NNP Greeks/NNP Chryseïs/NNP Briseïs/NNP Agamemnon/NNP Achilles/NNP\n\n\n\n\n\n11.1.10 Analyzing Corpora\nnltk includes a number of corpora, which are large bodies of text. We will try out some methods on the 1851 novel Moby Dick by Herman Melville.\n\nfrom nltk.book import *\n\n*** Introductory Examples for the NLTK Book ***\nLoading text1, ..., text9 and sent1, ..., sent9\nType the name of the text or sentence to view it.\nType: 'texts()' or 'sents()' to list the materials.\n\n\ntext1: Moby Dick by Herman Melville 1851\n\n\ntext2: Sense and Sensibility by Jane Austen 1811\ntext3: The Book of Genesis\n\n\ntext4: Inaugural Address Corpus\n\n\ntext5: Chat Corpus\ntext6: Monty Python and the Holy Grail\n\n\ntext7: Wall Street Journal\ntext8: Personals Corpus\ntext9: The Man Who Was Thursday by G . K . Chesterton 1908\n\n\n\n11.1.10.1 Concordance\nconcordance allows us to find all instances of a word in a text. We can use this to find all instances of the word “whale” in Moby Dick.\n\ntext1.concordance(\"whale\")\n\nDisplaying 25 of 1226 matches:\ns , and to teach them by what name a whale - fish is to be called in our tongue\nt which is not true .\" -- HACKLUYT \" WHALE . ... Sw . and Dan . HVAL . This ani\nulted .\" -- WEBSTER ' S DICTIONARY \" WHALE . ... It is more immediately from th\nISH . WAL , DUTCH . HWAL , SWEDISH . WHALE , ICELANDIC . WHALE , ENGLISH . BALE\nHWAL , SWEDISH . WHALE , ICELANDIC . WHALE , ENGLISH . BALEINE , FRENCH . BALLE\nleast , take the higgledy - piggledy whale statements , however authentic , in \n dreadful gulf of this monster ' s ( whale ' s ) mouth , are immediately lost a\n patient Job .\" -- RABELAIS . \" This whale ' s liver was two cartloads .\" -- ST\n Touching that monstrous bulk of the whale or ork we have received nothing cert\n of oil will be extracted out of one whale .\" -- IBID . \" HISTORY OF LIFE AND D\nise .\" -- KING HENRY . \" Very like a whale .\" -- HAMLET . \" Which to secure , n\nrestless paine , Like as the wounded whale to shore flies thro ' the maine .\" -\n. OF SPERMA CETI AND THE SPERMA CETI WHALE . VIDE HIS V . E . \" Like Spencer ' \nt had been a sprat in the mouth of a whale .\" -- PILGRIM ' S PROGRESS . \" That \nEN ' S ANNUS MIRABILIS . \" While the whale is floating at the stern of the ship\ne ship called The Jonas - in - the - Whale . ... Some say the whale can ' t ope\n in - the - Whale . ... Some say the whale can ' t open his mouth , but that is\n masts to see whether they can see a whale , for the first discoverer has a duc\n for his pains . ... I was told of a whale taken near Shetland , that had above\noneers told me that he caught once a whale in Spitzbergen that was white all ov\n2 , one eighty feet in length of the whale - bone kind came in , which ( as I w\nn master and kill this Sperma - ceti whale , for I could never hear of any of t\n . 1729 . \"... and the breath of the whale is frequendy attended with such an i\ned with hoops and armed with ribs of whale .\" -- RAPE OF THE LOCK . \" If we com\ncontemptible in the comparison . The whale is doubtless the largest animal in c\n\n\n\n\n11.1.10.2 Dispersion Plot\ndispersion_plot allows us to see how a word is used throughout a text. We can use this to see the representation of characters throughout Moby Dick.\n\ntext1.dispersion_plot([\"Ahab\", \"Ishmael\", \"Starbuck\", \"Queequeg\"])\n\n/usr/local/lib/python3.11/site-packages/nltk/draw/__init__.py:15: UserWarning: nltk.draw package not loaded (please install Tkinter library).\n  warnings.warn(\"nltk.draw package not loaded (please install Tkinter library).\")\n\n\n\n\n\n\n\n11.1.10.3 Frequency Distribution\nFreqDist allows us to see the frequency of each word in a text. We can use this to see the most common words in Moby Dick.\n\nfrom nltk import FreqDist\nfdist1 = FreqDist(text1)\nprint(fdist1)\n\n<FreqDist with 19317 samples and 260819 outcomes>\n\n\nWe can use the list of stop words generated previously to help us focus on meaningful words.\n\ntext1_imp = [w for w in text1 if w not in stop_words and w.isalpha()]\nfdist2 = FreqDist(text1_imp)\nfdist2.most_common(20)\n\n[('I', 2124),\n ('whale', 906),\n ('one', 889),\n ('But', 705),\n ('like', 624),\n ('The', 612),\n ('upon', 538),\n ('man', 508),\n ('ship', 507),\n ('Ahab', 501),\n ('ye', 460),\n ('old', 436),\n ('sea', 433),\n ('would', 421),\n ('And', 369),\n ('head', 335),\n ('though', 335),\n ('boat', 330),\n ('time', 324),\n ('long', 318)]\n\n\nWe can visualize the frequency distribution using plot.\n\nfdist2.plot(20, cumulative=True)\n\n\n\n\n<AxesSubplot: xlabel='Samples', ylabel='Cumulative Counts'>\n\n\n\n\n11.1.10.4 Collocations\ncollocations allows us to find words that commonly appear together. We can use this to find the most common collocations in Moby Dick.\n\ntext1.collocations()\n\nSperm Whale; Moby Dick; White Whale; old man; Captain Ahab; sperm\nwhale; Right Whale; Captain Peleg; New Bedford; Cape Horn; cried Ahab;\nyears ago; lower jaw; never mind; Father Mapple; cried Stubb; chief\nmate; white whale; ivory leg; one hand\n\n\n\n\n\n11.1.11 Conclusion\nIn this tutorial, we have learned how to use nltk to perform basic text analysis. There are many methods included in this package that help provide structure to text. These methods can be used in conjunction with other packages to perform more complex analysis. For example, a dataframe of open-ended customer feedback could be processed to identify common themes, as well as the polarity of the feedback.\n\n\n11.1.12 Resources\n\nNLTK Documentation\nNLTK Book"
  },
  {
    "objectID": "nycrash.html#first-glance",
    "href": "nycrash.html#first-glance",
    "title": "12  NYC Crash Data",
    "section": "12.1 First Glance",
    "text": "12.1 First Glance\nConsider the NYC Crash Data in January 2022.\n\nimport pandas as pd\n\njan23 = pd.read_csv(\"data/nyc_crashes_202301.csv\")\njan23.head()\njan23.describe()\n\n\n\n\n\n  \n    \n      \n      ZIP CODE\n      LATITUDE\n      LONGITUDE\n      NUMBER OF PERSONS INJURED\n      NUMBER OF PERSONS KILLED\n      NUMBER OF PEDESTRIANS INJURED\n      NUMBER OF PEDESTRIANS KILLED\n      NUMBER OF CYCLIST INJURED\n      NUMBER OF CYCLIST KILLED\n      NUMBER OF MOTORIST INJURED\n      NUMBER OF MOTORIST KILLED\n      COLLISION_ID\n    \n  \n  \n    \n      count\n      4796.000000\n      6764.000000\n      6764.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7.244000e+03\n    \n    \n      mean\n      10893.521685\n      40.234660\n      -73.033403\n      0.502761\n      0.002347\n      0.116372\n      0.000690\n      0.033269\n      0.000414\n      0.333103\n      0.001242\n      4.599022e+06\n    \n    \n      std\n      526.392428\n      4.430595\n      8.041457\n      0.813641\n      0.051164\n      0.397927\n      0.026265\n      0.180118\n      0.020348\n      0.749174\n      0.038951\n      2.365885e+03\n    \n    \n      min\n      10001.000000\n      0.000000\n      -74.250150\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      4.594332e+06\n    \n    \n      25%\n      10457.000000\n      40.663230\n      -73.964888\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      4.597113e+06\n    \n    \n      50%\n      11208.000000\n      40.711880\n      -73.921230\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      4.599058e+06\n    \n    \n      75%\n      11239.000000\n      40.775640\n      -73.865389\n      1.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      4.600953e+06\n    \n    \n      max\n      11694.000000\n      40.912827\n      0.000000\n      21.000000\n      2.000000\n      19.000000\n      1.000000\n      2.000000\n      1.000000\n      8.000000\n      2.000000\n      4.605324e+06\n    \n  \n\n\n\n\nFrequency tables for categorical variables.\n\njan23[\"BOROUGH\"].value_counts(dropna=False)\n\nNaN              2448\nBROOKLYN         1653\nQUEENS           1316\nBRONX             821\nMANHATTAN         811\nSTATEN ISLAND     195\nName: BOROUGH, dtype: int64\n\n\nSome tables are too long.\n\n# jan23[\"CONTRIBUTING FACTOR VEHICLE 1\"].value_counts(dropna=False)\nwith pd.option_context('display.max_rows', None):\n    print(jan23[\"VEHICLE TYPE CODE 1\"].value_counts(dropna=False))\n\nSedan                                  3478\nStation Wagon/Sport Utility Vehicle    2505\nTaxi                                    177\nPick-up Truck                           162\nNaN                                     136\nBus                                     135\nBox Truck                               110\nBike                                     84\nAmbulance                                55\nTractor Truck Diesel                     53\nE-Bike                                   50\nVan                                      43\nMotorcycle                               28\nE-Scooter                                28\nDump                                     18\nGarbage or Refuse                        17\nFlat Bed                                 17\nMoped                                    17\nPK                                       16\nConvertible                              11\nChassis Cab                               9\nCarry All                                 9\nTanker                                    7\nTow Truck / Wrecker                       7\nTractor Truck Gasoline                    7\nLIMO                                      5\nAMBULANCE                                 5\nMotorscooter                              5\nFlat Rack                                 3\nMotorbike                                 3\n4 dr sedan                                3\nConcrete Mixer                            2\nFiretruck                                 2\nMOTOR SCOO                                2\nSTAK                                      2\nSANITATION                                1\nBeverage Truck                            1\nSCHOOL BUS                                1\nWaste truc                                1\nForklift                                  1\nAMBU                                      1\nUTILITY                                   1\nLog                                       1\nPAS                                       1\n3-Door                                    1\nFDNY AMBUL                                1\nMulti-Wheeled Vehicle                     1\nScooter                                   1\nStake or Rack                             1\nLift Boom                                 1\nGas Moped                                 1\nBulk Agriculture                          1\nArmored Truck                             1\nTRAILER                                   1\nelectric s                                1\nMotorized                                 1\nVAn                                       1\nELECTRIC S                                1\nUTIL                                      1\nFORK LIFT                                 1\nSTREET SWE                                1\nGARBAGE TR                                1\nChevy                                     1\ncart                                      1\nTow Truck                                 1\nNYC FDNY #                                1\nScooter ga                                1\nVAN TRUCK                                 1\nName: VEHICLE TYPE CODE 1, dtype: int64\n\n\nCross-tables\n\npd.crosstab(index = jan23[\"CONTRIBUTING FACTOR VEHICLE 1\"],\n            columns = jan23[\"BOROUGH\"], dropna = False)\n\n\n\n\n\n  \n    \n      BOROUGH\n      BRONX\n      BROOKLYN\n      MANHATTAN\n      QUEENS\n      STATEN ISLAND\n    \n    \n      CONTRIBUTING FACTOR VEHICLE 1\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Accelerator Defective\n      3\n      4\n      1\n      1\n      0\n    \n    \n      Aggressive Driving/Road Rage\n      9\n      12\n      6\n      12\n      0\n    \n    \n      Alcohol Involvement\n      7\n      32\n      9\n      30\n      8\n    \n    \n      Animals Action\n      0\n      0\n      0\n      0\n      1\n    \n    \n      Backing Unsafely\n      31\n      57\n      29\n      54\n      10\n    \n    \n      Brakes Defective\n      3\n      6\n      2\n      4\n      1\n    \n    \n      Cell Phone (hand-Held)\n      0\n      1\n      0\n      0\n      0\n    \n    \n      Driver Inattention/Distraction\n      177\n      415\n      211\n      307\n      45\n    \n    \n      Driver Inexperience\n      15\n      41\n      20\n      18\n      6\n    \n    \n      Driverless/Runaway Vehicle\n      1\n      1\n      2\n      3\n      0\n    \n    \n      Drugs (illegal)\n      2\n      5\n      1\n      0\n      0\n    \n    \n      Failure to Keep Right\n      1\n      2\n      0\n      2\n      0\n    \n    \n      Failure to Yield Right-of-Way\n      58\n      108\n      51\n      157\n      12\n    \n    \n      Fatigued/Drowsy\n      0\n      0\n      0\n      3\n      1\n    \n    \n      Fell Asleep\n      3\n      8\n      3\n      10\n      0\n    \n    \n      Following Too Closely\n      30\n      56\n      38\n      69\n      10\n    \n    \n      Glare\n      2\n      2\n      1\n      2\n      1\n    \n    \n      Illnes\n      1\n      1\n      0\n      5\n      1\n    \n    \n      Lane Marking Improper/Inadequate\n      1\n      0\n      0\n      0\n      0\n    \n    \n      Lost Consciousness\n      2\n      3\n      1\n      4\n      2\n    \n    \n      Obstruction/Debris\n      1\n      1\n      1\n      1\n      1\n    \n    \n      Other Electronic Device\n      0\n      0\n      1\n      1\n      0\n    \n    \n      Other Lighting Defects\n      1\n      0\n      0\n      0\n      0\n    \n    \n      Other Vehicular\n      40\n      33\n      28\n      29\n      3\n    \n    \n      Outside Car Distraction\n      5\n      5\n      0\n      3\n      0\n    \n    \n      Oversized Vehicle\n      5\n      7\n      2\n      3\n      1\n    \n    \n      Passenger Distraction\n      1\n      4\n      5\n      4\n      0\n    \n    \n      Passing Too Closely\n      41\n      70\n      36\n      44\n      1\n    \n    \n      Passing or Lane Usage Improper\n      26\n      68\n      33\n      80\n      9\n    \n    \n      Pavement Defective\n      0\n      2\n      0\n      0\n      0\n    \n    \n      Pavement Slippery\n      1\n      6\n      5\n      5\n      0\n    \n    \n      Pedestrian/Bicyclist/Other Pedestrian Error/Confusion\n      10\n      14\n      17\n      3\n      1\n    \n    \n      Physical Disability\n      1\n      0\n      0\n      1\n      1\n    \n    \n      Reaction to Uninvolved Vehicle\n      1\n      6\n      10\n      12\n      4\n    \n    \n      Steering Failure\n      5\n      4\n      0\n      7\n      1\n    \n    \n      Texting\n      0\n      0\n      0\n      0\n      1\n    \n    \n      Tinted Windows\n      0\n      1\n      1\n      0\n      0\n    \n    \n      Tire Failure/Inadequate\n      0\n      2\n      0\n      3\n      1\n    \n    \n      Traffic Control Device Improper/Non-Working\n      0\n      0\n      0\n      1\n      0\n    \n    \n      Traffic Control Disregarded\n      25\n      39\n      16\n      50\n      8\n    \n    \n      Turning Improperly\n      22\n      48\n      20\n      35\n      11\n    \n    \n      Unsafe Lane Changing\n      7\n      16\n      14\n      9\n      2\n    \n    \n      Unsafe Speed\n      29\n      32\n      22\n      56\n      8\n    \n    \n      Unspecified\n      229\n      508\n      209\n      273\n      38\n    \n    \n      View Obstructed/Limited\n      15\n      15\n      7\n      12\n      4\n    \n    \n      Windshield Inadequate\n      1\n      0\n      0\n      0\n      0"
  },
  {
    "objectID": "nycrash.html#some-cleaning",
    "href": "nycrash.html#some-cleaning",
    "title": "12  NYC Crash Data",
    "section": "12.2 Some Cleaning",
    "text": "12.2 Some Cleaning\nQuestions from Dr. Douglas Bates:\n\nThe CRASH_DATEs are all in the correct month and there are no missing values\nThere are no missing values in the CRASH_TIMEs but there are 117 values of exactly 00:00:00. Is this a matter of bad luck when the clock strikes midnight?\nOver 1/3 of the ZIP_CODE and BOROUGH values are missing. There are the same number of missing values in these columns - do they always co-occur? If LATITUDE and LONGITUDE are available, can they be used to infer the ZIP_CODE?\nThere are 178 unique non-missing ZIP_CODE values as stated in the Jamboree description. (“Trust, but verify.”) Is there really a zip code of 10000 in New York?\nThere are 20 values of 0.0 for LATITUDE and LONGITUDE? These are obviously incorrect - should they be coded as missing?\nIs it redundant to keep the LOCATIO in addition to LATITUDE and LONGITUDE?\nThe COLLISION_ID is unique to each row and can be used as a key. The values are not consecutive - why not?\nThe NUMBER_OF_... columns seem reasonable. A further consistency check is suggested in the Jamboree tasks.\nIn the CONTRIBUTING_FACTOR_… columns, is Unspecified different from missing?\nThe codes in the VEHICLE_TYPE_CODE_... columns are the usual hodge-podge of results from “free-form” data entry. Should unk, UNK, UNKNOWN, and Unknown be converted to missing?\nIn contrast, the codes in the CONTRIBUTING_FACTOR_... columns appear to be standardized (not sure why Illnes isn’t Illness).\n\n\nwith pd.option_context('display.max_rows', None):\n    print(jan23[\"CRASH TIME\"].value_counts())\n\n0:00     116\n15:00     75\n18:00     72\n17:00     69\n10:00     65\n8:00      61\n8:30      59\n13:00     58\n7:00      58\n14:00     56\n19:00     55\n16:00     54\n23:00     54\n12:00     54\n9:00      53\n20:00     53\n17:30     51\n14:30     51\n10:30     51\n12:30     51\n11:00     49\n9:30      42\n15:30     40\n16:30     39\n18:30     38\n21:00     38\n6:00      36\n11:30     35\n20:30     35\n6:30      33\n4:00      32\n1:00      31\n5:00      31\n22:00     31\n13:30     30\n23:30     27\n22:30     27\n6:50      27\n21:30     26\n18:20     26\n15:20     25\n17:20     25\n15:15     24\n18:45     24\n14:50     23\n7:30      23\n15:45     23\n1:30      23\n14:40     23\n8:20      23\n19:30     23\n9:45      22\n13:20     22\n3:00      22\n16:20     22\n7:50      22\n18:40     21\n21:45     21\n16:45     21\n17:45     21\n10:50     21\n16:10     21\n8:40      20\n10:40     20\n8:15      20\n20:50     19\n1:20      19\n13:40     19\n18:35     19\n11:20     19\n17:40     19\n6:20      19\n9:40      19\n2:00      19\n18:50     18\n18:10     18\n9:20      18\n9:15      18\n20:15     18\n12:50     18\n16:35     18\n0:30      18\n12:20     18\n14:45     18\n22:40     18\n12:15     18\n16:40     18\n12:45     18\n19:40     18\n10:45     17\n17:50     17\n19:50     17\n19:20     17\n8:45      17\n15:50     17\n17:55     17\n10:20     17\n12:10     17\n3:30      16\n13:45     16\n17:35     16\n7:45      16\n11:40     16\n15:10     16\n16:15     16\n11:45     16\n14:15     16\n15:40     16\n13:15     15\n9:50      15\n17:15     15\n1:45      15\n6:45      15\n19:10     15\n12:40     15\n18:05     15\n22:20     14\n16:50     14\n17:25     14\n21:40     14\n20:40     14\n5:30      14\n8:50      14\n11:15     14\n22:45     14\n12:25     14\n11:50     14\n16:55     14\n11:10     14\n14:20     13\n14:35     13\n8:55      13\n19:45     13\n17:10     13\n11:55     13\n8:25      13\n4:30      13\n7:20      13\n14:55     13\n6:40      13\n23:40     12\n6:55      12\n19:15     12\n6:05      12\n11:05     12\n10:15     12\n7:05      12\n8:35      12\n7:15      12\n13:10     12\n15:35     12\n22:15     12\n23:45     12\n5:45      12\n19:05     12\n21:50     11\n7:10      11\n16:25     11\n9:25      11\n14:10     11\n19:55     11\n14:05     11\n17:05     11\n5:35      11\n22:10     11\n20:10     11\n10:25     11\n13:25     11\n15:05     10\n20:45     10\n6:15      10\n19:35     10\n23:15     10\n3:50      10\n20:35     10\n0:40      10\n5:10      10\n18:15     10\n23:20     10\n21:15     10\n2:20      10\n7:25      10\n9:05       9\n4:50       9\n23:50      9\n18:29      9\n1:50       9\n20:20      9\n9:10       9\n13:05      9\n2:30       9\n19:25      9\n16:05      9\n11:04      9\n13:50      9\n20:25      9\n22:50      9\n7:40       9\n15:25      9\n0:20       9\n16:49      9\n10:35      9\n18:25      9\n1:10       8\n22:41      8\n21:20      8\n0:55       8\n3:45       8\n0:10       8\n12:35      8\n3:10       8\n1:55       8\n23:05      8\n8:17       8\n12:05      8\n5:40       8\n18:55      8\n8:10       8\n8:05       8\n17:16      8\n5:50       8\n10:55      8\n23:35      7\n18:08      7\n18:51      7\n11:34      7\n21:10      7\n6:10       7\n2:40       7\n22:25      7\n10:05      7\n17:54      7\n3:20       7\n16:48      7\n18:34      7\n5:20       7\n15:55      7\n4:20       7\n20:05      7\n1:15       7\n10:28      7\n4:40       7\n12:55      7\n18:18      7\n1:05       7\n23:25      7\n15:43      7\n4:15       7\n3:15       7\n7:35       7\n22:35      7\n21:35      7\n19:19      6\n14:51      6\n11:25      6\n10:10      6\n17:13      6\n20:37      6\n20:23      6\n14:58      6\n17:08      6\n19:49      6\n17:39      6\n21:08      6\n23:13      6\n22:55      6\n12:08      6\n10:19      6\n23:23      6\n7:48       6\n16:28      6\n14:22      6\n18:22      6\n8:44       6\n18:58      6\n16:33      6\n6:54       6\n7:55       6\n7:06       6\n8:08       6\n2:15       6\n21:04      6\n21:25      6\n19:18      6\n5:55       6\n3:25       6\n5:05       6\n0:45       6\n20:55      6\n17:29      6\n15:33      6\n2:35       6\n0:50       6\n14:54      6\n15:57      6\n13:24      6\n13:36      6\n18:28      6\n2:50       6\n8:14       6\n6:25       6\n10:37      6\n21:01      5\n19:51      5\n0:06       5\n17:11      5\n3:40       5\n16:37      5\n18:16      5\n14:57      5\n20:14      5\n19:57      5\n1:25       5\n7:19       5\n18:06      5\n13:34      5\n13:52      5\n20:59      5\n8:28       5\n19:36      5\n13:28      5\n14:11      5\n12:46      5\n17:44      5\n2:27       5\n16:38      5\n17:46      5\n8:58       5\n19:26      5\n15:26      5\n22:18      5\n11:56      5\n14:25      5\n11:32      5\n18:48      5\n19:11      5\n8:47       5\n13:04      5\n18:07      5\n20:24      5\n9:35       5\n21:58      5\n23:06      5\n8:16       5\n19:52      5\n12:51      5\n17:28      5\n4:37       5\n18:04      5\n17:26      5\n13:48      5\n13:55      5\n10:54      5\n18:52      5\n17:58      5\n21:06      5\n19:21      5\n16:16      5\n14:28      5\n17:12      5\n18:37      5\n22:12      5\n11:35      5\n18:44      5\n14:44      5\n1:40       5\n13:07      5\n18:24      5\n0:25       5\n12:48      5\n0:09       5\n21:05      5\n13:35      5\n0:35       5\n21:55      5\n9:22       5\n20:48      5\n21:29      5\n21:07      5\n11:17      5\n16:58      4\n13:56      4\n6:37       4\n14:21      4\n7:41       4\n20:18      4\n5:44       4\n6:49       4\n16:02      4\n20:49      4\n14:39      4\n5:12       4\n22:05      4\n13:09      4\n8:43       4\n4:10       4\n12:54      4\n19:28      4\n17:14      4\n5:38       4\n1:21       4\n15:21      4\n0:18       4\n0:15       4\n18:57      4\n21:03      4\n19:06      4\n17:48      4\n23:38      4\n12:18      4\n6:53       4\n21:23      4\n19:59      4\n16:31      4\n10:04      4\n13:54      4\n8:02       4\n6:52       4\n16:51      4\n23:33      4\n0:42       4\n17:17      4\n9:44       4\n19:12      4\n19:13      4\n22:17      4\n16:27      4\n15:48      4\n10:48      4\n8:52       4\n7:18       4\n17:52      4\n7:08       4\n15:04      4\n9:59       4\n14:26      4\n20:17      4\n17:57      4\n6:38       4\n20:32      4\n23:19      4\n18:14      4\n11:24      4\n19:47      4\n12:03      4\n10:18      4\n14:06      4\n14:38      4\n19:16      4\n3:35       4\n4:25       4\n16:13      4\n8:12       4\n14:53      4\n21:13      4\n6:58       4\n14:56      4\n8:36       4\n18:26      4\n13:53      4\n11:23      4\n22:14      4\n15:41      4\n9:41       4\n13:57      4\n13:02      4\n15:16      4\n15:13      4\n10:11      4\n4:05       4\n20:06      4\n11:42      4\n14:37      4\n5:07       4\n0:05       4\n10:56      4\n8:21       4\n21:47      4\n9:55       4\n16:04      4\n17:36      4\n22:58      4\n8:34       4\n8:46       4\n16:14      4\n15:58      4\n15:42      4\n6:27       4\n19:23      4\n16:44      4\n8:59       4\n23:10      4\n12:37      4\n18:43      4\n21:16      4\n5:15       4\n22:23      4\n11:18      4\n17:49      4\n4:55       4\n15:08      4\n10:38      4\n9:28       4\n13:18      4\n20:31      4\n16:47      4\n13:43      4\n9:11       4\n9:57       3\n12:28      3\n12:07      3\n2:45       3\n3:17       3\n3:02       3\n0:13       3\n12:47      3\n12:29      3\n23:39      3\n23:48      3\n12:33      3\n18:38      3\n20:41      3\n16:12      3\n16:43      3\n14:14      3\n16:32      3\n23:09      3\n21:44      3\n18:09      3\n22:39      3\n17:51      3\n6:59       3\n1:18       3\n7:42       3\n23:54      3\n17:34      3\n11:19      3\n5:27       3\n8:41       3\n23:55      3\n23:07      3\n21:51      3\n14:24      3\n9:03       3\n23:01      3\n7:39       3\n22:13      3\n22:46      3\n3:05       3\n18:17      3\n19:46      3\n7:51       3\n15:52      3\n9:07       3\n20:51      3\n11:38      3\n14:33      3\n14:02      3\n8:48       3\n8:57       3\n15:22      3\n18:56      3\n18:47      3\n15:44      3\n21:21      3\n4:45       3\n17:06      3\n16:59      3\n17:21      3\n17:42      3\n20:44      3\n18:12      3\n12:01      3\n17:43      3\n18:02      3\n7:52       3\n22:53      3\n10:39      3\n14:01      3\n19:33      3\n17:41      3\n20:42      3\n12:34      3\n6:35       3\n2:55       3\n20:11      3\n1:42       3\n17:47      3\n12:38      3\n23:57      3\n18:42      3\n16:24      3\n4:34       3\n9:12       3\n0:19       3\n17:18      3\n2:10       3\n2:07       3\n0:47       3\n22:08      3\n0:57       3\n21:09      3\n7:38       3\n14:03      3\n4:36       3\n20:39      3\n9:54       3\n13:44      3\n19:04      3\n14:16      3\n11:29      3\n2:21       3\n15:12      3\n13:46      3\n1:29       3\n11:53      3\n14:27      3\n18:03      3\n20:52      3\n12:11      3\n18:41      3\n7:26       3\n10:33      3\n21:33      3\n8:13       3\n19:09      3\n0:11       3\n1:35       3\n14:29      3\n17:38      3\n6:07       3\n19:03      3\n3:23       3\n0:01       3\n16:26      3\n22:52      3\n19:53      3\n11:51      3\n17:31      3\n21:54      3\n17:07      3\n3:11       3\n8:42       3\n9:26       3\n9:42       3\n16:21      3\n17:22      3\n13:37      3\n9:53       3\n20:07      3\n17:33      3\n23:31      3\n19:32      3\n0:04       3\n18:32      3\n19:48      3\n22:11      3\n20:26      3\n7:12       3\n16:36      3\n16:46      3\n8:49       3\n11:13      3\n15:32      3\n14:12      3\n21:11      3\n12:39      3\n9:51       3\n1:34       3\n18:19      3\n14:04      3\n17:59      3\n20:12      3\n9:18       3\n10:17      3\n21:38      3\n6:06       3\n3:29       3\n21:02      3\n22:36      3\n4:35       3\n10:57      3\n22:24      3\n19:38      3\n9:29       3\n9:47       3\n8:24       3\n11:16      3\n17:53      3\n13:31      3\n18:27      3\n17:02      3\n13:14      3\n10:36      3\n16:41      3\n3:16       3\n19:54      3\n12:59      3\n2:26       3\n9:01       3\n21:42      2\n9:06       2\n11:52      2\n0:07       2\n11:49      2\n18:13      2\n17:24      2\n12:04      2\n5:59       2\n15:51      2\n11:48      2\n17:19      2\n3:37       2\n19:31      2\n7:01       2\n8:38       2\n4:51       2\n21:56      2\n15:19      2\n19:56      2\n21:22      2\n18:39      2\n20:38      2\n15:11      2\n14:31      2\n0:24       2\n3:53       2\n9:04       2\n20:57      2\n20:16      2\n15:47      2\n20:28      2\n7:09       2\n9:49       2\n20:29      2\n16:22      2\n14:47      2\n9:17       2\n9:19       2\n11:33      2\n17:37      2\n18:23      2\n8:27       2\n21:52      2\n5:49       2\n1:51       2\n14:13      2\n20:09      2\n11:44      2\n12:23      2\n9:08       2\n22:51      2\n10:32      2\n13:49      2\n16:08      2\n16:17      2\n6:46       2\n22:38      2\n12:16      2\n9:31       2\n19:17      2\n7:56       2\n12:52      2\n2:53       2\n14:36      2\n22:27      2\n15:49      2\n13:27      2\n19:58      2\n15:29      2\n10:34      2\n12:53      2\n12:13      2\n18:53      2\n5:25       2\n12:14      2\n14:52      2\n10:46      2\n4:57       2\n15:54      2\n23:14      2\n14:59      2\n1:44       2\n1:47       2\n15:38      2\n15:06      2\n6:31       2\n23:12      2\n19:14      2\n7:44       2\n10:12      2\n11:28      2\n13:59      2\n0:12       2\n23:18      2\n15:39      2\n21:27      2\n23:28      2\n4:19       2\n7:04       2\n21:41      2\n20:13      2\n4:28       2\n23:24      2\n3:55       2\n19:22      2\n7:46       2\n14:17      2\n21:26      2\n16:42      2\n12:12      2\n9:43       2\n2:52       2\n17:23      2\n16:18      2\n4:42       2\n8:06       2\n6:19       2\n15:01      2\n15:34      2\n23:44      2\n5:47       2\n13:17      2\n15:28      2\n18:36      2\n10:41      2\n10:24      2\n17:27      2\n19:08      2\n1:13       2\n21:32      2\n2:29       2\n1:52       2\n4:16       2\n1:24       2\n17:04      2\n16:53      2\n20:47      2\n21:59      2\n20:22      2\n3:48       2\n0:34       2\n2:41       2\n12:44      2\n3:14       2\n18:46      2\n9:39       2\n6:21       2\n14:19      2\n4:54       2\n14:42      2\n0:02       2\n10:51      2\n3:07       2\n5:42       2\n3:04       2\n0:26       2\n2:11       2\n16:01      2\n20:53      2\n9:23       2\n8:04       2\n10:53      2\n16:29      2\n15:03      2\n22:47      2\n0:17       2\n7:17       2\n7:34       2\n10:26      2\n18:11      2\n18:21      2\n21:18      2\n10:01      2\n8:29       2\n15:36      2\n23:11      2\n2:25       2\n11:57      2\n2:42       2\n17:56      2\n20:02      2\n16:56      2\n6:18       2\n22:49      2\n6:44       2\n0:49       2\n13:13      2\n17:09      2\n23:59      2\n2:51       2\n17:32      2\n8:33       2\n13:41      2\n0:16       2\n23:21      2\n14:48      2\n9:46       2\n10:22      2\n12:41      2\n13:12      2\n19:41      2\n4:43       2\n4:53       2\n3:12       2\n0:43       2\n21:19      2\n16:07      2\n13:06      2\n21:37      2\n20:36      2\n14:09      2\n14:23      2\n23:02      2\n18:54      2\n23:42      2\n13:26      2\n5:01       2\n23:08      2\n0:32       2\n16:03      2\n1:32       2\n23:41      2\n8:09       2\n3:54       2\n11:47      2\n8:26       2\n22:04      2\n10:16      2\n23:52      2\n15:23      2\n16:34      2\n2:08       2\n8:01       2\n2:36       2\n5:36       2\n4:32       2\n0:27       2\n20:54      2\n15:18      2\n7:07       2\n22:01      2\n22:26      2\n18:49      2\n6:36       2\n20:04      2\n9:32       2\n22:16      2\n1:17       2\n12:57      2\n15:02      2\n1:01       2\n2:05       2\n1:38       2\n7:03       2\n16:11      2\n0:38       2\n13:22      2\n10:07      2\n13:01      2\n0:08       2\n5:09       2\n13:38      2\n23:16      2\n11:09      1\n2:56       1\n19:43      1\n1:36       1\n7:13       1\n16:09      1\n13:21      1\n3:57       1\n12:36      1\n22:42      1\n22:03      1\n3:56       1\n6:17       1\n11:07      1\n4:46       1\n13:32      1\n1:16       1\n2:22       1\n10:42      1\n4:07       1\n23:51      1\n9:36       1\n2:34       1\n20:19      1\n16:52      1\n11:46      1\n4:31       1\n10:23      1\n18:33      1\n7:58       1\n10:29      1\n3:46       1\n1:11       1\n1:22       1\n1:09       1\n22:33      1\n5:19       1\n11:36      1\n9:34       1\n12:17      1\n3:59       1\n23:53      1\n2:04       1\n13:39      1\n4:47       1\n10:02      1\n10:03      1\n6:43       1\n0:23       1\n21:53      1\n0:03       1\n2:54       1\n2:18       1\n6:33       1\n2:38       1\n0:58       1\n6:08       1\n14:32      1\n22:07      1\n13:47      1\n19:37      1\n5:37       1\n1:59       1\n6:14       1\n22:34      1\n5:22       1\n20:58      1\n1:19       1\n3:08       1\n0:41       1\n18:01      1\n9:16       1\n10:21      1\n7:29       1\n13:16      1\n1:12       1\n18:59      1\n2:59       1\n1:57       1\n11:03      1\n5:34       1\n5:33       1\n6:24       1\n0:37       1\n1:49       1\n1:56       1\n2:23       1\n11:22      1\n13:19      1\n4:48       1\n2:17       1\n0:28       1\n19:29      1\n7:37       1\n19:02      1\n2:47       1\n4:39       1\n4:27       1\n0:53       1\n4:09       1\n2:48       1\n3:47       1\n3:36       1\n11:43      1\n3:09       1\n4:49       1\n1:08       1\n6:47       1\n14:49      1\n0:33       1\n4:17       1\n16:54      1\n6:09       1\n12:42      1\n22:29      1\n23:22      1\n22:31      1\n0:31       1\n5:11       1\n5:32       1\n5:39       1\n7:31       1\n3:28       1\n8:32       1\n19:34      1\n5:41       1\n10:13      1\n12:27      1\n1:26       1\n6:23       1\n1:02       1\n22:56      1\n19:44      1\n22:54      1\n7:24       1\n6:29       1\n12:24      1\n8:31       1\n3:26       1\n22:37      1\n9:24       1\n23:46      1\n23:27      1\n1:31       1\n3:52       1\n7:54       1\n8:51       1\n9:27       1\n1:14       1\n5:24       1\n22:09      1\n3:43       1\n10:59      1\n5:02       1\n3:19       1\n6:51       1\n1:58       1\n5:46       1\n1:37       1\n15:07      1\n13:33      1\n12:06      1\n16:19      1\n11:59      1\n6:03       1\n22:02      1\n11:08      1\n20:01      1\n6:04       1\n15:14      1\n23:32      1\n21:36      1\n7:57       1\n19:01      1\n10:14      1\n12:56      1\n15:17      1\n10:43      1\n16:39      1\n3:03       1\n11:39      1\n12:49      1\n2:28       1\n9:58       1\n5:29       1\n11:58      1\n8:18       1\n11:12      1\n22:59      1\n6:41       1\n5:03       1\n7:33       1\n22:21      1\n6:56       1\n8:56       1\n17:03      1\n1:03       1\n7:59       1\n7:43       1\n15:31      1\n12:19      1\n15:24      1\n7:49       1\n6:16       1\n20:43      1\n20:21      1\n10:06      1\n6:22       1\n14:43      1\n15:46      1\n11:11      1\n7:23       1\n15:53      1\n5:28       1\n2:44       1\n10:08      1\n9:33       1\n19:07      1\n20:56      1\n23:47      1\n5:21       1\n7:22       1\n9:02       1\n3:44       1\n13:29      1\n4:58       1\n2:33       1\n2:13       1\n5:52       1\n11:37      1\n17:01      1\n23:17      1\n2:58       1\n6:13       1\n6:26       1\n7:28       1\n3:24       1\n11:06      1\n19:27      1\n4:08       1\n10:09      1\n13:08      1\n21:14      1\n0:56       1\n12:32      1\n12:02      1\n5:06       1\n15:27      1\n0:48       1\n11:54      1\n3:38       1\n4:23       1\n7:53       1\n14:46      1\n6:01       1\n21:12      1\n8:37       1\n2:02       1\n21:43      1\n3:42       1\n6:48       1\n23:49      1\n22:48      1\n1:39       1\n0:44       1\n22:43      1\n20:08      1\n5:56       1\n8:53       1\n13:58      1\n4:06       1\n2:03       1\n5:26       1\n11:14      1\n8:03       1\n0:36       1\n2:14       1\n23:56      1\n6:42       1\n8:23       1\n14:18      1\nName: CRASH TIME, dtype: int64\n\n\nFor example, here are some cleaning steps:\n\nimport numpy as np\n\njan23[\"CONTRIBUTING FACTOR VEHICLE 1\"] = (\n    jan23[\"CONTRIBUTING FACTOR VEHICLE 1\"].replace([\"Unspecified\"], np.nan))\njan23[\"CONTRIBUTING FACTOR VEHICLE 2\"] = (\n    jan23[\"CONTRIBUTING FACTOR VEHICLE 2\"].replace([\"Unspecified\"], np.nan))\njan23[\"CONTRIBUTING FACTOR VEHICLE 3\"] = (\n    jan23[\"CONTRIBUTING FACTOR VEHICLE 3\"].replace([\"Unspecified\"], np.nan))\njan23[\"CONTRIBUTING FACTOR VEHICLE 4\"] = (\n    jan23[\"CONTRIBUTING FACTOR VEHICLE 4\"].replace([\"Unspecified\"], np.nan))\njan23[\"CONTRIBUTING FACTOR VEHICLE 5\"] = (\n    jan23[\"CONTRIBUTING FACTOR VEHICLE 5\"].replace([\"Unspecified\"], np.nan))\njan23[\"LATITUDE\"] = jan23[\"LATITUDE\"].replace([0.0], np.nan)\njan23[\"LONGITUDE\"] = jan23[\"LONGITUDE\"].replace([0.0], np.nan)\njan23.describe()\n\n\n\n\n\n  \n    \n      \n      ZIP CODE\n      LATITUDE\n      LONGITUDE\n      NUMBER OF PERSONS INJURED\n      NUMBER OF PERSONS KILLED\n      NUMBER OF PEDESTRIANS INJURED\n      NUMBER OF PEDESTRIANS KILLED\n      NUMBER OF CYCLIST INJURED\n      NUMBER OF CYCLIST KILLED\n      NUMBER OF MOTORIST INJURED\n      NUMBER OF MOTORIST KILLED\n      COLLISION_ID\n    \n  \n  \n    \n      count\n      4796.000000\n      6683.000000\n      6683.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7.244000e+03\n    \n    \n      mean\n      10893.521685\n      40.722317\n      -73.918590\n      0.502761\n      0.002347\n      0.116372\n      0.000690\n      0.033269\n      0.000414\n      0.333103\n      0.001242\n      4.599022e+06\n    \n    \n      std\n      526.392428\n      0.081602\n      0.085654\n      0.813641\n      0.051164\n      0.397927\n      0.026265\n      0.180118\n      0.020348\n      0.749174\n      0.038951\n      2.365885e+03\n    \n    \n      min\n      10001.000000\n      40.504658\n      -74.250150\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      4.594332e+06\n    \n    \n      25%\n      10457.000000\n      40.665350\n      -73.965530\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      4.597113e+06\n    \n    \n      50%\n      11208.000000\n      40.713196\n      -73.922740\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      4.599058e+06\n    \n    \n      75%\n      11239.000000\n      40.777754\n      -73.867714\n      1.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      4.600953e+06\n    \n    \n      max\n      11694.000000\n      40.912827\n      -73.702095\n      21.000000\n      2.000000\n      19.000000\n      1.000000\n      2.000000\n      1.000000\n      8.000000\n      2.000000\n      4.605324e+06\n    \n  \n\n\n\n\nBy the data dictionary, OFF STREET NAME is the street address of the collision site. Some records have OFF STREET NAME but missing LATITUDE and LONGITUDE. The geocode can be filled by geocoding the street address with package"
  },
  {
    "objectID": "nycrash.html#filling-the-missing-zip-codes-by-reverse-geocoding",
    "href": "nycrash.html#filling-the-missing-zip-codes-by-reverse-geocoding",
    "title": "12  NYC Crash Data",
    "section": "12.3 Filling the Missing Zip Codes by Reverse Geocoding",
    "text": "12.3 Filling the Missing Zip Codes by Reverse Geocoding\nThe package uszipcode is the most powerful and easy to use programmable zipcode database in Python. It provides information about 42,724 zipcodes in the US with data crawled from <data.census.gov>. See its documentation for details.\n\nfrom uszipcode import SearchEngine\n\nsr = SearchEngine()\nsr.by_zipcode(\"10001\")\n\nSimpleZipcode(zipcode='10001', zipcode_type='STANDARD', major_city='New York', post_office_city='New York, NY', common_city_list=['New York'], county='New York County', state='NY', lat=40.75, lng=-74.0, timezone='America/New_York', radius_in_miles=0.9090909090909091, area_code_list='718,917,347,646', population=21102, population_density=33959.0, land_area_in_sqmi=0.62, water_area_in_sqmi=0.0, housing_units=12476, occupied_housing_units=11031, median_home_value=650200, median_household_income=81671, bounds_west=-74.008621, bounds_east=-73.984076, bounds_north=40.759731, bounds_south=40.743451)\n\n\nWe can use uszipcode to reverse geocode a point by its coordinates. The returned zipcode can be used to handle missing zipcode.\n\nz = sr.by_coordinates(40.769993, -73.915825, radius = 1)\nz[0].zipcode\nz[0].median_home_value\n\n597700\n\n\nOnce we have found the zipcode, we can find its borough. See the complete NYC zip code list.\n\ndef nyczip2burough(zip):\n    nzip = int(zip)\n    if nzip >= 10001 and nzip <= 10282:\n        return \"MANHATTAN\"\n    elif nzip >= 10301 and nzip <= 10314:\n        return \"STATEN ISLAND\"\n    elif nzip >= 10451 and nzip <= 10475:\n        return \"BRONX\"\n    elif nzip >= 11004 and nzip <= 11109:\n        return \"QUEENS\"\n    elif nzip >= 11351 and nzip <= 11697:\n        return \"QUEENS\"\n    elif nzip >= 11201 and nzip <= 11256:\n        return \"BROOKLYN\"\n    else:\n        return np.nan\n\nLet’s try it out:\n\nnyczip2burough(z[0].zipcode)\n\n'QUEENS'\n\n\nHere is a vectorized version:\n\nimport numpy as np\nimport pandas as pd\nfrom typing import Union, List\n\ndef nyczip2borough(zips: Union[np.ndarray, pd.Series]) -> Union[np.ndarray, pd.Series]:\n    zips = zips.values if isinstance(zips, pd.Series) else zips\n    condlist = [\n        (zips >= 10001) & (zips <= 10282),\n        (zips >= 10301) & (zips <= 10314),\n        (zips >= 10451) & (zips <= 10475),\n        (zips >= 11004) & (zips <= 11109),\n        (zips >= 11351) & (zips <= 11697),\n        (zips >= 11201) & (zips <= 11256),\n    ]\n    choicelist = [\n        \"MANHATTAN\",\n        \"STATEN ISLAND\",\n        \"BRONX\",\n        \"QUEENS\",\n        \"QUEENS\",\n        \"BROOKLYN\",\n    ]\n    result = np.select(condlist, choicelist, default=np.nan)\n    return pd.Series(result) if isinstance(zips, pd.Series) else result\n\nTry it out\n\nnyczip2borough(jan23[\"ZIP CODE\"].dropna().head(10))\n\narray(['BROOKLYN', 'QUEENS', 'MANHATTAN', 'QUEENS', 'BROOKLYN', 'QUEENS',\n       'QUEENS', 'BROOKLYN', 'QUEENS', 'STATEN ISLAND'], dtype='<U32')\n\n\nThe uszipcode package provides databases at the zip code level from the US Census. Such information could be merged with the NYC crash data for further analysis.\n\nfrom uszipcode import SearchEngine, SimpleZipcode\nimport os\n# set the default database file location\ndb_file = os.path.join(os.getenv(\"HOME\"), \"simple_db.sqlite\")\nsearch = SearchEngine(db_file_path=db_file)\nsearch.by_zipcode(\"10030\")\n\nSimpleZipcode(zipcode='10030', zipcode_type='STANDARD', major_city='New York', post_office_city='New York, NY', common_city_list=['New York'], county='New York County', state='NY', lat=40.82, lng=-73.94, timezone='America/New_York', radius_in_miles=0.5681818181818182, area_code_list='212,646,917', population=26999, population_density=96790.0, land_area_in_sqmi=0.28, water_area_in_sqmi=0.0, housing_units=12976, occupied_housing_units=11395, median_home_value=509000, median_household_income=31925, bounds_west=-73.948677, bounds_east=-73.936232, bounds_north=40.824032, bounds_south=40.812791)\n\n\nThe SQL database of US zip code is stored in $HOME/.uszipcode. It can be imported as a pandas dataframe.\n\nimport sqlite3\nimport pandas as pd\n# change to your own path after installing uszipcode\ncon = sqlite3.connect(db_file)\nzipdf = pd.read_sql_query(\"SELECT * from simple_zipcode\", con)\nzipdf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 42724 entries, 0 to 42723\nData columns (total 24 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   zipcode                  42724 non-null  object \n 1   zipcode_type             42724 non-null  object \n 2   major_city               42724 non-null  object \n 3   post_office_city         33104 non-null  object \n 4   common_city_list         41877 non-null  object \n 5   county                   42724 non-null  object \n 6   state                    42724 non-null  object \n 7   lat                      42724 non-null  float64\n 8   lng                      42724 non-null  float64\n 9   timezone                 42724 non-null  object \n 10  radius_in_miles          33104 non-null  float64\n 11  area_code_list           42724 non-null  object \n 12  population               31448 non-null  float64\n 13  population_density       31448 non-null  float64\n 14  land_area_in_sqmi        31448 non-null  float64\n 15  water_area_in_sqmi       31448 non-null  float64\n 16  housing_units            31448 non-null  float64\n 17  occupied_housing_units   31448 non-null  float64\n 18  median_home_value        31448 non-null  float64\n 19  median_household_income  31448 non-null  float64\n 20  bounds_west              33104 non-null  float64\n 21  bounds_east              33104 non-null  float64\n 22  bounds_north             33104 non-null  float64\n 23  bounds_south             33104 non-null  float64\ndtypes: float64(15), object(9)\nmemory usage: 7.8+ MB\n\n\nThe zip code dataframe can be merged with the crash dataframe."
  },
  {
    "objectID": "nycrash.html#map-the-crash-sites",
    "href": "nycrash.html#map-the-crash-sites",
    "title": "12  NYC Crash Data",
    "section": "12.4 Map the Crash Sites",
    "text": "12.4 Map the Crash Sites\nWe can do this with package gmplot. See instructions from this tutorial.\n\nimport gmplot\nimport numpy as np\n\n# prepare the geododes\nlatitude  = jan23[\"LATITUDE\"].dropna().values\nlongitude = jan23[\"LONGITUDE\"].dropna().values\n\n# center of the map and zoom level\ngmap = gmplot.GoogleMapPlotter(40.769737, -73.91244, 14)\n\n# plot heatmap\ngmap.heatmap(latitude, longitude)\ngmap.scatter(latitude, longitude, c = 'r', marker = True)\n# gmap.scatter(latitude, longitude, '#FF0000', size = 50, marker = False)\n# Your Google_API_Key\n# gmap.apikey = \"put your key here\"\n# save it to html\ngmap.draw(r\"nycrashmap.html\")"
  },
  {
    "objectID": "exercises.html",
    "href": "exercises.html",
    "title": "13  Exercises",
    "section": "",
    "text": "Pick up Git basics and set up an account at GitHub if you don’t have one. Please practice the tips on Git in the notes. Make sure you have at least 10 commits in the repo, each with informative message. Keep checking the status of your repo with git status. My grader will grade the repo.\n\nClone the ids-s23 repo to your own computer.\nAdd your name and wishes to the Wishlist; commit with an informative message.\nRemove the Last, First entry from the list; commit.\nCreate a new file called add.qmd containing a few lines of texts; commit.\nRemove add.qmd (pretending that this is by accident; commit.\nRecover the accidently removed file add.qmd; add a long line (a paragraph without a hard break); add a short line (under 80 characters); commit.\nChange one word in the long line and one word in the short line; use git diff to see the difference from the last commit; commit.\nPut the repo into the GitHub Classroom homework repo with git remote add and git push.\n\nGet ready for contributing to the classnotes.\n\nCreate a fork of the ids-s23 repo into your own GitHub account.\nClone it to your local computer.\nMake a new branch to experiment with your changes.\nCheckout your branch and add your wishes to the wish list; push to your GitHub account.\nMake a pull request to my ids-s23 repo from your fork at GitHub. Make sure you have clear messages to document the changes.\n\nWrite a function to demonstrate the Monty Hall problem through simulation. The function takes two arguments ndoors and ntrials, representing the number of doors in the experiment and the number of trails in a simulation, respectively. The function should return the proportion of wins for both the switch and no-switch strategy. Apply your function with 3 doors and 5 doors, both with 1000 trials. Include sufficient text around the code to explain your them.\nWrite a function to do a Monte Carlo approximation of \\(\\pi\\). The function takes a Monte Carlo sample size n as input, and returns a point estimate of \\(\\pi\\) and a 95% confidence interval. Apply your function with sample size 1000, 2000, 4000, and 8000. Repeat the experiment 1000 times for each sample size and check the empirical probability that the confidence intervals cover the true value of \\(\\pi\\). Comment on the results.\nFind the first 10-digit prime number occurring in consecutive digits of \\(e\\). This was a Google recruiting ad\nThe NYC motor vehicle collisions data with documentation is available from NYC Open Data. The raw data needs some cleaning. (JY: Add variable name cleaning next year.)\n\nUse the filter from the website to download the crash data of January 2023; save it under a directory data with an informative name (e.g., nyc_crashes_202301.csv).\nGet basic summaries of each variable: missing percentage; descriptive statistics for continuous variables; frequency tables for discrete variables.\nAre the LATITUDE and LONGITIDE values all look legitimate? If not (e.g., zeroes), code them as missing values.\nIf OFF STREET NAME is not missing, are there any missing LATITUDE and LONGITUDE? If so, geocode the addresses.\n(Optional) Are the missing patterns of ON STREET NAME and LATITUDE the same? Summarize the missing patterns by a cross table. If ON STREET NAME and CROSS STREET NAME are available, use geocoding by intersection to fill the LATITUDE and LONGITUDE.\nAre ZIP CODE and BOROUGH always missing together? If LATITUDE and LONGITUDE are available, use reverse geocoding to fill the ZIP CODE and BOROUGH.\nPrint the whole frequency table of CONTRIBUTING FACTOR VEHICLE 1. Convert lower cases to uppercases and check the frequencies again.\nProvided an opportunity to meet the data provider, what suggestions do you have to make the data better based on your data exploration experience?\n\nExcept the first problem, use the cleaned data set with missing geocode imputed (data/nyc_crashes_202301_cleaned.csv).\n\nConstruct a contigency table for missing in geocode (latitude and longitude) by borough. Is the missing pattern the same across borough? Formulate a hypothesis and test it.\nConstruct a hour variable with integer values from 0 to 23. Plot the histogram of the number of crashes by hour. Plot it by borough.\nOverlay the locations of the crashes on a map of NYC. The map could be a static map or Google map.\nCreate a new variable injury which is one if the number of persons injured is 1 or more; and zero otherwise. Construct a cross table for injury versus borough. Test the null hypothesis that the two variables are not associated.\nMerge the crash data with the zip code database.\nFit a logistic model with injury as the outcome variable and covariates that are available in the data or can be engineered from the data. For example, zip code level covariates can be obtained by merging with the zip code database.\n\nUsing the cleaned NYC crash data, perform classification of injury with support vector machine and compare the results with the benchmark from regularized logistic regression. Use the last week’s data as testing data.\n\nExplain the parameters you used in your fitting for each method.\nExplain the confusion matrix retult from each fit.\nCompare the performance of the two approaches in terms of accuracy, precision, recall, F1-score, and AUC.\n\n(Mid-term team project) The NYC Open Data of 311 Service Requests contains all requests from 2010 to present. We consider a subset of it with request time between 00:00:00 01/15/2023 and 24:00:00 01/21/2023. The subset is available in CSV format as data/nyc311_011523-012123_by022023.csv. Read the data dictionary to understand the meaning of the variables,\n\nClean the data: fill missing fields as much as possible; check for obvious data entry errors (e.g., can Closed Date be earlier than Created Date?); summarize your suggestions to the data curator in several bullet points.\nRemove requests that are not made to NYPD and create a new variable duration, which represents the time period from the Created Date to Closed Date. Note that duration may be censored for some requests. Visualize the distribution of uncensored duration by weekdays/weekend and by borough, and test whether the distributions are the same across weekdays/weekends of their creation and across boroughs.\nDefine a binary variable over3h which is 1 if duration is greater than 3 hours. Note that it can be obtained even for censored duration. Build a model to predict over3h. If your model has tuning parameters, justify their choices. Apply this model to the 311 requests of NYPD in the week of 01/22/2023. Assess the performance of your model.\nNow you know the data quite well. Come up with a research question of interest that can be answered by the data, which could be analytics or visualizations. Perform the needed analyses and answer your question."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "VanderPlas, Jake. 2016. Python Data Science Handbook:\nEssential Tools for Working with Data. O’Reilly Media,\nInc."
  }
]