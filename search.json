[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Data Science",
    "section": "",
    "text": "Preface\nThe notets are a Quarto book; for details visit https://quarto.org/docs/books.\nThe notes are a joint effort of the instructor and the students in STAT 3255/5255, Spring 2023. The GitHub repo is at https://github.com/statds/ids-s23. The GitHub repo of the notes from Spring 2022 are available at https://github.com/statds/ids-s22.\nAn interesting quote from VanderPlas (2016):\n\nWhen a technologically-minded person is asked to help a friend, family member, or colleague with a computer problem, most of the time it’s less a matter of knowing the answer as much as knowing how to quickly find an unknown answer. In data science it’s the same: searchable web resources such as online documentation, mailing-list threads, and StackOverflow answers contain a wealth of information, even (especially?) if it is a topic you’ve found yourself searching before. Being an effective practitioner of data science is less about memorizing the tool or command you should use for every possible situation, and more about learning to effectively find the information you don’t know, whether through a web search engine or another means.\n\n\n\n\n\nVanderPlas, Jake. 2016. Python Data Science Handbook: Essential Tools for Working with Data. O’Reilly Media, Inc."
  },
  {
    "objectID": "intro.html#what-is-data-science",
    "href": "intro.html#what-is-data-science",
    "title": "1  Introduction",
    "section": "1.1 What Is Data Science?",
    "text": "1.1 What Is Data Science?\nOne widely accepted concept is the three pillars of data science: mathematics/statistics, computer science, and domain knowledge.\nIn her 2014 Presidential Address, Prof. Bin Yu, then President of the Institute of Mathematical Statistics, gave an interesting definition: \\[\n\\mbox{Data Science} =\n\\mbox{S}\\mbox{D}\\mbox{C}^3,\n\\] where S is Statistics, D is domain/science knowledge, and the three C’s are computing, collaboration/teamwork, and communication to outsiders."
  },
  {
    "objectID": "intro.html#expectations-from-this-course",
    "href": "intro.html#expectations-from-this-course",
    "title": "1  Introduction",
    "section": "1.2 Expectations from This Course",
    "text": "1.2 Expectations from This Course\n\nProficiency in project management with Git.\nProficiency in project report with Quarto.\nHands-on experience with real-world data science project.\nCompetency in using Python and its extensions for data science.\nFull grasp of the meaning of the results from data science algorithms.\nBasic understanding the principles of the data science methods."
  },
  {
    "objectID": "intro.html#computing-environment",
    "href": "intro.html#computing-environment",
    "title": "1  Introduction",
    "section": "1.3 Computing Environment",
    "text": "1.3 Computing Environment\nAll setups are operating system dependent. As soon as possible, stay away from Windows. Otherwise, good luck (you will need it).\n\n1.3.1 Command Line Interface\nOn Linux or MacOS, simply open a terminal.\nOn Windows, several options can be considered.\n\nCygwin (with X): https://x.cygwin.com\nGit Bash: https://www.gitkraken.com/blog/what-is-git-bash\n\nTo jump start, here is a tutorial: Ubunto Linux for beginners.\nAt least, you need to know how to handle files and traverse across directories. The tab completion and introspection supports are very useful.\n\n\n1.3.2 Python\nSet up Python on your computer:\n\nPython 3.\nPython package manager miniconda or pip.\nIntegrated Development Environment (IDE) (Jupyter Notebook; RStudio; VS Code; Emacs; etc.)\n\nI will be using IPython and Jupyter Notebook in class.\nReadability is important! Check your Python coding styles against the recommended styles: https://peps.python.org/pep-0008/. A good place to start is the Section on “Code Lay-out”.\nOnline books on Python for data science:\n\n“Python Data Science Handbook: Essential Tools for Working with Data,” First Edition, by Jake VanderPlas, O’Reilly Media, 2016.\n\n\n“Python for Data Analysis: Data Wrangling with Pan- das, NumPy, and IPython.” Third Edition, by Wes McK- inney, O’Reilly Media, 2022."
  },
  {
    "objectID": "intro.html#data-challenges",
    "href": "intro.html#data-challenges",
    "title": "1  Introduction",
    "section": "1.4 Data Challenges",
    "text": "1.4 Data Challenges\n\nASA Data Challenge Expo\nKaggle\nDrivenData\nTop 10 Data Science Competitions in 2023"
  },
  {
    "objectID": "intro.html#wishlist",
    "href": "intro.html#wishlist",
    "title": "1  Introduction",
    "section": "1.5 Wishlist",
    "text": "1.5 Wishlist\nThis is a wish list from all members of the class (alphabetical order). Add yours; note the syntax of nested list in Markdown.\n\nAlsubai, Nadia\n\nBecome familiar with both machine learning and deep learning\nBecome proficient in at least one machine or deep learning library for Python\n\nBedard, Kaitlyn\n\nLearn how to use Git proficiently\nGain practical experience using data science methods\nLearn to use python libraries for data science\n\nCheu, Catherine\n\nLearn more about command prompts in Git Bash\nLearn more about data science principles\nLearn better programming techniques in Python\nBecome better in simulation techniques.\n\nHo, Garrick\n\nI want to be more confident with git\nBe able to create a data science project\nLearn more about data science\n\nJones, Courtney\n\nBecome proficient in Git and VS Code\nDeepen my understanding of data science\nBecome comfortable with the Terminal on my computer\n\nKarandikar, Shivaram\n\nUnderstand the workflow and life cycle of a data science project.\nLearn how to code efficiently.\n\nLunetta, Giovanni\n\nLearn to properly clean a dataset\nBecome proficient in building a machine learning model\n\nMastrorilli, Ginamarie\n\nBecome more comfortable with git.\nIncrease my ability to learn new programs.\n\nNguyen, Christine\n\nBecome proficient in Git and apply it.\nBuild my Python programming skills further.\n\nNhan, Nathan\n\nProperly learn how to use Git\nIncrease my understanding of data science and data collection\n\n\nNoel, Luke\n\nBecome proficient in Git/Github\nLearn data science techniques like deep/machine learning, etc.\n\nParchekani, Kian\n\nLearn how to use Git, Quarto\nGet an introduction to data science\nLearn if a career in this field is right for me\nCollaborate with others and gain project experience\n\nShen, Tong\n\nGet familiar with machine learning\nLearn how to deal with big secondary data\nGain some experiences with python\n\nSullivan, Collin\n\nI would like to learn to be able to use data science well enough to get a job in the field\nDiscover if this area of statistics is one that I am passionate about\nGain some project experience that I can cite or reference in interviews\nBe able to speak intelligently about data science and it’s facets\nGain practical experience\n\nWang, Chaoyang\n\nLearn Deep Learning and application on Finance\n\nYan, Jun\n\nMake data science more accessible to undergraduates\nCo-develop a Quarto Book in collaboration with the students\n\nYang Kang, Chua\n\nLearn more about git and github\nApply Data Science skill to my research\nDevelop a new machine learning model\n\nYeung, Shannon\n\nLearn more about Git\nget aa more well rounded understanding of data science\n\nYi, Guanghong\n\nKnow more about Data Science, and what Data Scientists do\nDo one(or more) real life data science project, and gain some practical experience.\n\nZheng, Michael\n\nBecome more comfortable with git\nLearn how to complete a data science project from start to finish\n\n\n\n1.5.1 Presentation Orders\nThe topic presentation order is set up in class.\n\npresenters = [\"Alsubai, Nadia\",\n              \"Bedard, Kaitlyn\",\n              \"Cheu, Catherine\",\n              \"Chua, Yang Kang\",\n              \"Cummins, Patrick\",\n              \"Ho, Garrick\",\n              \"Jones, Courtney\",\n              \"Karandikar, Shivaram\",\n              \"Lunetta, Giovanni\",\n              \"Mastrorilli, Ginamarie\",\n              \"Nguyen, Christine\",\n              \"Nhan, Nathan\",\n              \"Noel, Luke\",\n              \"Parchekani, Kian\",\n              \"Shen, Tong\",\n              \"Sullivan, Colin\",\n              \"Wang, Chaoyang\",\n              \"Whitney, William\",\n              \"Yeung, Shannon\",\n              \"Yi, Guanghong\",\n              \"Zheng, Michael\"]\n\nimport random\nrandom.seed(71323498112697523) # jointly set by the class on 01/30/2023\nrandom.sample(presenters, len(presenters))\n\n['Cheu, Catherine',\n 'Ho, Garrick',\n 'Mastrorilli, Ginamarie',\n 'Yi, Guanghong',\n 'Karandikar, Shivaram',\n 'Chua, Yang Kang',\n 'Jones, Courtney',\n 'Sullivan, Colin',\n 'Shen, Tong',\n 'Alsubai, Nadia',\n 'Yeung, Shannon',\n 'Bedard, Kaitlyn',\n 'Nhan, Nathan',\n 'Parchekani, Kian',\n 'Noel, Luke',\n 'Whitney, William',\n 'Wang, Chaoyang',\n 'Nguyen, Christine',\n 'Cummins, Patrick',\n 'Zheng, Michael',\n 'Lunetta, Giovanni']"
  },
  {
    "objectID": "intro.html#presentation-task-board",
    "href": "intro.html#presentation-task-board",
    "title": "1  Introduction",
    "section": "1.6 Presentation Task Board",
    "text": "1.6 Presentation Task Board\nHere is a list of tasks:\n\nImport/Export data\nDescriptive statistics\nStatistical hypothesis tests scypy.stats\nModel formulas with patsy\nStatistical models with statsmodels\nData visualization with matplotlib\nGrammer of graphics for python plotnine\nHandling spatial data with geopandas\nShow your Data in a Google map with gmplot\nCalling C/C++ from Python\nCalling R from Python and vice versa\nDevelop a Python module\n\nPlease use the following table to sign up. \n\n\n\n\n\n\n\n\nDate\nPresenter\nTopic\n\n\n\n\n02/06\nCheu, Catherine\nIntroduction to matplotlib in Python\n\n\n02/08\nHo, Garrick\nPandas part 1\n\n\n02/13\nMastrorilli, Ginamarie\nPandas part 2\n\n\n02/13\nYi, Guanghong\nGrammer of graphics for python plotnine\n\n\n02/15\nKarandikar, Shivaram\nText processing with NLTK\n\n\n02/20\nChua, Yang Kang\nSupport Vector Machine with scikit-learn\n\n\n02/20\nJones, Courtney\nDescriptive Statistics\n\n\n02/22\nSullivan, Colin\nStatistical hypothesis tests scypy.stats\n\n\n02/27\nShen, Tong\nDecision tree with scikit-learn\n\n\n02/27\nAlsubai, Nadia\nWeb Scraping with Beautiful Soup\n\n\n03/01\nBedard, Kaitlyn\n\n\n\n03/06\nNhan, Nathan\n\n\n\n03/08\nParchekani, Kian\n\n\n\n03/20\nNoel, Luke\nPlotting on maps with gmplot\n\n\n03/20\nWhitney, William\n\n\n\n03/22\nWang, Chaoyang\n\n\n\n03/27\nNguyen, Christine\nCalling R from Python and vice versa\n\n\n03/27\nCummins, Patrick\n\n\n\n03/29\nZheng, Michael\n\n\n\n04/03\nLunetta, Giovanni"
  },
  {
    "objectID": "intro.html#contribute-to-the-class-notes",
    "href": "intro.html#contribute-to-the-class-notes",
    "title": "1  Introduction",
    "section": "1.7 Contribute to the Class Notes",
    "text": "1.7 Contribute to the Class Notes\n\nStart a new branch and switch to the new branch.\nOn the new branch, add a qmd file for your presentation\nEdit _quarto.yml add a line for your qmd file to include it in the notes.\nWork on your qmd file, test with quarto render.\nWhen satisfied, commit and make a pull request.\n\nI have added a template file mysection.qmd and a new line to _quarto.yml as an example."
  },
  {
    "objectID": "git.html#set-up",
    "href": "git.html#set-up",
    "title": "2  Project Management with Git",
    "section": "2.1 Set Up",
    "text": "2.1 Set Up\nTo set up GitHub (other services like Bitbucket or GitLab are similar), you need to\n\nGenerate an SSH key if you don’t have one already.\nSign up an GitHub account.\nAdd the SSH key to your GitHub account"
  },
  {
    "objectID": "git.html#most-frequently-used-git-commands",
    "href": "git.html#most-frequently-used-git-commands",
    "title": "2  Project Management with Git",
    "section": "2.2 Most Frequently Used Git Commands",
    "text": "2.2 Most Frequently Used Git Commands\n\ngit clone\ngit pull\ngit status\ngit add\ngit remove\ngit commit\ngit push"
  },
  {
    "objectID": "git.html#tips-on-using-git",
    "href": "git.html#tips-on-using-git",
    "title": "2  Project Management with Git",
    "section": "2.3 Tips on using Git:",
    "text": "2.3 Tips on using Git:\n\nUse the command line interface instead of the web interface (e.g., upload on GitHub)\nMake frequent small commits instead of rare large commits.\nMake commit messages informative and meaningful.\nName your files/folders by some reasonable convention.\n\nLower cases are better than upper cases.\nNo blanks in file/folder names.\n\nKeep the repo clean by not tracking generated files.\nCreat a .gitignore file for better output from git status.\nKeep the linewidth of sources to under 80 for better git diff view."
  },
  {
    "objectID": "git.html#pull-request",
    "href": "git.html#pull-request",
    "title": "2  Project Management with Git",
    "section": "2.4 Pull Request",
    "text": "2.4 Pull Request\nTo contribute to an open source project (e.g., our classnotes), use pull requests. Pull requests “let you tell others about changes you’ve pushed to a branch in a repository on GitHub. Once a pull request is opened, you can discuss and review the potential changes with collaborators and add follow-up commits before your changes are merged into the base branch.”\nWatch this YouTube video: GitHub pull requests in 100 seconds."
  },
  {
    "objectID": "quarto.html",
    "href": "quarto.html",
    "title": "3  Reproducibile Data Science with Quarto",
    "section": "",
    "text": "Data science projects should be reproducible to be trustworthy. Dynamic documents facilitate reproducibility. Quarto is an open-source dynamic document preparation system, ideal for scientific and technical publishing. From the official websites, Quarto can be used to:\n\nCreate dynamic content with Python, R, Julia, and Observable.\nAuthor documents as plain text markdown or Jupyter notebooks.\nPublish high-quality articles, reports, presentations, websites, blogs, and books in HTML, PDF, MS Word, ePub, and more.\nAuthor with scientific markdown, including equations, citations, crossrefs, figure panels, callouts, advanced layout, and more.\n\nOf course, Quarto can be used to write homework, exams, and reports in this course.\nTo get started, see documentation at Quarto.\nThis is an extra line."
  },
  {
    "objectID": "python.html#the-python-world",
    "href": "python.html#the-python-world",
    "title": "4  Python Refreshment",
    "section": "4.1 The Python World",
    "text": "4.1 The Python World\n\nFunction: a block of organized, reusable code to complete certain task.\nModule: a file containing a collection of functions, variables, and statements.\nPackage: a structured directory containing collections of modules and an __init.py__ file by which the directory is interpreted as a package.\nLibrary: a collection of related functionality of codes. It is a reusable chunk of code that we can use by importing it in our program, we can just use it by importing that library and calling the method of that library with period(.).\n\nSee, for example, how to build a Python libratry."
  },
  {
    "objectID": "python.html#standard-library",
    "href": "python.html#standard-library",
    "title": "4  Python Refreshment",
    "section": "4.2 Standard Library",
    "text": "4.2 Standard Library\nPython’s has an extensive standard library that offers a wide range of facilities as indicated by the long table of contents listed below. See documentation online.\n\nThe library contains built-in modules (written in C) that provide access to system functionality such as file I/O that would otherwise be inaccessible to Python programmers, as well as modules written in Python that provide standardized solutions for many problems that occur in everyday programming. Some of these modules are explicitly designed to encourage and enhance the portability of Python programs by abstracting away platform-specifics into platform-neutral APIs.\n\nQuestion: How to get the constant \\(e\\) to an arbitary precision?\nThe constant is only represented by a given double precision.\n\nimport math\nprint(\"%0.20f\" % math.e)\nprint(\"%0.80f\" % math.e)\n\n2.71828182845904509080\n2.71828182845904509079559829842764884233474731445312500000000000000000000000000000\n\n\nNow use package decimal to export with an arbitary precision.\n\nimport decimal  # for what?\n\n## set the required number digits to 150\ndecimal.getcontext().prec = 150\ndecimal.Decimal(1).exp().to_eng_string()\ndecimal.Decimal(1).exp().to_eng_string()[2:]\n\n'71828182845904523536028747135266249775724709369995957496696762772407663035354759457138217852516642742746639193200305992181741359662904357290033429526'"
  },
  {
    "objectID": "python.html#important-libraries",
    "href": "python.html#important-libraries",
    "title": "4  Python Refreshment",
    "section": "4.3 Important Libraries",
    "text": "4.3 Important Libraries\n\nNumPy\npandas\nmatplotlib\nIPython/Jupyter\nSciPy\nscikit-learn\nstatsmodels\n\nQuestion: how to draw a random sample from a normal distribution and evaluate the density and distributions at these points?\n\nfrom scipy.stats import norm\n\nmu, sigma = 2, 4\nmean, var, skew, kurt = norm.stats(mu, sigma, moments='mvsk')\nprint(mean, var, skew, kurt)\nx = norm.rvs(loc = mu, scale = sigma, size = 10)\nx\n\n2.0 16.0 0.0 0.0\n\n\narray([ 0.07461869, -3.94412908, -1.55035816,  0.11643293,  1.11222155,\n       -1.83876486,  0.79395469, -7.84008326,  3.86468282,  2.73404202])\n\n\nThe pdf and cdf can be evaluated:\n\nnorm.pdf(x, loc = mu, scale = sigma)\n\narray([0.08882567, 0.03306173, 0.0672633 , 0.08926887, 0.09730912,\n       0.06292973, 0.09530362, 0.00483882, 0.08946655, 0.09807028])"
  },
  {
    "objectID": "python.html#writing-a-function",
    "href": "python.html#writing-a-function",
    "title": "4  Python Refreshment",
    "section": "4.4 Writing a Function",
    "text": "4.4 Writing a Function\nConsider the Fibonacci Sequence \\(1, 1, 2, 3, 5, 8, 13, 21, 34, ...\\). The next number is found by adding up the two numbers before it. We are going to use 3 ways to solve the problems.\nThe first is a recursive solution.\n\ndef fib_rs(n):\n    if (n==1 or n==2):\n        return 1\n    else:\n        return fib_rs(n - 1) + fib_rs(n - 2)\n\n%timeit fib_rs(10)\n\n10.7 µs ± 175 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\nThe second uses dynamic programming memoization.\n\ndef fib_dm_helper(n, mem):\n    if mem[n] is not None:\n        return mem[n]\n    elif (n == 1 or n == 2):\n        result = 1\n    else:\n        result = fib_dm_helper(n - 1, mem) + fib_dm_helper(n - 2, mem)\n    mem[n] = result\n    return result\n\ndef fib_dm(n):\n    mem = [None] * (n + 1)\n    return fib_dm_helper(n, mem)\n\n%timeit fib_dm(10)\n\n2.78 µs ± 301 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\nThe third is still dynamic programming but bottom-up.\n\ndef fib_dbu(n):\n    mem = [None] * (n + 1)\n    mem[1]=1;\n    mem[2]=1;\n    for i in range(3,n+1):\n        mem[i] = mem[i-1] + mem[i-2]\n    return mem[n]\n\n\n%timeit fib_dbu(500)\n\n70.1 µs ± 4.79 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\nApparently, the three solutions have very different performance for larger n.\n\n4.4.1 Monte Hall\nHere is a function that performs the Monte Hall experiments.\n\nimport numpy as np\n\ndef montehall(ndoors, ntrials):\n    doors = np.arange(1, ndoors + 1) / 10\n    prize = np.random.choice(doors, size=ntrials)\n    player = np.random.choice(doors, size=ntrials)\n    host = np.array([np.random.choice([d for d in doors\n                                       if d not in [player[x], prize[x]]])\n                     for x in range(ntrials)])\n    player2 = np.array([np.random.choice([d for d in doors\n                                          if d not in [player[x], host[x]]])\n                        for x in range(ntrials)])\n    return {'noswitch': np.sum(prize == player), 'switch': np.sum(prize == player2)}\n\nTest it out:\n\nmontehall(3, 1000)\nmontehall(4, 1000)\n\n{'noswitch': 279, 'switch': 372}\n\n\nThe true value for the two strategies with \\(n\\) doors are, respectively, \\(1 / n\\) and \\(\\frac{n - 1}{n (n - 2)}\\)."
  },
  {
    "objectID": "python.html#variables-versus-objects",
    "href": "python.html#variables-versus-objects",
    "title": "4  Python Refreshment",
    "section": "4.5 Variables versus Objects",
    "text": "4.5 Variables versus Objects\nIn Python, variables and the objects they point to actually live in two different places in the computer memory. Think of variables as pointers to the objects they’re associated with, rather than being those objects. This matters when multiple variables point to the same object.\n\nx = [1, 2, 3]  # create a list; x points to the list\ny = x          # y also points to the same list in the memory\ny.append(4)    # append to y\nx              # x changed!\n\n[1, 2, 3, 4]\n\n\nNow check their addresses\n\nprint(id(x))   # address of x\nprint(id(y))   # address of y\n\n4849331008\n4849331008\n\n\nNonetheless, some data types in Python are “immutable”, meaning that their values cannot be changed in place. One such example is strings.\n\nx = \"abc\"\ny = x\ny = \"xyz\"\nx\n\n'abc'\n\n\nNow check their addresses\n\nprint(id(x))   # address of x\nprint(id(y))   # address of y\n\n4443925640\n4564582896\n\n\nQuestion: What’s mutable and what’s immutable?\nAnything that is a collection of other objects is mutable, except tuples.\nNot all manipulations of mutable objects change the object rather than create a new object. Sometimes when you do something to a mutable object, you get back a new object. Manipulations that change an existing object, rather than create a new one, are referred to as “in-place mutations” or just “mutations.” So:\n\nAll manipulations of immutable types create new objects.\nSome manipulations of mutable types create new objects.\n\nDifferent variables may all be pointing at the same object is preserved through function calls (a behavior known as “pass by object-reference”). So if you pass a list to a function, and that function manipulates that list using an in-place mutation, that change will affect any variable that was pointing to that same object outside the function.\n\nx = [1, 2, 3]\ny = x\n\ndef append_42(input_list):\n    input_list.append(42)\n    return input_list\n\nappend_42(x)\n\n[1, 2, 3, 42]\n\n\nNote that both x and y have been appended by \\(42\\)."
  },
  {
    "objectID": "python.html#number-representation",
    "href": "python.html#number-representation",
    "title": "4  Python Refreshment",
    "section": "4.6 Number Representation",
    "text": "4.6 Number Representation\nNumers in a computer’s memory are represented by binary styles (on and off of bits).\n\n4.6.1 Integers\nIf not careful, It is easy to be bitten by overflow with integers when using Numpy and Pandas in Python.\n\nimport numpy as np\n\nx = np.array(2**63 - 1 , dtype='int')\nx\n# This should be the largest number numpy can display, with\n# the default int8 type (64 bits)\n\narray(9223372036854775807)\n\n\nWhat if we increment it by 1?\n\ny = np.array(x + 1, dtype='int')\ny\n# Because of the overflow, it becomes negative!\n\narray(-9223372036854775808)\n\n\nFor vanilla Python, the overflow errors are checked and more digits are allocated when needed, at the cost of being slow.\n\n2**63 * 1000\n\n9223372036854775808000\n\n\nThis number is 1000 times largger than the prior number, but still displayed perfectly without any overflows\n\n\n4.6.2 Floating Number\nStandard double-precision floating point number uses 64 bits. Among them, 1 is for sign, 11 is for exponent, and 52 are fraction significand, See https://en.wikipedia.org/wiki/Double-precision_floating-point_format. The bottom line is that, of course, not every real number is exactly representable.\n\n0.1 + 0.1 + 0.1 == 0.3\n\nFalse\n\n\n\n0.3 - 0.2 == 0.1\n\nFalse\n\n\nWhat is really going on?\n\nimport decimal\ndecimal.Decimal(0.1)\n\nDecimal('0.1000000000000000055511151231257827021181583404541015625')\n\n\nBecause the mantissa bits are limited, it can not represent a floating point that’s both very big and very precise. Most computers can represent all integers up to \\(2^{53}\\), after that it starts skipping numbers.\n\n2.1**53 + 1 == 2.1**53\n\n# Find a number larger than 2 to the 53rd\n\nTrue\n\n\n\nx = 2.1**53\nfor i in range(1000000):\n    x = x + 1\nx == 2.1**53\n\nTrue\n\n\nWe add 1 to x by 1000000 times, but it still equal to its initial value, 2.1**53. This is because this number is too big that computer can’t handle it with precision like add 1.\nMachine epsilon is the smallest positive floating-point number x such that 1 + x != 1.\n\nprint(np.finfo(float).eps)\nprint(np.finfo(np.float32).eps)\n\n2.220446049250313e-16\n1.1920929e-07"
  },
  {
    "objectID": "stats.html#tests-for-exploratory-data-analysis",
    "href": "stats.html#tests-for-exploratory-data-analysis",
    "title": "5  Statistical Tests and Models",
    "section": "5.1 Tests for Exploratory Data Analysis",
    "text": "5.1 Tests for Exploratory Data Analysis\nA collection of functions are available from scipy.stats.\n\nComparing the locations of two samples\n\nttest_ind: t-test for two independent samples\nttest_rel: t-test for paired samples\nranksums: Wilcoxon rank-sum test for two independent samples\nwilcoxon: Wilcoxon signed-rank test for paired samples\n\nComparing the locations of multiple samples\n\nf_oneway: one-way ANOVA\nkruskal: Kruskal-Wallis H-test\n\nTests for associations in contigency tables\n\nchi2_contingency: Chi-square test of independence of variables\nfisher_exact: Fisher exact test on a 2x2 contingency table\n\nGoodness of fit\n\ngoodness_of_fit: distribution could contain unspecified parameters\nanderson: Anderson-Darling test\nkstest: Kolmogorov-Smirnov test\nchisquare: one-way chi-square test\nnormaltest: test for normality\n\n\nSince R has a richer collections of statistical functions, we can call R function from Python with rpy2. See, for example, a blog on this subject."
  },
  {
    "objectID": "stats.html#linear-model",
    "href": "stats.html#linear-model",
    "title": "5  Statistical Tests and Models",
    "section": "5.2 Linear Model",
    "text": "5.2 Linear Model\nLet’s simulate some data for illustrations.\n\nimport numpy as np\nimport statsmodels.api as sm\n\nnobs = 200\nncov = 5\nnp.random.seed(123)\nx = np.random.random((nobs, ncov)) # Uniform over [0, 1)\nbeta = np.repeat(1, ncov)\ny = 2 + np.dot(x, beta) + np.random.normal(size = nobs)\n\nCheck the shape of y:\n\ny.shape\n\n(200,)\n\n\nCheck the shape of x:\n\nx.shape\n\n(200, 5)\n\n\nThat is, the true linear regression model is \\[\ny = 2 + x_1 + x_2 + x_3 + x_4 + x_5 + \\epsilon.\n\\]\nA regression model for the observed data can be fitted as\n\nxmat = sm.add_constant(x)\nmymod = sm.OLS(y, xmat)\nmyfit = mymod.fit()\nmyfit.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:            y          R-squared:             0.309\n\n\n  Model:                   OLS         Adj. R-squared:        0.292\n\n\n  Method:             Least Squares    F-statistic:           17.38\n\n\n  Date:             Mon, 13 Feb 2023   Prob (F-statistic): 3.31e-14\n\n\n  Time:                 14:10:48       Log-Likelihood:      -272.91\n\n\n  No. Observations:         200        AIC:                   557.8\n\n\n  Df Residuals:             194        BIC:                   577.6\n\n\n  Df Model:                   5                                    \n\n\n  Covariance Type:      nonrobust                                  \n\n\n\n\n           coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  const     1.8754     0.282     6.656  0.000     1.320     2.431\n\n\n  x1        1.1703     0.248     4.723  0.000     0.682     1.659\n\n\n  x2        0.8988     0.235     3.825  0.000     0.435     1.362\n\n\n  x3        0.9784     0.238     4.114  0.000     0.509     1.448\n\n\n  x4        1.3418     0.250     5.367  0.000     0.849     1.835\n\n\n  x5        0.6027     0.239     2.519  0.013     0.131     1.075\n\n\n\n\n  Omnibus:        0.810   Durbin-Watson:         1.978\n\n\n  Prob(Omnibus):  0.667   Jarque-Bera (JB):      0.903\n\n\n  Skew:          -0.144   Prob(JB):              0.637\n\n\n  Kurtosis:       2.839   Cond. No.               8.31\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nQuestions to review:\n\nHow are the regression coefficients interpreted? Intercept?\nWhy does it make sense to center the covariates?\n\nNow we form a data frame with the variables\n\nimport pandas as pd\ndf = np.concatenate((y.reshape((nobs, 1)), x), axis = 1)\ndf = pd.DataFrame(data = df,\n                  columns = [\"y\"] + [\"x\" + str(i) for i in range(1,\n                  ncov + 1)])\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 200 entries, 0 to 199\nData columns (total 6 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   y       200 non-null    float64\n 1   x1      200 non-null    float64\n 2   x2      200 non-null    float64\n 3   x3      200 non-null    float64\n 4   x4      200 non-null    float64\n 5   x5      200 non-null    float64\ndtypes: float64(6)\nmemory usage: 9.5 KB\n\n\nLet’s use a formula to specify the regression model as in R, and fit a robust linear model (rlm) instead of OLS. Note that the model specification and the function interface is similar to R.\n\nimport statsmodels.formula.api as smf\nmymod = smf.rlm(formula = \"y ~ x1 + x2 + x3 + x4 + x5\", data = df)\nmyfit = mymod.fit()\nmyfit.summary()\n\n\n\nRobust linear Model Regression Results\n\n  Dep. Variable:          y          No. Observations:      200\n\n\n  Model:                 RLM         Df Residuals:          194\n\n\n  Method:               IRLS         Df Model:                5\n\n\n  Norm:                HuberT                                  \n\n\n  Scale Est.:            mad                                   \n\n\n  Cov Type:              H1                                    \n\n\n  Date:           Mon, 13 Feb 2023                             \n\n\n  Time:               14:10:48                                 \n\n\n  No. Iterations:        16                                    \n\n\n\n\n               coef     std err      z      P>|z|  [0.025    0.975]  \n\n\n  Intercept     1.8353     0.294     6.246  0.000     1.259     2.411\n\n\n  x1            1.1254     0.258     4.355  0.000     0.619     1.632\n\n\n  x2            0.9664     0.245     3.944  0.000     0.486     1.447\n\n\n  x3            0.9995     0.248     4.029  0.000     0.513     1.486\n\n\n  x4            1.3275     0.261     5.091  0.000     0.816     1.839\n\n\n  x5            0.6768     0.250     2.712  0.007     0.188     1.166\n\nIf the model instance has been used for another fit with different fit parameters, then the fit options might not be the correct ones anymore .\n\n\nFor model diagnostics, one can check residual plots.\n\nimport matplotlib.pyplot as plt\n\nmyOlsFit = smf.ols(formula = \"y ~ x1 + x2 + x3 + x4 + x5\", data = df).fit()\nfig = plt.figure(figsize = (6, 6))\n## residual versus x1; can do the same for other covariates\nfig = sm.graphics.plot_regress_exog(myOlsFit, 'x1', fig=fig)\n\neval_env: 1\n\n\n\n\n\nSee more on residual diagnostics and specification tests."
  },
  {
    "objectID": "stats.html#generalized-linear-regression",
    "href": "stats.html#generalized-linear-regression",
    "title": "5  Statistical Tests and Models",
    "section": "5.3 Generalized Linear Regression",
    "text": "5.3 Generalized Linear Regression\nA linear regression model cannot be applied to presence/absence or count data.\nBinary or count data need to be modeled under a generlized framework. Consider a binary or count variable \\(Y\\) with possible covariates \\(X\\). A generalized model describes a transformation \\(g\\) of the conditional mean \\(E[Y | X]\\) by a linear predictor \\(X^{\\top}\\beta\\). That is \\[\ng( E[Y | X] ) = X^{\\top} \\beta.\n\\] The transformation \\(g\\) is known as the link function.\nFor logistic regression with binary outcomes, the link function is the logit function \\[\ng(u) = \\log \\frac{u}{1 - u}, \\quad u \\in (0, 1).\n\\]\nWhat is the interpretation of the regression coefficients in a logistic regression? Intercept?\nA logistic regression can be fit with statsmodels.api.glm.\nLet’s generate some binary data first by dichotomizing existing variables.\n\ndf['yb' ] = np.where(df['y' ] > 2.5, 1, 0)\ndf['x1b'] = np.where(df['x1'] > 0.5, 1, 0)\n\nFit a logistic regression for y1b.\n\nmylogistic = smf.glm(formula = 'yb ~ x1b + x2 + x3 + x4 + x5', data = df,\n                     family = sm.families.Binomial())\nmylfit = mylogistic.fit()\nmylfit.summary()\n\n\n\nGeneralized Linear Model Regression Results\n\n  Dep. Variable:          yb          No. Observations:       200 \n\n\n  Model:                  GLM         Df Residuals:           194 \n\n\n  Model Family:        Binomial       Df Model:                 5 \n\n\n  Link Function:         Logit        Scale:                1.0000\n\n\n  Method:                IRLS         Log-Likelihood:      -30.654\n\n\n  Date:            Mon, 13 Feb 2023   Deviance:             61.307\n\n\n  Time:                14:10:49       Pearson chi2:          217. \n\n\n  No. Iterations:          7          Pseudo R-squ. (CS):  0.05871\n\n\n  Covariance Type:     nonrobust                                  \n\n\n\n\n               coef     std err      z      P>|z|  [0.025    0.975]  \n\n\n  Intercept    -0.4354     1.309    -0.333  0.739    -3.002     2.131\n\n\n  x1b           1.2721     0.845     1.505  0.132    -0.384     2.928\n\n\n  x2            2.6897     1.418     1.897  0.058    -0.089     5.469\n\n\n  x3           -0.0537     1.270    -0.042  0.966    -2.542     2.435\n\n\n  x4            2.6576     1.438     1.848  0.065    -0.160     5.476\n\n\n  x5            1.8752     1.330     1.409  0.159    -0.732     4.483\n\n\n\n\nIf we treat y1b as count data, a Poisson regression can be fitted.\n\nmyPois = smf.glm(formula = 'yb ~ x1b + x2 + x3 + x4 + x5', data = df,\n                 family = sm.families.Poisson())\nmypfit = myPois.fit()\nmypfit.summary()\n\n\n\nGeneralized Linear Model Regression Results\n\n  Dep. Variable:          yb          No. Observations:       200 \n\n\n  Model:                  GLM         Df Residuals:           194 \n\n\n  Model Family:         Poisson       Df Model:                 5 \n\n\n  Link Function:          Log         Scale:                1.0000\n\n\n  Method:                IRLS         Log-Likelihood:      -199.55\n\n\n  Date:            Mon, 13 Feb 2023   Deviance:             17.091\n\n\n  Time:                14:10:49       Pearson chi2:          8.99 \n\n\n  No. Iterations:          4          Pseudo R-squ. (CS): 0.002487\n\n\n  Covariance Type:     nonrobust                                  \n\n\n\n\n               coef     std err      z      P>|z|  [0.025    0.975]  \n\n\n  Intercept    -0.2045     0.288    -0.710  0.477    -0.769     0.360\n\n\n  x1b           0.0445     0.146     0.304  0.761    -0.242     0.331\n\n\n  x2            0.0965     0.249     0.387  0.699    -0.392     0.585\n\n\n  x3           -0.0088     0.254    -0.035  0.972    -0.507     0.490\n\n\n  x4            0.1072     0.267     0.402  0.688    -0.416     0.630\n\n\n  x5            0.0750     0.253     0.296  0.767    -0.421     0.571"
  },
  {
    "objectID": "visual.html",
    "href": "visual.html",
    "title": "6  Visualization",
    "section": "",
    "text": "7 Introduction to matplotlib"
  },
  {
    "objectID": "visual.html#introduction",
    "href": "visual.html#introduction",
    "title": "6  Visualization",
    "section": "7.1 Introduction",
    "text": "7.1 Introduction\nThe matplotlib library can provide methods in plotting and arranging data visually in order to help viewers understand the main concepts of the data analysis. In this chapter, a progression of graphs will be shown to demonstrate some of the capabilities the library has to graph and plot data.\nThere are several types of graphs that can be used, such as:\n1. Scatterplot\n2. Line plot\n3. 3D plot"
  },
  {
    "objectID": "visual.html#how-to-use",
    "href": "visual.html#how-to-use",
    "title": "6  Visualization",
    "section": "7.2 How to Use",
    "text": "7.2 How to Use\nThe library is not part of the defaults used in Python. To install, use either pip or Anaconda. The following command should be used:\n\n# pip install matplotlib\n\nIf the above does not work, then you would need the path of pip or Python to put before the command.\n\n7.2.1 Examples\nLet’s start with a simple scatter plot. We would need to import the libraries as shown. For this example, we use the pyplot submodule, abbreviated to plt. We will use randomly generated data in 3 dimensions (x,y,z).\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(8465);\n\nx = np.random.uniform(0, 3, 10);\ny = np.random.uniform(0, 3, 10);\nz = np.random.uniform(0, 3, 10);\n\nplt.scatter(x, y)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.show()\n\n\n\n\nWe could start plotting another plot, but we have not saved our scatterplot as an object. Thus, it will get overridden by whatever we plot next. If we want to keep a plot, we can save as a figure object. In addition, if we need multiple plots together, we can use a subplot shown as follows.\n\nfigure, (fig1, fig2) = plt.subplots(1, 2, figsize = (8, 6))\n\nfig1.scatter(y, z, marker = '^')\nfig2.scatter(x, y, color = 'red')\n\nplt.show()\n\n\n\n\nWe can also do 3d plots alongside 2d, but we need a different function in order to do so. The following uses 3d axes to plot the scatterplot.\n\nfigure = plt.figure()\n\n# Make 3D axes for fig1\n\nfig2 = figure.add_subplot(122, projection = '3d')\nfig1 = figure.add_subplot(121)\n\n# Plot\n\nfig1.plot(x, z, label = \"Line Graph\")\nfig2.scatter(x, y, z, c = z, cmap = 'cool', label = \"Scatter in 3D\")\nfig1.legend()\nfig2.legend()\n\nplt.show()"
  },
  {
    "objectID": "visual.html#animation",
    "href": "visual.html#animation",
    "title": "6  Visualization",
    "section": "7.3 Animation",
    "text": "7.3 Animation\nAnimations can also be done through matplotlib. This requires the use of the animation submodule which has a variety functions that can be used to plot animations. Inputs required include the frames and other functions needed to update the plots per frame.\n\nimport matplotlib.animation as animation\n\ndef updatept(self):\n    z = 10;\n\nWe can use the FuncAnimation(args,updatept(),frames) to update."
  },
  {
    "objectID": "visual.html#conclusion",
    "href": "visual.html#conclusion",
    "title": "6  Visualization",
    "section": "7.4 Conclusion",
    "text": "7.4 Conclusion\nWe have demonstrated some capabilities of the matplotlib library but more complex methods of plotting and arranging visual elements can be found in the documentation."
  },
  {
    "objectID": "nycrash.html#first-glance",
    "href": "nycrash.html#first-glance",
    "title": "7  NYC Crash Data",
    "section": "7.1 First Glance",
    "text": "7.1 First Glance\nConsider the NYC Crash Data in January 2022.\n\nimport pandas as pd\n\njan22 = pd.read_csv(\"data/nyc_mv_collisions_202201.csv\")\njan22.head()\njan22.describe()\n\n\n\n\n\n  \n    \n      \n      ZIP CODE\n      LATITUDE\n      LONGITUDE\n      NUMBER OF PERSONS INJURED\n      NUMBER OF PERSONS KILLED\n      NUMBER OF PEDESTRIANS INJURED\n      NUMBER OF PEDESTRIANS KILLED\n      NUMBER OF CYCLIST INJURED\n      NUMBER OF CYCLIST KILLED\n      NUMBER OF MOTORIST INJURED\n      NUMBER OF MOTORIST KILLED\n      COLLISION_ID\n    \n  \n  \n    \n      count\n      5025.000000\n      7097.000000\n      7097.000000\n      7659.000000\n      7659.000000\n      7659.000000\n      7659.000000\n      7659.000000\n      7659.0\n      7659.000000\n      7659.000000\n      7.659000e+03\n    \n    \n      mean\n      10908.957015\n      40.609463\n      -73.705503\n      0.404753\n      0.002350\n      0.084345\n      0.001306\n      0.021935\n      0.0\n      0.287505\n      0.000914\n      4.495510e+06\n    \n    \n      std\n      514.740028\n      2.160598\n      3.919451\n      0.726622\n      0.048425\n      0.290332\n      0.036113\n      0.149131\n      0.0\n      0.692545\n      0.030220\n      2.338111e+03\n    \n    \n      min\n      10000.000000\n      0.000000\n      -74.249980\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.0\n      0.000000\n      0.000000\n      4.491064e+06\n    \n    \n      25%\n      10459.000000\n      40.665085\n      -73.960370\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.0\n      0.000000\n      0.000000\n      4.493532e+06\n    \n    \n      50%\n      11209.000000\n      40.712505\n      -73.917310\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.0\n      0.000000\n      0.000000\n      4.495533e+06\n    \n    \n      75%\n      11354.000000\n      40.786957\n      -73.862274\n      1.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.0\n      0.000000\n      0.000000\n      4.497480e+06\n    \n    \n      max\n      11697.000000\n      40.909206\n      0.000000\n      8.000000\n      1.000000\n      3.000000\n      1.000000\n      2.000000\n      0.0\n      8.000000\n      1.000000\n      4.500594e+06\n    \n  \n\n\n\n\nFrequency tables for categorical variables.\n\njan22[\"BOROUGH\"].value_counts(dropna=False)\n\nNaN              2634\nBROOKLYN         1744\nQUEENS           1393\nBRONX             948\nMANHATTAN         737\nSTATEN ISLAND     203\nName: BOROUGH, dtype: int64\n\n\nSome tables are too long.\n\n# jan22[\"CONTRIBUTING FACTOR VEHICLE 1\"].value_counts(dropna=False)\nwith pd.option_context('display.max_rows', None):\n    print(jan22[\"VEHICLE TYPE CODE 1\"].value_counts(dropna=False))\n\nSedan                                  3685\nStation Wagon/Sport Utility Vehicle    2766\nTaxi                                    205\nBus                                     151\nPick-up Truck                           148\nBox Truck                               120\nNaN                                     120\nBike                                     54\nTractor Truck Diesel                     49\nVan                                      49\nAmbulance                                39\nE-Bike                                   30\nPK                                       25\nGarbage or Refuse                        24\nDump                                     20\nMotorcycle                               20\nE-Scooter                                18\nConvertible                              14\nSnow Plow                                11\nFlat Bed                                 11\nCarry All                                 9\nTractor Truck Gasoline                    8\nTanker                                    8\nAMBULANCE                                 5\nMoped                                     4\nChassis Cab                               4\nConcrete Mixer                            3\nArmored Truck                             3\nFIRE TRUCK                                3\nTow Truck / Wrecker                       3\n4 dr sedan                                2\nFlat Rack                                 2\nStake or Rack                             2\nLIMO                                      2\nFIRETRUCK                                 2\n3-Door                                    2\nTruck                                     1\nFORD                                      1\nBulk Agriculture                          1\nunk                                       1\ntr                                        1\nFire truck                                1\nM2                                        1\nTow Truck                                 1\nFDNY TRUCK                                1\nPas (4dr s                                1\nAmbu                                      1\nmoped                                     1\nDelivery                                  1\nBackhoe                                   1\nPICK UP TR                                1\nD2                                        1\nPassenger                                 1\nBeverage Truck                            1\nMotorized Home                            1\nDELV                                      1\nCarriage                                  1\nSELF                                      1\nMOTOR SCOO                                1\nTRAILER                                   1\nPedicab                                   1\nPICK-UP TR                                1\nRefrigerated Van                          1\nMotorbike                                 1\nMotorscooter                              1\nFiretruck                                 1\nSchool Bus                                1\nPower shov                                1\npassenger                                 1\nUTILITY                                   1\nMinibike                                  1\nLunch Wagon                               1\nForklift                                  1\nUSPS small                                1\nName: VEHICLE TYPE CODE 1, dtype: int64\n\n\nCross-tables\n\npd.crosstab(index = jan22[\"CONTRIBUTING FACTOR VEHICLE 1\"],\n            columns = jan22[\"BOROUGH\"], dropna = False)\n\n\n\n\n\n  \n    \n      BOROUGH\n      BRONX\n      BROOKLYN\n      MANHATTAN\n      QUEENS\n      STATEN ISLAND\n    \n    \n      CONTRIBUTING FACTOR VEHICLE 1\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Accelerator Defective\n      0\n      2\n      0\n      3\n      0\n    \n    \n      Aggressive Driving/Road Rage\n      3\n      9\n      6\n      5\n      1\n    \n    \n      Alcohol Involvement\n      24\n      27\n      15\n      37\n      2\n    \n    \n      Animals Action\n      0\n      0\n      0\n      1\n      0\n    \n    \n      Backing Unsafely\n      36\n      61\n      33\n      56\n      4\n    \n    \n      Brakes Defective\n      2\n      8\n      1\n      5\n      1\n    \n    \n      Cell Phone (hand-Held)\n      0\n      0\n      1\n      1\n      0\n    \n    \n      Driver Inattention/Distraction\n      186\n      392\n      187\n      330\n      41\n    \n    \n      Driver Inexperience\n      16\n      20\n      19\n      18\n      5\n    \n    \n      Driverless/Runaway Vehicle\n      4\n      2\n      2\n      1\n      1\n    \n    \n      Drugs (illegal)\n      1\n      2\n      2\n      0\n      0\n    \n    \n      Failure to Keep Right\n      0\n      0\n      0\n      3\n      0\n    \n    \n      Failure to Yield Right-of-Way\n      51\n      126\n      39\n      155\n      22\n    \n    \n      Fatigued/Drowsy\n      0\n      1\n      0\n      2\n      0\n    \n    \n      Fell Asleep\n      5\n      10\n      4\n      6\n      1\n    \n    \n      Following Too Closely\n      27\n      66\n      33\n      86\n      5\n    \n    \n      Glare\n      1\n      4\n      2\n      2\n      1\n    \n    \n      Illnes\n      1\n      2\n      2\n      3\n      1\n    \n    \n      Lost Consciousness\n      2\n      1\n      2\n      1\n      2\n    \n    \n      Obstruction/Debris\n      1\n      1\n      4\n      0\n      2\n    \n    \n      Other Vehicular\n      23\n      36\n      20\n      12\n      3\n    \n    \n      Outside Car Distraction\n      3\n      1\n      2\n      4\n      0\n    \n    \n      Oversized Vehicle\n      7\n      2\n      4\n      2\n      1\n    \n    \n      Passenger Distraction\n      1\n      2\n      2\n      2\n      0\n    \n    \n      Passing Too Closely\n      32\n      68\n      40\n      57\n      3\n    \n    \n      Passing or Lane Usage Improper\n      28\n      64\n      27\n      64\n      3\n    \n    \n      Pavement Defective\n      4\n      0\n      1\n      2\n      1\n    \n    \n      Pavement Slippery\n      44\n      36\n      10\n      44\n      20\n    \n    \n      Pedestrian/Bicyclist/Other Pedestrian Error/Confusion\n      7\n      12\n      14\n      2\n      0\n    \n    \n      Physical Disability\n      1\n      2\n      0\n      1\n      1\n    \n    \n      Prescription Medication\n      1\n      1\n      0\n      0\n      0\n    \n    \n      Reaction to Uninvolved Vehicle\n      8\n      12\n      12\n      10\n      2\n    \n    \n      Steering Failure\n      1\n      2\n      1\n      3\n      0\n    \n    \n      Tinted Windows\n      0\n      0\n      0\n      1\n      0\n    \n    \n      Tire Failure/Inadequate\n      1\n      1\n      0\n      1\n      0\n    \n    \n      Traffic Control Device Improper/Non-Working\n      0\n      2\n      0\n      0\n      0\n    \n    \n      Traffic Control Disregarded\n      20\n      37\n      26\n      50\n      5\n    \n    \n      Turning Improperly\n      32\n      38\n      12\n      34\n      12\n    \n    \n      Unsafe Lane Changing\n      16\n      23\n      17\n      11\n      1\n    \n    \n      Unsafe Speed\n      38\n      44\n      14\n      51\n      7\n    \n    \n      Unspecified\n      303\n      597\n      175\n      308\n      53\n    \n    \n      View Obstructed/Limited\n      11\n      14\n      3\n      11\n      1"
  },
  {
    "objectID": "nycrash.html#some-cleaning",
    "href": "nycrash.html#some-cleaning",
    "title": "7  NYC Crash Data",
    "section": "7.2 Some Cleaning",
    "text": "7.2 Some Cleaning\nQuestions from Dr. Douglas Bates:\n\nThe CRASH_DATEs are all in the correct month and there are no missing values\nThere are no missing values in the CRASH_TIMEs but there are 117 values of exactly 00:00:00. Is this a matter of bad luck when the clock strikes midnight?\nOver 1/3 of the ZIP_CODE and BOROUGH values are missing. There are the same number of missing values in these columns - do they always co-occur? If LATITUDE and LONGITUDE are available, can they be used to infer the ZIP_CODE?\nThere are 178 unique non-missing ZIP_CODE values as stated in the Jamboree description. (“Trust, but verify.”) Is there really a zip code of 10000 in New York?\nThere are 20 values of 0.0 for LATITUDE and LONGITUDE? These are obviously incorrect - should they be coded as missing?\nIs it redundant to keep the LOCATIO in addition to LATITUDE and LONGITUDE?\nThe COLLISION_ID is unique to each row and can be used as a key. The values are not consecutive - why not?\nThe NUMBER_OF_... columns seem reasonable. A further consistency check is suggested in the Jamboree tasks.\nIn the CONTRIBUTING_FACTOR_… columns, is Unspecified different from missing?\nThe codes in the VEHICLE_TYPE_CODE_... columns are the usual hodge-podge of results from “free-form” data entry. Should unk, UNK, UNKNOWN, and Unknown be converted to missing?\nIn contrast, the codes in the CONTRIBUTING_FACTOR_... columns appear to be standardized (not sure why Illnes isn’t Illness).\n\n\nwith pd.option_context('display.max_rows', None):\n    print(jan22[\"CRASH TIME\"].value_counts())\n\n0:00     117\n15:00    100\n14:00     86\n17:00     78\n12:00     75\n8:00      75\n16:00     74\n10:00     73\n9:00      72\n13:00     68\n19:00     62\n18:00     62\n8:30      59\n11:30     58\n17:30     56\n14:30     53\n20:00     53\n11:00     51\n10:30     46\n21:00     45\n18:30     45\n7:00      45\n16:30     44\n19:30     44\n22:00     43\n9:30      42\n15:30     42\n13:30     41\n6:00      41\n23:00     41\n6:30      38\n20:30     34\n12:30     33\n8:40      32\n14:50     32\n7:30      31\n8:15      29\n4:00      28\n14:40     27\n14:45     25\n15:40     25\n8:20      25\n16:40     24\n18:15     24\n8:45      24\n5:00      24\n11:50     24\n16:15     24\n9:50      24\n22:30     23\n11:15     23\n5:30      23\n17:20     23\n16:50     22\n23:30     22\n4:30      22\n10:15     22\n17:50     22\n17:40     22\n17:45     21\n19:15     21\n19:50     20\n22:20     20\n13:50     20\n18:10     20\n15:15     20\n7:50      20\n17:15     20\n8:50      20\n16:35     20\n21:50     20\n11:45     20\n19:45     20\n18:45     19\n15:45     19\n18:50     19\n15:20     19\n0:30      19\n2:00      19\n16:45     18\n18:25     18\n8:10      18\n9:10      18\n7:15      18\n3:00      18\n18:40     18\n22:50     18\n0:15      18\n9:45      18\n18:20     17\n23:50     17\n15:50     17\n10:20     17\n7:45      17\n15:35     17\n12:50     17\n9:15      17\n23:15     16\n21:45     16\n20:40     16\n12:40     16\n13:40     16\n6:45      16\n15:55     15\n16:10     15\n21:30     15\n16:20     15\n14:20     15\n1:30      15\n17:10     15\n13:45     15\n7:20      15\n20:45     15\n22:45     15\n9:20      15\n15:10     15\n19:40     14\n20:15     14\n14:15     14\n12:45     14\n11:25     14\n12:20     14\n19:20     14\n4:15      14\n8:25      14\n5:50      14\n17:55     14\n11:40     14\n21:15     14\n20:35     14\n0:40      14\n11:20     14\n7:10      14\n6:50      14\n23:20     13\n17:05     13\n13:15     13\n12:15     13\n14:35     13\n16:05     13\n10:40     13\n22:15     13\n10:45     13\n17:25     13\n14:10     13\n23:40     13\n5:40      13\n7:05      13\n0:10      13\n21:20     13\n8:55      13\n17:35     12\n11:35     12\n15:25     12\n21:35     12\n9:40      12\n6:20      12\n4:20      12\n0:20      12\n13:20     12\n11:10     12\n18:35     12\n8:35      11\n6:10      11\n18:05     11\n20:50     11\n2:50      11\n2:30      11\n1:00      11\n16:24     11\n16:25     11\n7:40      11\n20:20     11\n6:55      11\n7:55      11\n15:05     10\n22:40     10\n6:25      10\n9:25      10\n0:45      10\n1:50      10\n19:55     10\n0:05      10\n12:55     10\n6:15      10\n10:50     10\n14:05     10\n5:20      10\n19:10     10\n4:55       9\n21:40      9\n2:05       9\n21:10      9\n18:58      9\n12:10      9\n10:10      9\n18:55      9\n5:55       9\n4:45       9\n8:05       9\n1:15       9\n23:55      9\n15:44      9\n22:05      9\n22:25      9\n13:35      9\n10:25      8\n13:05      8\n22:35      8\n4:35       8\n15:48      8\n14:25      8\n10:35      8\n5:45       8\n12:35      8\n2:25       8\n4:25       8\n15:58      8\n3:15       8\n1:45       8\n14:49      8\n13:25      8\n0:25       8\n12:25      8\n19:25      8\n12:05      8\n0:50       8\n2:55       8\n17:33      8\n1:35       8\n2:45       8\n14:55      7\n0:01       7\n1:25       7\n3:30       7\n0:34       7\n7:25       7\n17:58      7\n6:40       7\n12:14      7\n22:55      7\n10:05      7\n18:06      7\n14:14      7\n23:05      7\n1:20       7\n16:09      7\n4:10       7\n14:12      7\n3:20       7\n6:05       7\n9:05       7\n23:10      7\n18:38      7\n15:21      7\n12:27      7\n23:45      7\n13:10      7\n7:35       7\n10:33      7\n4:50       7\n16:55      7\n13:14      7\n19:35      6\n21:05      6\n14:44      6\n11:01      6\n15:33      6\n12:02      6\n3:45       6\n20:19      6\n1:55       6\n2:40       6\n12:48      6\n9:55       6\n20:25      6\n5:05       6\n8:51       6\n18:19      6\n16:48      6\n11:55      6\n19:46      6\n20:10      6\n8:04       6\n17:32      6\n13:55      6\n20:18      6\n23:39      6\n17:21      6\n18:07      6\n7:27       6\n5:25       6\n3:40       6\n23:41      6\n6:03       6\n17:12      6\n1:10       6\n8:26       6\n2:20       6\n5:10       6\n20:43      6\n8:58       6\n17:44      5\n15:43      5\n3:35       5\n15:56      5\n11:06      5\n18:27      5\n22:10      5\n23:25      5\n14:37      5\n12:44      5\n13:31      5\n18:14      5\n16:42      5\n15:11      5\n18:48      5\n16:27      5\n20:08      5\n16:43      5\n6:35       5\n16:28      5\n21:49      5\n17:22      5\n18:54      5\n14:07      5\n22:38      5\n17:13      5\n17:14      5\n16:04      5\n15:53      5\n5:46       5\n8:18       5\n23:06      5\n15:24      5\n17:02      5\n15:19      5\n20:55      5\n4:05       5\n10:55      5\n17:19      5\n21:03      5\n16:46      5\n9:28       5\n15:03      5\n17:18      5\n23:11      5\n20:22      5\n10:57      5\n22:01      5\n15:51      5\n17:16      5\n8:07       5\n8:11       5\n18:22      5\n12:18      5\n16:23      5\n21:58      5\n3:50       5\n12:57      5\n21:46      5\n21:25      5\n6:24       5\n9:23       5\n14:57      5\n15:42      5\n15:38      5\n9:35       5\n15:13      5\n9:34       5\n17:53      5\n16:12      5\n5:33       5\n13:13      5\n19:39      5\n6:58       5\n20:36      5\n20:09      5\n2:07       5\n17:27      5\n2:18       5\n11:26      5\n7:03       5\n0:07       5\n16:14      5\n18:03      5\n14:11      5\n15:27      5\n19:05      4\n12:28      4\n6:18       4\n8:22       4\n14:58      4\n14:42      4\n19:21      4\n23:32      4\n13:32      4\n15:54      4\n14:02      4\n3:25       4\n11:05      4\n8:03       4\n20:26      4\n9:56       4\n4:27       4\n17:48      4\n22:47      4\n3:55       4\n7:49       4\n18:34      4\n1:16       4\n11:11      4\n7:46       4\n22:14      4\n7:11       4\n19:53      4\n8:17       4\n22:34      4\n14:43      4\n16:41      4\n23:01      4\n12:37      4\n21:55      4\n17:06      4\n21:33      4\n13:28      4\n18:01      4\n7:54       4\n14:53      4\n6:37       4\n10:28      4\n18:51      4\n16:37      4\n19:52      4\n21:13      4\n17:37      4\n13:08      4\n21:48      4\n16:22      4\n19:24      4\n2:35       4\n19:44      4\n17:47      4\n13:07      4\n18:04      4\n8:41       4\n18:17      4\n9:08       4\n11:43      4\n18:24      4\n18:28      4\n23:35      4\n13:39      4\n13:01      4\n8:38       4\n20:06      4\n13:53      4\n20:44      4\n3:24       4\n12:03      4\n15:31      4\n2:12       4\n19:04      4\n0:35       4\n20:24      4\n19:48      4\n2:10       4\n8:31       4\n20:05      4\n23:47      4\n15:32      4\n19:18      4\n19:19      4\n8:21       4\n16:02      4\n16:33      4\n19:51      4\n14:16      4\n14:03      4\n9:57       4\n23:49      4\n19:29      4\n10:02      4\n21:06      4\n13:17      4\n10:59      4\n18:08      4\n16:07      4\n16:47      4\n11:16      4\n5:18       4\n22:43      4\n21:17      4\n0:47       4\n17:57      4\n16:29      4\n20:37      4\n9:39       4\n15:14      4\n1:40       4\n10:01      4\n15:08      4\n8:08       4\n14:06      4\n20:54      4\n8:02       4\n0:54       4\n21:16      4\n17:36      4\n6:56       4\n11:07      3\n7:01       3\n11:47      3\n23:37      3\n14:56      3\n23:42      3\n3:54       3\n13:52      3\n10:32      3\n17:11      3\n23:34      3\n10:47      3\n4:40       3\n20:31      3\n16:26      3\n20:39      3\n3:05       3\n18:41      3\n18:43      3\n9:24       3\n9:26       3\n16:11      3\n8:37       3\n17:23      3\n12:41      3\n8:28       3\n14:08      3\n4:34       3\n14:21      3\n17:24      3\n12:11      3\n15:07      3\n19:49      3\n9:12       3\n7:14       3\n12:53      3\n12:47      3\n12:09      3\n1:02       3\n10:38      3\n19:06      3\n4:04       3\n17:29      3\n12:19      3\n3:52       3\n8:57       3\n17:38      3\n10:09      3\n17:08      3\n22:29      3\n10:12      3\n23:43      3\n14:17      3\n2:04       3\n13:36      3\n0:22       3\n10:53      3\n18:23      3\n6:38       3\n20:33      3\n0:52       3\n9:51       3\n20:42      3\n0:17       3\n7:41       3\n2:44       3\n21:28      3\n19:02      3\n22:33      3\n15:02      3\n11:31      3\n9:38       3\n12:23      3\n16:52      3\n16:36      3\n0:09       3\n23:23      3\n14:26      3\n0:23       3\n1:05       3\n16:01      3\n16:49      3\n19:34      3\n9:37       3\n0:31       3\n18:11      3\n12:06      3\n3:10       3\n13:02      3\n2:23       3\n1:08       3\n1:21       3\n22:18      3\n23:44      3\n11:38      3\n19:01      3\n9:48       3\n16:44      3\n9:46       3\n2:58       3\n14:38      3\n4:11       3\n13:27      3\n9:44       3\n9:22       3\n18:44      3\n14:29      3\n22:02      3\n8:13       3\n14:41      3\n16:18      3\n22:39      3\n7:04       3\n11:49      3\n6:27       3\n8:29       3\n19:27      3\n18:29      3\n19:26      3\n13:18      3\n12:56      3\n4:33       3\n11:33      3\n7:22       3\n12:21      3\n21:09      3\n8:48       3\n3:48       3\n2:15       3\n6:17       3\n19:58      3\n8:14       3\n12:24      3\n14:48      3\n21:43      3\n19:16      3\n16:39      3\n1:48       3\n7:08       3\n2:47       3\n4:48       3\n18:16      3\n12:26      3\n14:04      3\n16:38      3\n7:07       3\n5:15       3\n4:43       3\n4:36       3\n17:09      3\n15:12      3\n14:31      3\n15:47      3\n18:53      3\n12:36      3\n15:22      3\n15:16      3\n0:55       3\n1:39       3\n12:58      3\n19:07      3\n7:12       3\n5:48       3\n12:22      3\n21:14      3\n5:53       3\n16:53      3\n18:21      3\n16:21      3\n9:42       3\n11:28      3\n11:02      3\n12:54      3\n22:53      3\n1:22       3\n16:13      3\n11:13      3\n15:17      3\n11:12      3\n13:43      3\n13:26      3\n18:47      3\n14:46      3\n9:21       3\n5:03       3\n16:34      3\n12:04      3\n14:22      3\n18:49      3\n18:46      3\n21:38      3\n3:58       3\n19:36      3\n20:34      3\n22:16      3\n19:47      3\n1:28       3\n13:48      3\n10:13      3\n17:04      3\n10:11      3\n5:29       3\n17:26      3\n7:33       3\n14:59      2\n23:33      2\n7:26       2\n5:42       2\n20:57      2\n10:23      2\n2:06       2\n10:56      2\n6:19       2\n15:36      2\n15:52      2\n1:24       2\n12:42      2\n23:29      2\n6:31       2\n11:42      2\n2:48       2\n6:49       2\n6:09       2\n19:32      2\n8:19       2\n14:09      2\n14:13      2\n5:07       2\n14:24      2\n10:58      2\n21:53      2\n7:47       2\n19:54      2\n17:52      2\n10:48      2\n17:03      2\n1:57       2\n3:06       2\n13:56      2\n17:28      2\n2:54       2\n11:58      2\n20:07      2\n11:48      2\n22:28      2\n10:27      2\n19:43      2\n0:04       2\n3:29       2\n15:28      2\n14:52      2\n14:01      2\n16:56      2\n1:38       2\n11:09      2\n11:14      2\n21:36      2\n8:23       2\n0:46       2\n7:37       2\n6:14       2\n11:44      2\n6:23       2\n4:56       2\n4:47       2\n7:44       2\n0:51       2\n7:31       2\n20:53      2\n10:37      2\n4:59       2\n10:03      2\n22:26      2\n13:41      2\n6:46       2\n2:17       2\n4:58       2\n22:59      2\n9:43       2\n5:08       2\n8:43       2\n4:24       2\n3:04       2\n11:52      2\n22:24      2\n0:36       2\n21:34      2\n20:52      2\n20:01      2\n17:46      2\n17:54      2\n21:11      2\n23:36      2\n7:59       2\n22:48      2\n23:48      2\n5:36       2\n22:56      2\n20:56      2\n11:46      2\n23:59      2\n9:59       2\n15:34      2\n19:37      2\n18:12      2\n8:47       2\n15:46      2\n5:21       2\n11:18      2\n16:06      2\n19:22      2\n9:18       2\n20:14      2\n12:12      2\n14:34      2\n18:59      2\n19:12      2\n11:56      2\n18:26      2\n6:21       2\n18:13      2\n21:42      2\n1:53       2\n16:16      2\n5:16       2\n15:41      2\n0:41       2\n10:46      2\n23:13      2\n12:07      2\n10:42      2\n10:34      2\n2:14       2\n0:02       2\n19:28      2\n8:34       2\n11:34      2\n23:27      2\n12:01      2\n2:16       2\n8:42       2\n21:47      2\n2:32       2\n23:31      2\n2:36       2\n14:47      2\n5:28       2\n0:27       2\n4:17       2\n22:31      2\n0:12       2\n16:59      2\n22:04      2\n22:58      2\n11:27      2\n2:42       2\n5:09       2\n15:18      2\n1:37       2\n6:28       2\n17:07      2\n8:44       2\n17:49      2\n9:06       2\n4:32       2\n1:04       2\n17:34      2\n13:44      2\n22:23      2\n12:08      2\n5:35       2\n6:07       2\n13:34      2\n0:14       2\n10:41      2\n11:32      2\n18:56      2\n13:38      2\n4:23       2\n20:04      2\n20:11      2\n13:24      2\n21:24      2\n12:34      2\n18:09      2\n0:53       2\n22:07      2\n21:39      2\n0:58       2\n23:02      2\n3:42       2\n20:17      2\n9:58       2\n13:59      2\n0:43       2\n3:46       2\n8:09       2\n23:19      2\n3:44       2\n11:39      2\n11:22      2\n19:03      2\n12:32      2\n6:47       2\n23:04      2\n2:13       2\n21:37      2\n7:43       2\n12:29      2\n22:46      2\n23:16      2\n11:24      2\n9:16       2\n21:44      2\n19:17      2\n1:12       2\n3:08       2\n6:51       2\n13:21      2\n9:07       2\n0:08       2\n1:07       2\n3:14       2\n10:21      2\n11:54      2\n2:34       2\n12:13      2\n2:11       2\n1:11       2\n4:19       2\n6:36       2\n6:08       2\n23:12      2\n20:47      2\n14:54      2\n18:52      2\n6:22       2\n4:52       2\n14:33      2\n0:11       2\n9:31       2\n23:38      2\n22:19      2\n14:27      2\n13:58      2\n19:38      2\n1:32       2\n7:06       2\n23:51      2\n2:56       2\n8:24       2\n16:17      2\n10:43      2\n14:39      2\n13:22      2\n0:32       2\n8:39       2\n6:54       2\n1:13       2\n2:08       2\n19:59      2\n6:59       2\n10:24      2\n10:36      2\n21:32      2\n19:31      2\n20:23      2\n21:07      2\n7:32       2\n2:39       2\n5:06       2\n10:51      2\n21:23      2\n0:38       2\n8:27       2\n8:54       2\n6:13       2\n9:53       2\n7:42       2\n21:22      2\n16:32      2\n10:08      2\n1:03       2\n9:03       2\n7:58       2\n10:04      2\n8:56       2\n13:12      2\n15:59      2\n20:38      2\n0:03       2\n6:53       2\n5:27       2\n20:41      2\n0:42       2\n19:33      2\n7:39       2\n12:46      1\n15:29      1\n10:49      1\n17:59      1\n6:34       1\n3:38       1\n3:57       1\n9:52       1\n11:17      1\n21:51      1\n9:32       1\n12:17      1\n23:52      1\n18:37      1\n3:41       1\n2:33       1\n13:49      1\n12:52      1\n0:39       1\n20:28      1\n1:34       1\n22:08      1\n16:54      1\n5:39       1\n8:01       1\n14:23      1\n1:06       1\n21:04      1\n5:37       1\n3:13       1\n1:59       1\n1:46       1\n23:56      1\n9:29       1\n7:13       1\n23:17      1\n3:17       1\n6:04       1\n15:49      1\n2:43       1\n3:21       1\n11:51      1\n5:14       1\n10:31      1\n19:08      1\n1:01       1\n10:54      1\n9:09       1\n5:56       1\n21:26      1\n21:19      1\n13:11      1\n4:46       1\n20:48      1\n4:42       1\n6:16       1\n13:19      1\n7:52       1\n16:58      1\n0:49       1\n22:41      1\n8:36       1\n12:51      1\n17:17      1\n8:52       1\n2:52       1\n13:51      1\n21:08      1\n12:31      1\n22:12      1\n3:22       1\n13:47      1\n23:07      1\n8:12       1\n22:11      1\n3:07       1\n9:04       1\n5:49       1\n0:44       1\n4:37       1\n2:21       1\n19:42      1\n2:19       1\n22:54      1\n13:16      1\n18:39      1\n11:37      1\n9:13       1\n19:14      1\n19:13      1\n22:27      1\n20:29      1\n3:11       1\n23:24      1\n22:49      1\n5:24       1\n20:16      1\n11:23      1\n0:18       1\n1:26       1\n0:57       1\n18:31      1\n19:11      1\n1:14       1\n3:33       1\n7:28       1\n18:02      1\n10:14      1\n7:19       1\n0:16       1\n19:41      1\n1:19       1\n6:11       1\n21:21      1\n8:49       1\n22:36      1\n3:02       1\n23:09      1\n10:19      1\n13:57      1\n9:33       1\n5:13       1\n17:51      1\n3:37       1\n1:33       1\n20:02      1\n4:39       1\n15:37      1\n3:53       1\n5:11       1\n2:27       1\n10:29      1\n14:32      1\n4:12       1\n17:42      1\n5:38       1\n5:12       1\n17:43      1\n8:46       1\n3:01       1\n4:18       1\n11:29      1\n11:21      1\n10:16      1\n17:41      1\n17:01      1\n5:31       1\n20:32      1\n11:41      1\n6:33       1\n13:33      1\n1:47       1\n23:54      1\n22:06      1\n13:42      1\n21:52      1\n21:56      1\n9:41       1\n7:36       1\n13:37      1\n8:16       1\n3:32       1\n18:36      1\n7:56       1\n1:43       1\n16:51      1\n15:06      1\n7:57       1\n3:09       1\n7:24       1\n21:27      1\n6:57       1\n5:17       1\n9:17       1\n4:03       1\n0:33       1\n1:42       1\n14:36      1\n2:09       1\n7:51       1\n3:59       1\n16:31      1\n7:18       1\n8:06       1\n15:57      1\n21:57      1\n10:17      1\n7:09       1\n9:14       1\n18:32      1\n13:04      1\n0:28       1\n3:16       1\n21:54      1\n4:53       1\n2:49       1\n6:02       1\n1:49       1\n1:41       1\n2:53       1\n23:58      1\n2:41       1\n21:41      1\n15:01      1\n22:22      1\n4:06       1\n0:06       1\n10:44      1\n4:16       1\n5:22       1\n6:26       1\n0:24       1\n6:12       1\n0:37       1\n20:46      1\n23:18      1\n2:29       1\n23:03      1\n19:57      1\n21:01      1\n11:08      1\n23:46      1\n7:23       1\n22:03      1\n4:51       1\n12:33      1\n2:38       1\n5:41       1\n9:01       1\n20:49      1\n2:22       1\n17:56      1\n3:03       1\n2:31       1\n19:09      1\n1:23       1\n7:16       1\n20:13      1\n5:47       1\n14:19      1\n5:44       1\n0:48       1\n18:33      1\n14:28      1\n1:17       1\n7:29       1\n12:43      1\n18:57      1\n19:56      1\n13:06      1\n13:03      1\n7:38       1\n3:28       1\n10:18      1\n6:44       1\n23:28      1\n23:26      1\n18:42      1\n21:18      1\n1:29       1\n20:12      1\n12:38      1\n11:04      1\n20:21      1\n1:58       1\n12:16      1\n17:39      1\n1:09       1\n13:46      1\n1:36       1\n9:47       1\n8:59       1\n16:19      1\n2:37       1\n3:43       1\n0:29       1\n12:59      1\n20:58      1\n1:18       1\n3:36       1\n4:01       1\n7:53       1\n13:29      1\n21:59      1\n22:51      1\n4:38       1\n2:24       1\n3:18       1\n4:22       1\n7:48       1\n19:23      1\n4:28       1\n4:09       1\n22:37      1\n4:14       1\n22:44      1\n15:39      1\n22:09      1\n4:54       1\nName: CRASH TIME, dtype: int64\n\n\nFor example, here are some cleaning steps:\n\nimport numpy as np\n\njan22[\"CONTRIBUTING FACTOR VEHICLE 1\"] = (\n    jan22[\"CONTRIBUTING FACTOR VEHICLE 1\"].replace([\"Unspecified\"], np.nan))\njan22[\"CONTRIBUTING FACTOR VEHICLE 2\"] = (\n    jan22[\"CONTRIBUTING FACTOR VEHICLE 2\"].replace([\"Unspecified\"], np.nan))\njan22[\"CONTRIBUTING FACTOR VEHICLE 3\"] = (\n    jan22[\"CONTRIBUTING FACTOR VEHICLE 3\"].replace([\"Unspecified\"], np.nan))\njan22[\"CONTRIBUTING FACTOR VEHICLE 4\"] = (\n    jan22[\"CONTRIBUTING FACTOR VEHICLE 4\"].replace([\"Unspecified\"], np.nan))\njan22[\"CONTRIBUTING FACTOR VEHICLE 5\"] = (\n    jan22[\"CONTRIBUTING FACTOR VEHICLE 5\"].replace([\"Unspecified\"], np.nan))\njan22[\"LATITUDE\"] = jan22[\"LATITUDE\"].replace([0.0], np.nan)\njan22[\"LONGITUDE\"] = jan22[\"LONGITUDE\"].replace([0.0], np.nan)\njan22.describe()\n\n\n\n\n\n  \n    \n      \n      ZIP CODE\n      LATITUDE\n      LONGITUDE\n      NUMBER OF PERSONS INJURED\n      NUMBER OF PERSONS KILLED\n      NUMBER OF PEDESTRIANS INJURED\n      NUMBER OF PEDESTRIANS KILLED\n      NUMBER OF CYCLIST INJURED\n      NUMBER OF CYCLIST KILLED\n      NUMBER OF MOTORIST INJURED\n      NUMBER OF MOTORIST KILLED\n      COLLISION_ID\n    \n  \n  \n    \n      count\n      5025.000000\n      7077.000000\n      7077.000000\n      7659.000000\n      7659.000000\n      7659.000000\n      7659.000000\n      7659.000000\n      7659.0\n      7659.000000\n      7659.000000\n      7.659000e+03\n    \n    \n      mean\n      10908.957015\n      40.724228\n      -73.913799\n      0.404753\n      0.002350\n      0.084345\n      0.001306\n      0.021935\n      0.0\n      0.287505\n      0.000914\n      4.495510e+06\n    \n    \n      std\n      514.740028\n      0.083768\n      0.086007\n      0.726622\n      0.048425\n      0.290332\n      0.036113\n      0.149131\n      0.0\n      0.692545\n      0.030220\n      2.338111e+03\n    \n    \n      min\n      10000.000000\n      40.502340\n      -74.249980\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.0\n      0.000000\n      0.000000\n      4.491064e+06\n    \n    \n      25%\n      10459.000000\n      40.665474\n      -73.960440\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.0\n      0.000000\n      0.000000\n      4.493532e+06\n    \n    \n      50%\n      11209.000000\n      40.712696\n      -73.917564\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.0\n      0.000000\n      0.000000\n      4.495533e+06\n    \n    \n      75%\n      11354.000000\n      40.787506\n      -73.862816\n      1.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.0\n      0.000000\n      0.000000\n      4.497480e+06\n    \n    \n      max\n      11697.000000\n      40.909206\n      -73.701920\n      8.000000\n      1.000000\n      3.000000\n      1.000000\n      2.000000\n      0.0\n      8.000000\n      1.000000\n      4.500594e+06"
  },
  {
    "objectID": "nycrash.html#filling-the-missing-zip-codes-by-reverse-geocoding",
    "href": "nycrash.html#filling-the-missing-zip-codes-by-reverse-geocoding",
    "title": "7  NYC Crash Data",
    "section": "7.3 Filling the Missing Zip Codes by Reverse Geocoding",
    "text": "7.3 Filling the Missing Zip Codes by Reverse Geocoding\nThe package uszipcode is the most powerful and easy to use programmable zipcode database in Python. It provides information about 42,724 zipcodes in the US with data crawled from <data.census.gov>. See its documentation for details.\n\nfrom uszipcode import SearchEngine\n\nsr = SearchEngine()\nsr.by_zipcode(\"10001\")\n\nSimpleZipcode(zipcode='10001', zipcode_type='STANDARD', major_city='New York', post_office_city='New York, NY', common_city_list=['New York'], county='New York County', state='NY', lat=40.75, lng=-74.0, timezone='America/New_York', radius_in_miles=0.9090909090909091, area_code_list='718,917,347,646', population=21102, population_density=33959.0, land_area_in_sqmi=0.62, water_area_in_sqmi=0.0, housing_units=12476, occupied_housing_units=11031, median_home_value=650200, median_household_income=81671, bounds_west=-74.008621, bounds_east=-73.984076, bounds_north=40.759731, bounds_south=40.743451)\n\n\nWe can use uszipcode to reverse geocode a point by its coordinates. The returned zipcode can be used to handle missing zipcode.\n\nz = sr.by_coordinates(40.769993, -73.915825, radius = 1)\nz[0].zipcode\nz[0].median_home_value\n\n597700\n\n\nOnce we have found the zipcode, we can find its borough. See the complete NYC zip code list.\n\ndef nyczip2burough(zip):\n    nzip = int(zip)\n    if nzip >= 10001 and nzip <= 10282:\n        return \"MANHATTAN\"\n    elif nzip >= 10301 and nzip <= 10314:\n        return \"STATEN ISLAND\"\n    elif nzip >= 10451 and nzip <= 10475:\n        return \"BRONX\"\n    elif nzip >= 11004 and nzip <= 11109:\n        return \"QUEENS\"\n    elif nzip >= 11351 and nzip <= 11697:\n        return \"QUEENS\"\n    elif nzip >= 11201 and nzip <= 11256:\n        return \"BROOKLYN\"\n    else:\n        return np.nan\n\nLet’s try it out:\n\nnyczip2burough(z[0].zipcode)\n\n'QUEENS'\n\n\nHere is a vectorized version:\n\nimport numpy as np\nimport pandas as pd\nfrom typing import Union, List\n\ndef nyczip2borough(zips: Union[np.ndarray, pd.Series]) -> Union[np.ndarray, pd.Series]:\n    zips = zips.values if isinstance(zips, pd.Series) else zips\n    condlist = [\n        (zips >= 10001) & (zips <= 10282),\n        (zips >= 10301) & (zips <= 10314),\n        (zips >= 10451) & (zips <= 10475),\n        (zips >= 11004) & (zips <= 11109),\n        (zips >= 11351) & (zips <= 11697),\n        (zips >= 11201) & (zips <= 11256),\n    ]\n    choicelist = [\n        \"MANHATTAN\",\n        \"STATEN ISLAND\",\n        \"BRONX\",\n        \"QUEENS\",\n        \"QUEENS\",\n        \"BROOKLYN\",\n    ]\n    result = np.select(condlist, choicelist, default=np.nan)\n    return pd.Series(result) if isinstance(zips, pd.Series) else result\n\nTry it out\n\nnyczip2borough(jan22[\"ZIP CODE\"].dropna().head(10))\n\narray(['QUEENS', 'QUEENS', 'BROOKLYN', 'BRONX', 'QUEENS', 'BROOKLYN',\n       'BROOKLYN', 'MANHATTAN', 'BRONX', 'BRONX'], dtype='<U32')"
  },
  {
    "objectID": "mysection.html#introduction",
    "href": "mysection.html#introduction",
    "title": "8  My Presentation Topic",
    "section": "8.1 Introduction",
    "text": "8.1 Introduction\nPut an overview here. Use Markdown syntax."
  },
  {
    "objectID": "mysection.html#sub-topic-1",
    "href": "mysection.html#sub-topic-1",
    "title": "8  My Presentation Topic",
    "section": "8.2 Sub Topic 1",
    "text": "8.2 Sub Topic 1\nPut materials on topic 1 here\nPython examples can be put into python code chunks:\n\nimport pandas as pd\n\n# do something"
  },
  {
    "objectID": "mysection.html#sub-topic-2",
    "href": "mysection.html#sub-topic-2",
    "title": "8  My Presentation Topic",
    "section": "8.3 Sub Topic 2",
    "text": "8.3 Sub Topic 2\nPut materials on topic 2 here."
  },
  {
    "objectID": "mysection.html#conclusion",
    "href": "mysection.html#conclusion",
    "title": "8  My Presentation Topic",
    "section": "8.4 Conclusion",
    "text": "8.4 Conclusion\nPut sumaries here."
  },
  {
    "objectID": "exercises.html",
    "href": "exercises.html",
    "title": "9  Exercises",
    "section": "",
    "text": "Pick up Git basics and set up an account at GitHub if you don’t have one. Please practice the tips on Git in the notes. Make sure you have at least 10 commits in the repo, each with informative message. Keep checking the status of your repo with git status. My grader will grade the repo.\n\nClone the ids-s23 repo to your own computer.\nAdd your name and wishes to the Wishlist; commit with an informative message.\nRemove the Last, First entry from the list; commit.\nCreate a new file called add.qmd containing a few lines of texts; commit.\nRemove add.qmd (pretending that this is by accident; commit.\nRecover the accidently removed file add.qmd; add a long line (a paragraph without a hard break); add a short line (under 80 characters); commit.\nChange one word in the long line and one word in the short line; use git diff to see the difference from the last commit; commit.\nPut the repo into the GitHub Classroom homework repo with git remote add and git push.\n\nGet ready for contributing to the classnotes.\n\nCreate a fork of the ids-s23 repo into your own GitHub account.\nClone it to your local computer.\nMake a new branch to experiment with your changes.\nCheckout your branch and add your wishes to the wish list; push to your GitHub account.\nMake a pull request to my ids-s23 repo from your fork at GitHub. Make sure you have clear messages to document the changes.\n\nWrite a function to demonstrate the Monty Hall problem through simulation. The function takes two arguments ndoors and ntrials, representing the number of doors in the experiment and the number of trails in a simulation, respectively. The function should return the proportion of wins for both the switch and no-switch strategy. Apply your function with 3 doors and 5 doors, both with 1000 trials. Include sufficient text around the code to explain your them.\nWrite a function to do a Monte Carlo approximation of \\(\\pi\\). The function takes a Monte Carlo sample size n as input, and returns a point estimate of \\(\\pi\\) and a 95% confidence interval. Apply your function with sample size 1000, 2000, 4000, and 8000. Repeat the experiment 1000 times for each sample size and check the empirical probability that the confidence intervals cover the true value of \\(\\pi\\). Comment on the results.\nFind the first 10-digit prime number occurring in consecutive digits of \\(e\\). This was a Google recruiting ad\nThe NYC motor vehicle collisions data with documentation is available from NYC Open Data. The raw data needs some cleaning. (JY: Add variable name cleaning next year.)\n\nUse the filter from the website to download the crash data of January 2023; save it under a directory data with an informative name (e.g., nyc_crashes_202301.csv).\nGet basic summaries of each variable: missing percentage; descriptive statistics for continuous variables; frequency tables for discrete variables.\nAre the LATITUDE and LONGITIDE values all look legitimate? If not (e.g., zeroes), code them as missing values.\nIf OFF STREET NAME is not missing, are there any missing LATITUDE and LONGITUDE? If so, geocode the addresses.\n(Optional) Are the missing patterns of ON STREET NAME and LATITUDE the same? Summarize the missing patterns by a cross table. If ON STREET NAME and CROSS STREET NAME are available, use geocoding by intersection to fill the LATITUDE and LONGITUDE.\nAre ZIP CODE and BOROUGH always missing together? If LATITUDE and LONGITUDE are available, use reverse geocoding to fill the ZIP CODE and BOROUGH.\nPrint the whole frequency table of CONTRIBUTING FACTOR VEHICLE 1. Convert lower cases to uppercases and check the frequencies again.\nProvided an opportunity to meet the data provider, what suggestions do you have to make the data better based on your data exploration experience?\n\nExcept the first problem, use the cleaned data set with missing geocode imputed.\n\nConstruct a contigency table for missing in geocode (latitude and longitude) by borough. Is the missing pattern the same across borough? Formulate a hypothesis and test it.\nConstruct a hour variable with integer values from 0 to 23. Plot the histogram of the number of crashes by hour. Plot it by borough.\nOverlay the locations of the crashes on a map of NYC. The map could be a static map or Google map.\nCreate a new variable injury which is one if the number of persons killed is 1 or more; and zero otherwise. Construct a cross table for injury versus borough. Test the null hypothesis that the two variables are not associated.\nMerge the crash data with the zip code database.\nFit a logistic model with injury as the outcome variable and covariates that are available in the data or can be engineered from the data. For example, zip code level covariates can be obtained by merging with the zip code database."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "VanderPlas, Jake. 2016. Python Data Science Handbook:\nEssential Tools for Working with Data. O’Reilly Media,\nInc."
  }
]