[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Data Science",
    "section": "",
    "text": "Preface\nThe notets are a Quarto book; for details visit https://quarto.org/docs/books.\nThe notes are a joint effort of the instructor and the students in STAT 3255/5255, Spring 2023. The GitHub repo is at https://github.com/statds/ids-s23. The GitHub repo of the notes from Spring 2022 are available at https://github.com/statds/ids-s22.\nOur mid-term project on the 311 requests of New York City is to be showcased at the NYC Open Data Week during 2-3 pm ET, Monday, March 13, 2023..\nAn interesting quote from VanderPlas (2016):\n\nWhen a technologically-minded person is asked to help a friend, family member, or colleague with a computer problem, most of the time it’s less a matter of knowing the answer as much as knowing how to quickly find an unknown answer. In data science it’s the same: searchable web resources such as online documentation, mailing-list threads, and StackOverflow answers contain a wealth of information, even (especially?) if it is a topic you’ve found yourself searching before. Being an effective practitioner of data science is less about memorizing the tool or command you should use for every possible situation, and more about learning to effectively find the information you don’t know, whether through a web search engine or another means.\n\n\n\n\n\nVanderPlas, Jake. 2016. Python Data Science Handbook: Essential Tools for Working with Data. O’Reilly Media, Inc."
  },
  {
    "objectID": "intro.html#what-is-data-science",
    "href": "intro.html#what-is-data-science",
    "title": "1  Introduction",
    "section": "1.1 What Is Data Science?",
    "text": "1.1 What Is Data Science?\nOne widely accepted concept is the three pillars of data science: mathematics/statistics, computer science, and domain knowledge.\nIn her 2014 Presidential Address, Prof. Bin Yu, then President of the Institute of Mathematical Statistics, gave an interesting definition: \\[\n\\mbox{Data Science} =\n\\mbox{S}\\mbox{D}\\mbox{C}^3,\n\\] where S is Statistics, D is domain/science knowledge, and the three C’s are computing, collaboration/teamwork, and communication to outsiders."
  },
  {
    "objectID": "intro.html#expectations-from-this-course",
    "href": "intro.html#expectations-from-this-course",
    "title": "1  Introduction",
    "section": "1.2 Expectations from This Course",
    "text": "1.2 Expectations from This Course\n\nProficiency in project management with Git.\nProficiency in project report with Quarto.\nHands-on experience with real-world data science project.\nCompetency in using Python and its extensions for data science.\nFull grasp of the meaning of the results from data science algorithms.\nBasic understanding the principles of the data science methods."
  },
  {
    "objectID": "intro.html#computing-environment",
    "href": "intro.html#computing-environment",
    "title": "1  Introduction",
    "section": "1.3 Computing Environment",
    "text": "1.3 Computing Environment\nAll setups are operating system dependent. As soon as possible, stay away from Windows. Otherwise, good luck (you will need it).\n\n1.3.1 Command Line Interface\nOn Linux or MacOS, simply open a terminal.\nOn Windows, several options can be considered.\n\nCygwin (with X): https://x.cygwin.com\nGit Bash: https://www.gitkraken.com/blog/what-is-git-bash\n\nTo jump start, here is a tutorial: Ubunto Linux for beginners.\nAt least, you need to know how to handle files and traverse across directories. The tab completion and introspection supports are very useful.\n\n\n1.3.2 Python\nSet up Python on your computer:\n\nPython 3.\nPython package manager miniconda or pip.\nIntegrated Development Environment (IDE) (Jupyter Notebook; RStudio; VS Code; Emacs; etc.)\n\nI will be using IPython and Jupyter Notebook in class.\nReadability is important! Check your Python coding styles against the recommended styles: https://peps.python.org/pep-0008/. A good place to start is the Section on “Code Lay-out”.\nOnline books on Python for data science:\n\n“Python Data Science Handbook: Essential Tools for Working with Data,” First Edition, by Jake VanderPlas, O’Reilly Media, 2016.\n\n\n“Python for Data Analysis: Data Wrangling with Pan- das, NumPy, and IPython.” Third Edition, by Wes McK- inney, O’Reilly Media, 2022."
  },
  {
    "objectID": "intro.html#data-challenges",
    "href": "intro.html#data-challenges",
    "title": "1  Introduction",
    "section": "1.4 Data Challenges",
    "text": "1.4 Data Challenges\n\nASA Data Challenge Expo\nKaggle\nDrivenData\nTop 10 Data Science Competitions in 2023"
  },
  {
    "objectID": "intro.html#wishlist",
    "href": "intro.html#wishlist",
    "title": "1  Introduction",
    "section": "1.5 Wishlist",
    "text": "1.5 Wishlist\nThis is a wish list from all members of the class (alphabetical order). Add yours; note the syntax of nested list in Markdown.\n\nAlsubai, Nadia\n\nBecome familiar with both machine learning and deep learning\nBecome proficient in at least one machine or deep learning library for Python\n\nBedard, Kaitlyn\n\nLearn how to use Git proficiently\nGain practical experience using data science methods\nLearn to use python libraries for data science\n\nCheu, Catherine\n\nLearn more about command prompts in Git Bash\nLearn more about data science principles\nLearn better programming techniques in Python\nBecome better in simulation techniques.\n\nHo, Garrick\n\nI want to be more confident with git\nBe able to create a data science project\nLearn more about data science\n\nJones, Courtney\n\nBecome proficient in Git and VS Code\nDeepen my understanding of data science\nBecome comfortable with the Terminal on my computer\n\nKarandikar, Shivaram\n\nUnderstand the workflow and life cycle of a data science project.\nLearn how to code efficiently.\n\nLunetta, Giovanni\n\nLearn to properly clean a dataset\nBecome proficient in building a machine learning model\n\nMastrorilli, Ginamarie\n\nBecome more comfortable with git.\nIncrease my ability to learn new programs.\n\nNguyen, Christine\n\nBecome proficient in Git and apply it.\nBuild my Python programming skills further.\n\nNhan, Nathan\n\nProperly learn how to use Git\nIncrease my understanding of data science and data collection\n\n\nNoel, Luke\n\nBecome proficient in Git/Github\nLearn data science techniques like deep/machine learning, etc.\n\nParchekani, Kian\n\nLearn how to use Git, Quarto\nGet an introduction to data science\nLearn if a career in this field is right for me\nCollaborate with others and gain project experience\n\nShen, Tong\n\nGet familiar with machine learning\nLearn how to deal with big secondary data\nGain some experiences with python\n\nSullivan, Collin\n\nI would like to learn to be able to use data science well enough to get a job in the field\nDiscover if this area of statistics is one that I am passionate about\nGain some project experience that I can cite or reference in interviews\nBe able to speak intelligently about data science and it’s facets\nGain practical experience\n\nWang, Chaoyang\n\nLearn Deep Learning and application on Finance\n\nYan, Jun\n\nMake data science more accessible to undergraduates\nCo-develop a Quarto Book in collaboration with the students\n\nYang Kang, Chua\n\nLearn more about git and github\nApply Data Science skill to my research\nDevelop a new machine learning model\n\nYeung, Shannon\n\nLearn more about Git\nget aa more well rounded understanding of data science\n\nYi, Guanghong\n\nKnow more about Data Science, and what Data Scientists do\nDo one(or more) real life data science project, and gain some practical experience.\n\nZheng, Michael\n\nBecome more comfortable with git\nLearn how to complete a data science project from start to finish\n\n\n\n1.5.1 Presentation Orders\nThe topic presentation order is set up in class.\n\npresenters = [\"Alsubai, Nadia\",\n              \"Bedard, Kaitlyn\",\n              \"Cheu, Catherine\",\n              \"Chua, Yang Kang\",\n              \"Cummins, Patrick\",\n              \"Ho, Garrick\",\n              \"Jones, Courtney\",\n              \"Karandikar, Shivaram\",\n              \"Lunetta, Giovanni\",\n              \"Mastrorilli, Ginamarie\",\n              \"Nguyen, Christine\",\n              \"Nhan, Nathan\",\n              \"Noel, Luke\",\n              \"Parchekani, Kian\",\n              \"Shen, Tong\",\n              \"Sullivan, Colin\",\n              \"Wang, Chaoyang\",\n              \"Whitney, William\",\n              \"Yeung, Shannon\",\n              \"Yi, Guanghong\",\n              \"Zheng, Michael\"]\n\nimport random\nrandom.seed(71323498112697523) # jointly set by the class on 01/30/2023\nrandom.sample(presenters, len(presenters))\n\n['Cheu, Catherine',\n 'Ho, Garrick',\n 'Mastrorilli, Ginamarie',\n 'Yi, Guanghong',\n 'Karandikar, Shivaram',\n 'Chua, Yang Kang',\n 'Jones, Courtney',\n 'Sullivan, Colin',\n 'Shen, Tong',\n 'Alsubai, Nadia',\n 'Yeung, Shannon',\n 'Bedard, Kaitlyn',\n 'Nhan, Nathan',\n 'Parchekani, Kian',\n 'Noel, Luke',\n 'Whitney, William',\n 'Wang, Chaoyang',\n 'Nguyen, Christine',\n 'Cummins, Patrick',\n 'Zheng, Michael',\n 'Lunetta, Giovanni']"
  },
  {
    "objectID": "intro.html#presentation-task-board",
    "href": "intro.html#presentation-task-board",
    "title": "1  Introduction",
    "section": "1.6 Presentation Task Board",
    "text": "1.6 Presentation Task Board\nHere are some example tasks:\n\nImport/Export data\nDescriptive statistics\nStatistical hypothesis tests scypy.stats\nModel formulas with patsy\nStatistical models with statsmodels\nData visualization with matplotlib\nGrammer of graphics for python plotnine\nHandling spatial data with geopandas\nShow your Data in a Google map with gmplot\nRandom forest\nNaive Bayes\nBagging vs boosting\nCalling C/C++ from Python\nCalling R from Python and vice versa\nDevelop a Python module\nNeural networks\nDeep learning\nTensorFlow\nAutoencoders\nReinforcement learning\n\nPlease use the following table to sign up. \n\n\n\n\n\n\n\n\nDate\nPresenter\nTopic\n\n\n\n\n02/06\nCheu, Catherine\nIntroduction to matplotlib in Python\n\n\n02/08\nHo, Garrick\nPandas part 1\n\n\n02/13\nMastrorilli, Ginamarie\nPandas part 2\n\n\n02/13\nYi, Guanghong\nGrammer of graphics for python plotnine\n\n\n02/15\nKarandikar, Shivaram\nText processing with NLTK\n\n\n02/20\nChua, Yang Kang\nSupport Vector Machine with scikit-learn\n\n\n02/20\nJones, Courtney\nDescriptive Statistics\n\n\n02/22\nSullivan, Colin\nStatistical hypothesis tests scypy.stats\n\n\n02/27\nShen, Tong\nDecision tree with scikit-learn\n\n\n02/27\nAlsubai, Nadia\nWeb Scraping with Beautiful Soup\n\n\n03/01\nBedard, Kaitlyn\nHandling spatial data with geopandas\n\n\n03/06\nNhan, Nathan\n\n\n\n03/08\nParchekani, Kian\n\n\n\n03/20\nNoel, Luke\nPlotting on maps with gmplot\n\n\n03/20\nWhitney, William\n\n\n\n03/22\nWang, Chaoyang\n\n\n\n03/27\nNguyen, Christine\nCalling R from Python and vice versa\n\n\n03/27\nCummins, Patrick\n\n\n\n03/29\nZheng, Michael\n\n\n\n04/03\nLunetta, Giovanni\nIntro to Softmax Regression & Neural Networks with TensorFlow"
  },
  {
    "objectID": "intro.html#contribute-to-the-class-notes",
    "href": "intro.html#contribute-to-the-class-notes",
    "title": "1  Introduction",
    "section": "1.7 Contribute to the Class Notes",
    "text": "1.7 Contribute to the Class Notes\n\nStart a new branch and switch to the new branch.\nOn the new branch, add a qmd file for your presentation\nEdit _quarto.yml add a line for your qmd file to include it in the notes.\nWork on your qmd file, test with quarto render.\nWhen satisfied, commit and make a pull request.\n\nI have added a template file mysection.qmd and a new line to _quarto.yml as an example.\nFor more detailed style guidance, please see my notes on statistical wrirting."
  },
  {
    "objectID": "intro.html#homework-requirements",
    "href": "intro.html#homework-requirements",
    "title": "1  Introduction",
    "section": "1.8 Homework Requirements",
    "text": "1.8 Homework Requirements\n\nUse the repo from Git Classroom to submit your work. See Chapter 2.\n\nKeep the repo clean (no tracking generated files).\nNever “Upload” your files; use the git command lines.\nMake commit message informative (think about the readers).\n\nUse quarto source only. See Chapter 3.\nFor the conveinence of greading, add your html output to a release in your repo."
  },
  {
    "objectID": "intro.html#my-presentation-topic",
    "href": "intro.html#my-presentation-topic",
    "title": "1  Introduction",
    "section": "1.9 My Presentation Topic",
    "text": "1.9 My Presentation Topic\n\n1.9.1 Introduction\nPut an overview here. Use Markdown syntax.\n\n\n1.9.2 Sub Topic 1\nPut materials on topic 1 here\nPython examples can be put into python code chunks:\n\nimport pandas as pd\n\n# do something\n\n\n\n1.9.3 Sub Topic 2\nPut materials on topic 2 here.\n\n\n1.9.4 Conclusion\nPut sumaries here."
  },
  {
    "objectID": "git.html#set-up",
    "href": "git.html#set-up",
    "title": "2  Project Management with Git",
    "section": "2.1 Set Up",
    "text": "2.1 Set Up\nTo set up GitHub (other services like Bitbucket or GitLab are similar), you need to\n\nGenerate an SSH key if you don’t have one already.\nSign up an GitHub account.\nAdd the SSH key to your GitHub account"
  },
  {
    "objectID": "git.html#most-frequently-used-git-commands",
    "href": "git.html#most-frequently-used-git-commands",
    "title": "2  Project Management with Git",
    "section": "2.2 Most Frequently Used Git Commands",
    "text": "2.2 Most Frequently Used Git Commands\n\ngit clone\ngit pull\ngit status\ngit add\ngit remove\ngit commit\ngit push"
  },
  {
    "objectID": "git.html#tips-on-using-git",
    "href": "git.html#tips-on-using-git",
    "title": "2  Project Management with Git",
    "section": "2.3 Tips on using Git:",
    "text": "2.3 Tips on using Git:\n\nUse the command line interface instead of the web interface (e.g., upload on GitHub)\nMake frequent small commits instead of rare large commits.\nMake commit messages informative and meaningful.\nName your files/folders by some reasonable convention.\n\nLower cases are better than upper cases.\nNo blanks in file/folder names.\n\nKeep the repo clean by not tracking generated files.\nCreat a .gitignore file for better output from git status.\nKeep the linewidth of sources to under 80 for better git diff view."
  },
  {
    "objectID": "git.html#pull-request",
    "href": "git.html#pull-request",
    "title": "2  Project Management with Git",
    "section": "2.4 Pull Request",
    "text": "2.4 Pull Request\nTo contribute to an open source project (e.g., our classnotes), use pull requests. Pull requests “let you tell others about changes you’ve pushed to a branch in a repository on GitHub. Once a pull request is opened, you can discuss and review the potential changes with collaborators and add follow-up commits before your changes are merged into the base branch.”\nWatch this YouTube video: GitHub pull requests in 100 seconds."
  },
  {
    "objectID": "quarto.html",
    "href": "quarto.html",
    "title": "3  Reproducibile Data Science with Quarto",
    "section": "",
    "text": "Data science projects should be reproducible to be trustworthy. Dynamic documents facilitate reproducibility. Quarto is an open-source dynamic document preparation system, ideal for scientific and technical publishing. From the official websites, Quarto can be used to:\n\nCreate dynamic content with Python, R, Julia, and Observable.\nAuthor documents as plain text markdown or Jupyter notebooks.\nPublish high-quality articles, reports, presentations, websites, blogs, and books in HTML, PDF, MS Word, ePub, and more.\nAuthor with scientific markdown, including equations, citations, crossrefs, figure panels, callouts, advanced layout, and more.\n\nOf course, Quarto can be used to write homework, exams, and reports in this course.\nTo get started, see documentation at Quarto.\nThis is an extra line."
  },
  {
    "objectID": "python.html#the-python-world",
    "href": "python.html#the-python-world",
    "title": "4  Python Refreshment",
    "section": "4.1 The Python World",
    "text": "4.1 The Python World\n\nFunction: a block of organized, reusable code to complete certain task.\nModule: a file containing a collection of functions, variables, and statements.\nPackage: a structured directory containing collections of modules and an __init.py__ file by which the directory is interpreted as a package.\nLibrary: a collection of related functionality of codes. It is a reusable chunk of code that we can use by importing it in our program, we can just use it by importing that library and calling the method of that library with period(.).\n\nSee, for example, how to build a Python libratry."
  },
  {
    "objectID": "python.html#standard-library",
    "href": "python.html#standard-library",
    "title": "4  Python Refreshment",
    "section": "4.2 Standard Library",
    "text": "4.2 Standard Library\nPython’s has an extensive standard library that offers a wide range of facilities as indicated by the long table of contents listed below. See documentation online.\n\nThe library contains built-in modules (written in C) that provide access to system functionality such as file I/O that would otherwise be inaccessible to Python programmers, as well as modules written in Python that provide standardized solutions for many problems that occur in everyday programming. Some of these modules are explicitly designed to encourage and enhance the portability of Python programs by abstracting away platform-specifics into platform-neutral APIs.\n\nQuestion: How to get the constant \\(e\\) to an arbitary precision?\nThe constant is only represented by a given double precision.\n\nimport math\nprint(\"%0.20f\" % math.e)\nprint(\"%0.80f\" % math.e)\n\n2.71828182845904509080\n2.71828182845904509079559829842764884233474731445312500000000000000000000000000000\n\n\nNow use package decimal to export with an arbitary precision.\n\nimport decimal  # for what?\n\n## set the required number digits to 150\ndecimal.getcontext().prec = 150\ndecimal.Decimal(1).exp().to_eng_string()\ndecimal.Decimal(1).exp().to_eng_string()[2:]\n\n'71828182845904523536028747135266249775724709369995957496696762772407663035354759457138217852516642742746639193200305992181741359662904357290033429526'"
  },
  {
    "objectID": "python.html#important-libraries",
    "href": "python.html#important-libraries",
    "title": "4  Python Refreshment",
    "section": "4.3 Important Libraries",
    "text": "4.3 Important Libraries\n\nNumPy\npandas\nmatplotlib\nIPython/Jupyter\nSciPy\nscikit-learn\nstatsmodels\n\nQuestion: how to draw a random sample from a normal distribution and evaluate the density and distributions at these points?\n\nfrom scipy.stats import norm\n\nmu, sigma = 2, 4\nmean, var, skew, kurt = norm.stats(mu, sigma, moments='mvsk')\nprint(mean, var, skew, kurt)\nx = norm.rvs(loc = mu, scale = sigma, size = 10)\nx\n\n2.0 16.0 0.0 0.0\n\n\narray([ -4.44562667,   5.4660809 ,   3.5125469 , -11.83950295,\n         2.90749401,   4.70256047,   4.90975057,   7.73920953,\n         3.50416655,  -4.78896058])\n\n\nThe pdf and cdf can be evaluated:\n\nnorm.pdf(x, loc = mu, scale = sigma)\n\narray([0.02722693, 0.06851781, 0.09285403, 0.00025086, 0.09720154,\n       0.07938247, 0.07654966, 0.03563019, 0.09292742, 0.02362275])"
  },
  {
    "objectID": "python.html#writing-a-function",
    "href": "python.html#writing-a-function",
    "title": "4  Python Refreshment",
    "section": "4.4 Writing a Function",
    "text": "4.4 Writing a Function\nConsider the Fibonacci Sequence \\(1, 1, 2, 3, 5, 8, 13, 21, 34, ...\\). The next number is found by adding up the two numbers before it. We are going to use 3 ways to solve the problems.\nThe first is a recursive solution.\n\ndef fib_rs(n):\n    if (n==1 or n==2):\n        return 1\n    else:\n        return fib_rs(n - 1) + fib_rs(n - 2)\n\n%timeit fib_rs(10)\n\n10.5 µs ± 331 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\nThe second uses dynamic programming memoization.\n\ndef fib_dm_helper(n, mem):\n    if mem[n] is not None:\n        return mem[n]\n    elif (n == 1 or n == 2):\n        result = 1\n    else:\n        result = fib_dm_helper(n - 1, mem) + fib_dm_helper(n - 2, mem)\n    mem[n] = result\n    return result\n\ndef fib_dm(n):\n    mem = [None] * (n + 1)\n    return fib_dm_helper(n, mem)\n\n%timeit fib_dm(10)\n\n2.38 µs ± 26.5 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\nThe third is still dynamic programming but bottom-up.\n\ndef fib_dbu(n):\n    mem = [None] * (n + 1)\n    mem[1]=1;\n    mem[2]=1;\n    for i in range(3,n+1):\n        mem[i] = mem[i-1] + mem[i-2]\n    return mem[n]\n\n\n%timeit fib_dbu(500)\n\n63.9 µs ± 478 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\nApparently, the three solutions have very different performance for larger n.\n\n4.4.1 Monte Hall\nHere is a function that performs the Monte Hall experiments.\n\nimport numpy as np\n\ndef montehall(ndoors, ntrials):\n    doors = np.arange(1, ndoors + 1) / 10\n    prize = np.random.choice(doors, size=ntrials)\n    player = np.random.choice(doors, size=ntrials)\n    host = np.array([np.random.choice([d for d in doors\n                                       if d not in [player[x], prize[x]]])\n                     for x in range(ntrials)])\n    player2 = np.array([np.random.choice([d for d in doors\n                                          if d not in [player[x], host[x]]])\n                        for x in range(ntrials)])\n    return {'noswitch': np.sum(prize == player), 'switch': np.sum(prize == player2)}\n\nTest it out:\n\nmontehall(3, 1000)\nmontehall(4, 1000)\n\n{'noswitch': 236, 'switch': 388}\n\n\nThe true value for the two strategies with \\(n\\) doors are, respectively, \\(1 / n\\) and \\(\\frac{n - 1}{n (n - 2)}\\)."
  },
  {
    "objectID": "python.html#variables-versus-objects",
    "href": "python.html#variables-versus-objects",
    "title": "4  Python Refreshment",
    "section": "4.5 Variables versus Objects",
    "text": "4.5 Variables versus Objects\nIn Python, variables and the objects they point to actually live in two different places in the computer memory. Think of variables as pointers to the objects they’re associated with, rather than being those objects. This matters when multiple variables point to the same object.\n\nx = [1, 2, 3]  # create a list; x points to the list\ny = x          # y also points to the same list in the memory\ny.append(4)    # append to y\nx              # x changed!\n\n[1, 2, 3, 4]\n\n\nNow check their addresses\n\nprint(id(x))   # address of x\nprint(id(y))   # address of y\n\n4959635584\n4959635584\n\n\nNonetheless, some data types in Python are “immutable”, meaning that their values cannot be changed in place. One such example is strings.\n\nx = \"abc\"\ny = x\ny = \"xyz\"\nx\n\n'abc'\n\n\nNow check their addresses\n\nprint(id(x))   # address of x\nprint(id(y))   # address of y\n\n4554513544\n4675105776\n\n\nQuestion: What’s mutable and what’s immutable?\nAnything that is a collection of other objects is mutable, except tuples.\nNot all manipulations of mutable objects change the object rather than create a new object. Sometimes when you do something to a mutable object, you get back a new object. Manipulations that change an existing object, rather than create a new one, are referred to as “in-place mutations” or just “mutations.” So:\n\nAll manipulations of immutable types create new objects.\nSome manipulations of mutable types create new objects.\n\nDifferent variables may all be pointing at the same object is preserved through function calls (a behavior known as “pass by object-reference”). So if you pass a list to a function, and that function manipulates that list using an in-place mutation, that change will affect any variable that was pointing to that same object outside the function.\n\nx = [1, 2, 3]\ny = x\n\ndef append_42(input_list):\n    input_list.append(42)\n    return input_list\n\nappend_42(x)\n\n[1, 2, 3, 42]\n\n\nNote that both x and y have been appended by \\(42\\)."
  },
  {
    "objectID": "python.html#number-representation",
    "href": "python.html#number-representation",
    "title": "4  Python Refreshment",
    "section": "4.6 Number Representation",
    "text": "4.6 Number Representation\nNumers in a computer’s memory are represented by binary styles (on and off of bits).\n\n4.6.1 Integers\nIf not careful, It is easy to be bitten by overflow with integers when using Numpy and Pandas in Python.\n\nimport numpy as np\n\nx = np.array(2**63 - 1 , dtype='int')\nx\n# This should be the largest number numpy can display, with\n# the default int8 type (64 bits)\n\narray(9223372036854775807)\n\n\nWhat if we increment it by 1?\n\ny = np.array(x + 1, dtype='int')\ny\n# Because of the overflow, it becomes negative!\n\narray(-9223372036854775808)\n\n\nFor vanilla Python, the overflow errors are checked and more digits are allocated when needed, at the cost of being slow.\n\n2**63 * 1000\n\n9223372036854775808000\n\n\nThis number is 1000 times largger than the prior number, but still displayed perfectly without any overflows\n\n\n4.6.2 Floating Number\nStandard double-precision floating point number uses 64 bits. Among them, 1 is for sign, 11 is for exponent, and 52 are fraction significand, See https://en.wikipedia.org/wiki/Double-precision_floating-point_format. The bottom line is that, of course, not every real number is exactly representable.\n\n0.1 + 0.1 + 0.1 == 0.3\n\nFalse\n\n\n\n0.3 - 0.2 == 0.1\n\nFalse\n\n\nWhat is really going on?\n\nimport decimal\ndecimal.Decimal(0.1)\n\nDecimal('0.1000000000000000055511151231257827021181583404541015625')\n\n\nBecause the mantissa bits are limited, it can not represent a floating point that’s both very big and very precise. Most computers can represent all integers up to \\(2^{53}\\), after that it starts skipping numbers.\n\n2.1**53 + 1 == 2.1**53\n\n# Find a number larger than 2 to the 53rd\n\nTrue\n\n\n\nx = 2.1**53\nfor i in range(1000000):\n    x = x + 1\nx == 2.1**53\n\nTrue\n\n\nWe add 1 to x by 1000000 times, but it still equal to its initial value, 2.1**53. This is because this number is too big that computer can’t handle it with precision like add 1.\nMachine epsilon is the smallest positive floating-point number x such that 1 + x != 1.\n\nprint(np.finfo(float).eps)\nprint(np.finfo(np.float32).eps)\n\n2.220446049250313e-16\n1.1920929e-07"
  },
  {
    "objectID": "pandas.html#pandas-2.0-presentation",
    "href": "pandas.html#pandas-2.0-presentation",
    "title": "5  Data Manipulation with Pandas",
    "section": "5.1 Pandas 2.0 Presentation",
    "text": "5.1 Pandas 2.0 Presentation\nGinamarie Mastrorilli\nTopics Covered: - Combining Datasets: Merge and Join - Aggregation and Grouping - Pivot Tables - Vectorized String Operations - Working with Time Series - High-Performance Pandas: eval()\nUse the NYC Motor Vehicle Collisions Data - Data from Jan 01 2023 12:00 AM to Jan 31 2023 11:45 PM\n\nimport pandas as pd\nimport numpy as np\n\n\njan23 = pd.read_csv(\"data/nyc_crashes_202301.csv\") \njan23.head()\n\n\n\n\n\n  \n    \n      \n      CRASH DATE\n      CRASH TIME\n      BOROUGH\n      ZIP CODE\n      LATITUDE\n      LONGITUDE\n      LOCATION\n      ON STREET NAME\n      CROSS STREET NAME\n      OFF STREET NAME\n      ...\n      CONTRIBUTING FACTOR VEHICLE 2\n      CONTRIBUTING FACTOR VEHICLE 3\n      CONTRIBUTING FACTOR VEHICLE 4\n      CONTRIBUTING FACTOR VEHICLE 5\n      COLLISION_ID\n      VEHICLE TYPE CODE 1\n      VEHICLE TYPE CODE 2\n      VEHICLE TYPE CODE 3\n      VEHICLE TYPE CODE 4\n      VEHICLE TYPE CODE 5\n    \n  \n  \n    \n      0\n      01/01/2023\n      14:38\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      BROOKLYN QUEENS EXPRESSWAY RAMP\n      NaN\n      NaN\n      ...\n      Driver Inattention/Distraction\n      Driver Inattention/Distraction\n      NaN\n      NaN\n      4594563\n      Sedan\n      Sedan\n      Sedan\n      NaN\n      NaN\n    \n    \n      1\n      01/01/2023\n      8:04\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NASSAU EXPRESSWAY\n      NaN\n      NaN\n      ...\n      Unspecified\n      NaN\n      NaN\n      NaN\n      4594599\n      Sedan\n      Sedan\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      01/01/2023\n      18:05\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      10 AVENUE\n      11 AVENUE\n      NaN\n      ...\n      Unspecified\n      NaN\n      NaN\n      NaN\n      4594810\n      Sedan\n      Taxi\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      01/01/2023\n      23:45\n      NaN\n      NaN\n      40.769737\n      -73.91244\n      (40.769737, -73.91244)\n      ASTORIA BOULEVARD\n      37 STREET\n      NaN\n      ...\n      Driver Inattention/Distraction\n      NaN\n      NaN\n      NaN\n      4594595\n      Taxi\n      Taxi\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      01/01/2023\n      4:50\n      NaN\n      NaN\n      40.830555\n      -73.85072\n      (40.830555, -73.85072)\n      CASTLE HILL AVENUE\n      EAST 177 STREET\n      NaN\n      ...\n      Unspecified\n      NaN\n      NaN\n      NaN\n      4594761\n      Station Wagon/Sport Utility Vehicle\n      Sedan\n      NaN\n      NaN\n      NaN\n    \n  \n\n5 rows × 29 columns\n\n\n\n\njan23[\"CONTRIBUTING FACTOR VEHICLE 1\"] = (\n    jan23[\"CONTRIBUTING FACTOR VEHICLE 1\"].replace([\"Unspecified\"], np.nan))\njan23[\"CONTRIBUTING FACTOR VEHICLE 2\"] = (\n    jan23[\"CONTRIBUTING FACTOR VEHICLE 2\"].replace([\"Unspecified\"], np.nan))\njan23[\"CONTRIBUTING FACTOR VEHICLE 3\"] = (\n    jan23[\"CONTRIBUTING FACTOR VEHICLE 3\"].replace([\"Unspecified\"], np.nan))\njan23[\"CONTRIBUTING FACTOR VEHICLE 4\"] = (\n    jan23[\"CONTRIBUTING FACTOR VEHICLE 4\"].replace([\"Unspecified\"], np.nan))\njan23[\"CONTRIBUTING FACTOR VEHICLE 5\"] = (\n    jan23[\"CONTRIBUTING FACTOR VEHICLE 5\"].replace([\"Unspecified\"], np.nan))\njan23[\"LATITUDE\"] = jan23[\"LATITUDE\"].replace([0.0], np.nan)\njan23[\"LONGITUDE\"] = jan23[\"LONGITUDE\"].replace([0.0], np.nan)\njan23.describe()\n\n\n\n\n\n  \n    \n      \n      ZIP CODE\n      LATITUDE\n      LONGITUDE\n      NUMBER OF PERSONS INJURED\n      NUMBER OF PERSONS KILLED\n      NUMBER OF PEDESTRIANS INJURED\n      NUMBER OF PEDESTRIANS KILLED\n      NUMBER OF CYCLIST INJURED\n      NUMBER OF CYCLIST KILLED\n      NUMBER OF MOTORIST INJURED\n      NUMBER OF MOTORIST KILLED\n      COLLISION_ID\n    \n  \n  \n    \n      count\n      4796.000000\n      6683.000000\n      6683.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7.244000e+03\n    \n    \n      mean\n      10893.521685\n      40.722317\n      -73.918590\n      0.502761\n      0.002347\n      0.116372\n      0.000690\n      0.033269\n      0.000414\n      0.333103\n      0.001242\n      4.599022e+06\n    \n    \n      std\n      526.392428\n      0.081602\n      0.085654\n      0.813641\n      0.051164\n      0.397927\n      0.026265\n      0.180118\n      0.020348\n      0.749174\n      0.038951\n      2.365885e+03\n    \n    \n      min\n      10001.000000\n      40.504658\n      -74.250150\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      4.594332e+06\n    \n    \n      25%\n      10457.000000\n      40.665350\n      -73.965530\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      4.597113e+06\n    \n    \n      50%\n      11208.000000\n      40.713196\n      -73.922740\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      4.599058e+06\n    \n    \n      75%\n      11239.000000\n      40.777754\n      -73.867714\n      1.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      4.600953e+06\n    \n    \n      max\n      11694.000000\n      40.912827\n      -73.702095\n      21.000000\n      2.000000\n      19.000000\n      1.000000\n      2.000000\n      1.000000\n      8.000000\n      2.000000\n      4.605324e+06"
  },
  {
    "objectID": "pandas.html#combining-datasets-merge-and-join",
    "href": "pandas.html#combining-datasets-merge-and-join",
    "title": "5  Data Manipulation with Pandas",
    "section": "5.2 Combining Datasets: Merge and Join",
    "text": "5.2 Combining Datasets: Merge and Join\n\npd.merge( ) : allows a user to do one-to-one, one-to-many, and many-to-many joins\n\nOne-to-one joins mean each row is related on a single row in a different table using a key column\nOne-to-many joins mean each row in one table in related to one or more rows in a different table using a key column\nMany-to-many joins mean one or more rows in one table is related to one or more rows in a seperate table using a key column\n\n\n\n5.2.1 Ex: One-to One Join\n\nCreate 2 new Data Frames from the January 2023 data with a common column (Zip Code)\n\nFor this example, we are using .drop_duplicates() to get unique ZIP CODE values\n\nUsing pd.merge( ) the two data frames are combined using that common column as a key\n\n\ncrash_zip = jan23[[\"CRASH DATE\", \"ZIP CODE\"]].copy().drop_duplicates(subset = [ \"ZIP CODE\"])\ncrash_zip.tail() # to view DF of crash date & zip code\n\n\n\n\n\n  \n    \n      \n      CRASH DATE\n      ZIP CODE\n    \n  \n  \n    \n      3590\n      01/16/2023\n      10023.0\n    \n    \n      4304\n      01/19/2023\n      11363.0\n    \n    \n      5530\n      01/24/2023\n      11109.0\n    \n    \n      5732\n      01/25/2023\n      10280.0\n    \n    \n      7140\n      01/31/2023\n      10169.0\n    \n  \n\n\n\n\n\nborough_zip = jan23[[\"ZIP CODE\", \"BOROUGH\"]].copy().drop_duplicates(subset = [ \"ZIP CODE\"])\nborough_zip.tail() # to view DF of zip code and borough\n\n\n\n\n\n  \n    \n      \n      ZIP CODE\n      BOROUGH\n    \n  \n  \n    \n      3590\n      10023.0\n      MANHATTAN\n    \n    \n      4304\n      11363.0\n      QUEENS\n    \n    \n      5530\n      11109.0\n      QUEENS\n    \n    \n      5732\n      10280.0\n      MANHATTAN\n    \n    \n      7140\n      10169.0\n      MANHATTAN\n    \n  \n\n\n\n\n\nmerge_w_zip = pd.merge(crash_zip, borough_zip)\nmerge_w_zip.tail() # to view joined data frames\n\n\n\n\n\n  \n    \n      \n      CRASH DATE\n      ZIP CODE\n      BOROUGH\n    \n  \n  \n    \n      176\n      01/16/2023\n      10023.0\n      MANHATTAN\n    \n    \n      177\n      01/19/2023\n      11363.0\n      QUEENS\n    \n    \n      178\n      01/24/2023\n      11109.0\n      QUEENS\n    \n    \n      179\n      01/25/2023\n      10280.0\n      MANHATTAN\n    \n    \n      180\n      01/31/2023\n      10169.0\n      MANHATTAN\n    \n  \n\n\n\n\n\n\n5.2.2 We can specify the name of the key column using ‘on’\n\npd.merge(crash_zip, borough_zip, on = 'ZIP CODE').tail()\n\n\n\n\n\n  \n    \n      \n      CRASH DATE\n      ZIP CODE\n      BOROUGH\n    \n  \n  \n    \n      176\n      01/16/2023\n      10023.0\n      MANHATTAN\n    \n    \n      177\n      01/19/2023\n      11363.0\n      QUEENS\n    \n    \n      178\n      01/24/2023\n      11109.0\n      QUEENS\n    \n    \n      179\n      01/25/2023\n      10280.0\n      MANHATTAN\n    \n    \n      180\n      01/31/2023\n      10169.0\n      MANHATTAN\n    \n  \n\n\n\n\n\n\n5.2.3 Ex: One-to-Many Join\n\nCreate a new DataFrame consisting of Employee and Department\nCreate a new DataFrame consisting of Employee and Hire Year\n\n\nemp_dept = pd.DataFrame({'employee': ['Emily', 'Jake', 'Paul', 'Jackie'],\n                        'department': ['Accounting', 'HR', 'Engineering', 'Accounting']})\n\nemp_hire = pd.DataFrame({'employee': ['Emily', 'Jake', 'Paul', 'Jackie'],\n                        'hire_year': ['2020', '2019', '2023', '2011']})\n\n# combining to create a one to one join\ndept_hire = pd.merge(emp_dept,emp_hire) # to merge Employee, Department and Hire Year into one dataframe\ndept_hire\n\n\n\n\n\n  \n    \n      \n      employee\n      department\n      hire_year\n    \n  \n  \n    \n      0\n      Emily\n      Accounting\n      2020\n    \n    \n      1\n      Jake\n      HR\n      2019\n    \n    \n      2\n      Paul\n      Engineering\n      2023\n    \n    \n      3\n      Jackie\n      Accounting\n      2011\n    \n  \n\n\n\n\n\nCreate a new DataFrame that consists of Department and the Supervisor for that department\nMerge this new DataFrame with ‘dept_hire’ to create a Many-to-One join using Department as a key\n\n\ndept_sup = pd.DataFrame({'supervisor': ['Lily', 'Angela', 'Steven'],\n                        'department': ['Accounting', 'HR', 'Engineering']})\n\nsup_emp_dept_hire = pd.merge(dept_sup,dept_hire) # merge using Department as the key\nsup_emp_dept_hire\n\n\n\n\n\n  \n    \n      \n      supervisor\n      department\n      employee\n      hire_year\n    \n  \n  \n    \n      0\n      Lily\n      Accounting\n      Emily\n      2020\n    \n    \n      1\n      Lily\n      Accounting\n      Jackie\n      2011\n    \n    \n      2\n      Angela\n      HR\n      Jake\n      2019\n    \n    \n      3\n      Steven\n      Engineering\n      Paul\n      2023\n    \n  \n\n\n\n\n\n\n5.2.4 Ex: Merging when the Key has Different Variable Names\n\nWe will merge two dataframes that have a similar column containing the same information, but are named differently\nUsing the employee data from above, but changing ‘employee’ in ‘emp_hire’ to ‘employee_name’\nNeed to drop either ‘employee’ or ‘employee_name’ after merging to not have redundant information\n\n\nemp_dept_names = pd.DataFrame({'employee': ['Emily', 'Jake', 'Paul', 'Jackie'],\n                        'department': ['Accounting', 'HR', 'Engineering', 'Accounting']})\n\nemp_hire_names = pd.DataFrame({'employee_name': ['Emily', 'Jake', 'Paul', 'Jackie'],\n                        'hire_year': ['2020', '2019', '2023', '2011']})\n\n# to merge Employee, Department and Hire Year into one dataframe & drop column 'employee_name'\ndept_hire_names = pd.merge(emp_dept_names,emp_hire_names, left_on = 'employee', \n                           right_on = 'employee_name').drop('employee_name', axis = 1) \n\ndept_hire_names\n\n\n\n\n\n  \n    \n      \n      employee\n      department\n      hire_year\n    \n  \n  \n    \n      0\n      Emily\n      Accounting\n      2020\n    \n    \n      1\n      Jake\n      HR\n      2019\n    \n    \n      2\n      Paul\n      Engineering\n      2023\n    \n    \n      3\n      Jackie\n      Accounting\n      2011\n    \n  \n\n\n\n\n\n\n5.2.5 Ex: Joining ‘uszipcode’\n\nCreate a subset of jan23 data with 7 zipcodes\nUsing ‘uszipcode’ data to join the zip codes from jan23 with data provided in this package\n\n\nfrom uszipcode import SearchEngine\n\n\nsearch = SearchEngine()\n\n# create a DF of zip codes from jan23 & convert to integers\nzipcodes = pd.DataFrame(jan23[\"ZIP CODE\"].tail(15).dropna().reset_index(drop = True))\nzipcodes[\"ZIP CODE\"] = zipcodes[\"ZIP CODE\"].astype(int)\n\n# create new,empty column in the df to store the address information\nzipcodes['Address'] = None\n\n\n# using uszipcode library to retreive address info\nfor index, row in zipcodes.iterrows():\n    result = search.by_zipcode(row['ZIP CODE'])\n    zipcodes.at[index, 'Address'] = result.major_city + ', ' + result.state\n\nprint(zipcodes)\n\n    ZIP CODE       Address\n0      11228  Brooklyn, NY\n1      10027  New York, NY\n2      10040  New York, NY\n3      10035  New York, NY\n4      10035  New York, NY\n5      10457     Bronx, NY\n6      11203  Brooklyn, NY\n7      11208  Brooklyn, NY\n8      11230  Brooklyn, NY\n9      10033  New York, NY\n10     11435   Jamaica, NY"
  },
  {
    "objectID": "pandas.html#aggregation-and-grouping",
    "href": "pandas.html#aggregation-and-grouping",
    "title": "5  Data Manipulation with Pandas",
    "section": "5.3 Aggregation and Grouping",
    "text": "5.3 Aggregation and Grouping\nBuilt-In Pandas Aggregations (for DataFrame & Series objects): - count( ) - Total number of items - first( ), last( ) - First and last item - mean( ), median( )\n- Mean and median - min( ), max( )\n- Minimum and maximum - std( ), var( )\n- Standard deviation and variance - mad( )\n- Mean absolute deviation - prod( )\n- Product of all items - sum( )\n- Sum of all items - groupby() - compute aggregates on subsets of data\n\n5.3.1 Ex: Titanic Groupby\n\nWe will use the Titanic Data Set from the ‘seaborn’ library\n\n\nimport seaborn as sns\ntitanic = sns.load_dataset('titanic')\ntitanic.head()\n\n\n\n\n\n  \n    \n      \n      survived\n      pclass\n      sex\n      age\n      sibsp\n      parch\n      fare\n      embarked\n      class\n      who\n      adult_male\n      deck\n      embark_town\n      alive\n      alone\n    \n  \n  \n    \n      0\n      0\n      3\n      male\n      22.0\n      1\n      0\n      7.2500\n      S\n      Third\n      man\n      True\n      NaN\n      Southampton\n      no\n      False\n    \n    \n      1\n      1\n      1\n      female\n      38.0\n      1\n      0\n      71.2833\n      C\n      First\n      woman\n      False\n      C\n      Cherbourg\n      yes\n      False\n    \n    \n      2\n      1\n      3\n      female\n      26.0\n      0\n      0\n      7.9250\n      S\n      Third\n      woman\n      False\n      NaN\n      Southampton\n      yes\n      True\n    \n    \n      3\n      1\n      1\n      female\n      35.0\n      1\n      0\n      53.1000\n      S\n      First\n      woman\n      False\n      C\n      Southampton\n      yes\n      False\n    \n    \n      4\n      0\n      3\n      male\n      35.0\n      0\n      0\n      8.0500\n      S\n      Third\n      man\n      True\n      NaN\n      Southampton\n      no\n      True\n    \n  \n\n\n\n\n\nBelow the data is groupby ‘sex’ and the counts for each row are displayed\n\n\ntitanic.groupby('sex').count()\n\n\n\n\n\n  \n    \n      \n      survived\n      pclass\n      age\n      sibsp\n      parch\n      fare\n      embarked\n      class\n      who\n      adult_male\n      deck\n      embark_town\n      alive\n      alone\n    \n    \n      sex\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      female\n      314\n      314\n      261\n      314\n      314\n      314\n      312\n      314\n      314\n      314\n      97\n      312\n      314\n      314\n    \n    \n      male\n      577\n      577\n      453\n      577\n      577\n      577\n      577\n      577\n      577\n      577\n      106\n      577\n      577\n      577\n    \n  \n\n\n\n\n\n\n5.3.2 Ex: Crash Data Group By\n\nUsing the crash_zip DataFrame from above, grouping by ZIP CODE and using the count() method, we can see how many counts for each listed zip code\n\n\ncrash_zip.groupby('ZIP CODE').count()\n\n\n\n\n\n  \n    \n      \n      CRASH DATE\n    \n    \n      ZIP CODE\n      \n    \n  \n  \n    \n      10001.0\n      1\n    \n    \n      10002.0\n      1\n    \n    \n      10003.0\n      1\n    \n    \n      10004.0\n      1\n    \n    \n      10005.0\n      1\n    \n    \n      ...\n      ...\n    \n    \n      11436.0\n      1\n    \n    \n      11691.0\n      1\n    \n    \n      11692.0\n      1\n    \n    \n      11693.0\n      1\n    \n    \n      11694.0\n      1\n    \n  \n\n180 rows × 1 columns"
  },
  {
    "objectID": "pandas.html#pivot-tables",
    "href": "pandas.html#pivot-tables",
    "title": "5  Data Manipulation with Pandas",
    "section": "5.4 Pivot Tables",
    "text": "5.4 Pivot Tables\n\nCreates a two dimensional table using column data\nEasy way to visualize data to see patterns and summarize data\nUse ‘groupby( )’ to create a pivot table\n\n\n5.4.1 Ex: Pivot Table using Titanic Data Set\n\nWe will again use the Titanic data set, but now we will create a pivot table\n\nWe can use groupby() to help create a pivot table - Group the data by ‘sex’ and ‘class’ to select survival. - Then use the aggregate function mean() to show within the table\n\ntitanic.groupby(['sex', 'class'])['survived'].aggregate('mean').unstack()\n\n\n\n\n\n  \n    \n      class\n      First\n      Second\n      Third\n    \n    \n      sex\n      \n      \n      \n    \n  \n  \n    \n      female\n      0.968085\n      0.921053\n      0.500000\n    \n    \n      male\n      0.368852\n      0.157407\n      0.135447\n    \n  \n\n\n\n\n\n\n5.4.2 Ex: Pivot Table of Crash Data\n\nCreate a pivot table that shows the Number of Persons Injured for every Crash Date per Zip Code\n\n\njan23.pivot_table('NUMBER OF PERSONS INJURED', index = 'ZIP CODE', columns = 'CRASH DATE')\n\n\n\n\n\n  \n    \n      CRASH DATE\n      01/01/2023\n      01/02/2023\n      01/03/2023\n      01/04/2023\n      01/05/2023\n      01/06/2023\n      01/07/2023\n      01/08/2023\n      01/09/2023\n      01/10/2023\n      ...\n      01/22/2023\n      01/23/2023\n      01/24/2023\n      01/25/2023\n      01/26/2023\n      01/27/2023\n      01/28/2023\n      01/29/2023\n      01/30/2023\n      01/31/2023\n    \n    \n      ZIP CODE\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      10001.0\n      0.0\n      NaN\n      0.000000\n      NaN\n      0.0\n      NaN\n      NaN\n      0.0\n      0.0\n      NaN\n      ...\n      NaN\n      NaN\n      0.0\n      0.500000\n      0.0\n      0.0\n      1.00\n      1.0\n      0.500000\n      0.5\n    \n    \n      10002.0\n      0.0\n      0.5\n      0.666667\n      0.0\n      NaN\n      NaN\n      NaN\n      0.0\n      NaN\n      0.0\n      ...\n      0.5\n      NaN\n      0.0\n      1.333333\n      0.0\n      0.0\n      0.75\n      0.0\n      0.333333\n      1.0\n    \n    \n      10003.0\n      0.5\n      0.0\n      1.000000\n      1.0\n      NaN\n      NaN\n      0.5\n      0.0\n      0.0\n      NaN\n      ...\n      0.0\n      NaN\n      NaN\n      NaN\n      0.0\n      0.0\n      0.50\n      0.0\n      0.000000\n      1.0\n    \n    \n      10004.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      0.0\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      0.00\n      NaN\n      NaN\n      NaN\n    \n    \n      10005.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      1.0\n      NaN\n      NaN\n      NaN\n      0.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      11436.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      0.000000\n      NaN\n      NaN\n      0.00\n      NaN\n      0.000000\n      NaN\n    \n    \n      11691.0\n      0.0\n      0.5\n      1.000000\n      1.0\n      1.0\n      NaN\n      NaN\n      0.0\n      NaN\n      0.0\n      ...\n      0.5\n      1.0\n      0.0\n      0.000000\n      0.0\n      0.0\n      0.50\n      NaN\n      0.000000\n      0.0\n    \n    \n      11692.0\n      NaN\n      NaN\n      1.000000\n      1.0\n      0.0\n      1.0\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      0.0\n      NaN\n      NaN\n      NaN\n      NaN\n      0.0\n    \n    \n      11693.0\n      1.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      0.0\n      0.000000\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      11694.0\n      0.0\n      NaN\n      NaN\n      0.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      2.0\n      1.0\n      NaN\n      1.000000\n      0.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n180 rows × 31 columns"
  },
  {
    "objectID": "pandas.html#vectorized-string-operations",
    "href": "pandas.html#vectorized-string-operations",
    "title": "5  Data Manipulation with Pandas",
    "section": "5.5 Vectorized String Operations",
    "text": "5.5 Vectorized String Operations\n\nMust use the ‘str’ attribute of a Pandas Series and Index objects to access operations\nSome examples of operations:\n\nlen()\nlower()\n\ntranslate()\n\nslower()\nljust()\n\nupper()\n\nstartswith()\n\nisupper()\nrjust()\n\nfind()\n\nendswith()\n\nisnumeric()\ncapitalize()\n\nswapcase()\n\nistitle()\nrpartition()\n\n\n\n5.5.1 Ex: Create a Panda Series of ‘BOROUGH’ names and swap cases.\n\nbname = pd.Series(jan23['BOROUGH'])\nbname.head(15).dropna() # original \n\n5      BROOKLYN\n6        QUEENS\n7     MANHATTAN\n11       QUEENS\n12     BROOKLYN\n13       QUEENS\n14       QUEENS\nName: BOROUGH, dtype: object\n\n\n\nbname.str.capitalize().head(15).dropna()# to make first letter capital\n\n5      Brooklyn\n6        Queens\n7     Manhattan\n11       Queens\n12     Brooklyn\n13       Queens\n14       Queens\nName: BOROUGH, dtype: object\n\n\n\nbname.str.swapcase().head(15).dropna() # to make all lower case\n\n5      brooklyn\n6        queens\n7     manhattan\n11       queens\n12     brooklyn\n13       queens\n14       queens\nName: BOROUGH, dtype: object\n\n\n\nbname.str.len().head(15).dropna() # to return the length of the name and data type\n\n5     8.0\n6     6.0\n7     9.0\n11    6.0\n12    8.0\n13    6.0\n14    6.0\nName: BOROUGH, dtype: float64\n\n\n\nbname.str.startswith('B').head(15).dropna() # to see if starts with a letter B\n\n5      True\n6     False\n7     False\n11    False\n12     True\n13    False\n14    False\nName: BOROUGH, dtype: object"
  },
  {
    "objectID": "pandas.html#time-series",
    "href": "pandas.html#time-series",
    "title": "5  Data Manipulation with Pandas",
    "section": "5.6 Time Series",
    "text": "5.6 Time Series\n\nTime Stamps : Moments in time\n\nEx: July 4th, 2023 at 8:00 AM\nPandas provides the Timestamp type\n\nTime Intervals: Reference a length of time with a beginning and end\n\nEx: The year of 2022\nPandas provides the Period type\n\nTime Deltas/ Durations: Reference an exact length of time\n\nEx: 0.3 seconds\nPandas provides the Timedelta type\n\n\n\n5.6.0.1 Can create a Timestamp object\n\ncombines ‘datetime’ and ‘dateutil’ to be used as a Series or DataFrame\n\n\ndate = pd.to_datetime(\"2nd of February, 2023\")\ndate\ntype(date)\n\npandas._libs.tslibs.timestamps.Timestamp\n\n\n\n\n5.6.0.2 Can create Series that has time indexed data\n\nind = pd.DatetimeIndex(['2022-07-04', '2022-08-04',\n                          '2022-07-04', '2022-08-04'])\ninddata = pd.Series([0,1,2,3], index = ind)\ninddata\n\n2022-07-04    0\n2022-08-04    1\n2022-07-04    2\n2022-08-04    3\ndtype: int64\n\n\n\n\n5.6.0.3 Frequencies and Offsets\nThe following are the main codes avaiable:\n- D Calendar day    \n- B Business day\n- W Weekly      \n- M Month end   \n- BM   Business month end\n- Q Quarter end \n- BQ   Business quarter end\n- A Year end    \n- BA   Business year end\n- H Hours   \n- BH   Business hours\n- T Minutes     \n- S Seconds     \n- L Milliseonds     \n- U Microseconds        \n- N nanoseconds \n\n\n5.6.1 Ex: TimeDelta\n\ncreate a TimeDelta data type starting at 00:00:00 using frequency of 2 hours and 30 minutes (2H30T) over 5 periods.\n\n\npd.timedelta_range(0, periods = 5, freq = \"2H30T\")\n\nTimedeltaIndex(['0 days 00:00:00', '0 days 02:30:00', '0 days 05:00:00',\n                '0 days 07:30:00', '0 days 10:00:00'],\n               dtype='timedelta64[ns]', freq='150T')"
  },
  {
    "objectID": "pandas.html#high-performance-pandas-eval",
    "href": "pandas.html#high-performance-pandas-eval",
    "title": "5  Data Manipulation with Pandas",
    "section": "5.7 High Performance Pandas: eval()",
    "text": "5.7 High Performance Pandas: eval()\n\neval( ) uses string expressions to compute operations using DataFrames\n\nsupports all arithmetic operations, comparison operators, bitwise operators ( & , | ), and the use of ‘and’ and ‘or’ in Boolean expressions\n\n\n\nnrows, ncols = 10, 5 # creating 2 DF of 5 rows and 10 columns\nrand = np.random.RandomState(7)\ndfa, dfb = (pd.DataFrame(rand.rand(nrows, ncols))\n            for i in range (2))\n\n# to compute sum of dfa and dfb and place into one table\n\n\nprint(\"dfa\",dfa)\nprint(\"dfb\", dfb)\npd.eval('dfa + dfb')\n\ndfa           0         1         2         3         4\n0  0.076308  0.779919  0.438409  0.723465  0.977990\n1  0.538496  0.501120  0.072051  0.268439  0.499883\n2  0.679230  0.803739  0.380941  0.065936  0.288146\n3  0.909594  0.213385  0.452124  0.931206  0.024899\n4  0.600549  0.950130  0.230303  0.548490  0.909128\n5  0.133169  0.523413  0.750410  0.669013  0.467753\n6  0.204849  0.490766  0.372385  0.477401  0.365890\n7  0.837918  0.768648  0.313995  0.572625  0.276049\n8  0.452843  0.352978  0.657399  0.370351  0.459093\n9  0.719324  0.412992  0.906423  0.180452  0.741119\ndfb           0         1         2         3         4\n0  0.422374  0.426454  0.634380  0.522906  0.414886\n1  0.001427  0.092262  0.709394  0.524346  0.696160\n2  0.955468  0.682914  0.053129  0.308853  0.592595\n3  0.235120  0.964971  0.945048  0.848401  0.472324\n4  0.841477  0.131111  0.308734  0.462996  0.741847\n5  0.485825  0.136876  0.343537  0.324426  0.300419\n6  0.165501  0.414902  0.448121  0.774900  0.796391\n7  0.522390  0.460630  0.778214  0.887289  0.674919\n8  0.800479  0.939111  0.040656  0.875672  0.276563\n9  0.475764  0.796761  0.717242  0.147148  0.658748\n\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n    \n  \n  \n    \n      0\n      0.498682\n      1.206372\n      1.072789\n      1.246371\n      1.392875\n    \n    \n      1\n      0.539923\n      0.593383\n      0.781446\n      0.792785\n      1.196043\n    \n    \n      2\n      1.634698\n      1.486653\n      0.434070\n      0.374789\n      0.880740\n    \n    \n      3\n      1.144714\n      1.178356\n      1.397172\n      1.779607\n      0.497223\n    \n    \n      4\n      1.442026\n      1.081240\n      0.539037\n      1.011486\n      1.650976\n    \n    \n      5\n      0.618995\n      0.660289\n      1.093946\n      0.993439\n      0.768172\n    \n    \n      6\n      0.370350\n      0.905668\n      0.820505\n      1.252302\n      1.162281\n    \n    \n      7\n      1.360308\n      1.229278\n      1.092208\n      1.459914\n      0.950968\n    \n    \n      8\n      1.253322\n      1.292090\n      0.698055\n      1.246023\n      0.735656\n    \n    \n      9\n      1.195089\n      1.209753\n      1.623666\n      0.327599\n      1.399867"
  },
  {
    "objectID": "stats.html#tests-for-exploratory-data-analysis",
    "href": "stats.html#tests-for-exploratory-data-analysis",
    "title": "6  Statistical Tests and Models",
    "section": "6.1 Tests for Exploratory Data Analysis",
    "text": "6.1 Tests for Exploratory Data Analysis\nA collection of functions are available from scipy.stats.\n\nComparing the locations of two samples\n\nttest_ind: t-test for two independent samples\nttest_rel: t-test for paired samples\nranksums: Wilcoxon rank-sum test for two independent samples\nwilcoxon: Wilcoxon signed-rank test for paired samples\n\nComparing the locations of multiple samples\n\nf_oneway: one-way ANOVA\nkruskal: Kruskal-Wallis H-test\n\nTests for associations in contigency tables\n\nchi2_contingency: Chi-square test of independence of variables\nfisher_exact: Fisher exact test on a 2x2 contingency table\n\nGoodness of fit\n\ngoodness_of_fit: distribution could contain unspecified parameters\nanderson: Anderson-Darling test\nkstest: Kolmogorov-Smirnov test\nchisquare: one-way chi-square test\nnormaltest: test for normality\n\n\nSince R has a richer collections of statistical functions, we can call R function from Python with rpy2. See, for example, a blog on this subject.\nFor example, fisher_exact can only handle 2x2 contingency tables. For contingency tables larger than 2x2, we can call fisher.text() from R through rpy2. See this StackOverflow post. Note that the . in function names and arguments are replaced with _.\n\nimport pandas as pd\nimport numpy as np\nimport rpy2.robjects.numpy2ri\nfrom rpy2.robjects.packages import importr\nrpy2.robjects.numpy2ri.activate()\n\nstats = importr('stats')\n\njan23 = pd.read_csv(\"data/nyc_crashes_202301.csv\")\njan23[\"injury\"] = np.where(jan23[\"NUMBER OF PERSONS INJURED\"] > 0, 1, 0)\nm = pd.crosstab(jan23[\"injury\"], jan23[\"BOROUGH\"])\n\nres = stats.fisher_test(m.to_numpy(), simulate_p_value = True)\nres[0][0]\n\nLoading custom .Rprofile\n\n\n0.09695152423788106"
  },
  {
    "objectID": "stats.html#linear-model",
    "href": "stats.html#linear-model",
    "title": "6  Statistical Tests and Models",
    "section": "6.2 Linear Model",
    "text": "6.2 Linear Model\nLet’s simulate some data for illustrations.\n\nimport numpy as np\n\nnobs = 200\nncov = 5\nnp.random.seed(123)\nx = np.random.random((nobs, ncov)) # Uniform over [0, 1)\nbeta = np.repeat(1, ncov)\ny = 2 + np.dot(x, beta) + np.random.normal(size = nobs)\n\nCheck the shape of y:\n\ny.shape\n\n(200,)\n\n\nCheck the shape of x:\n\nx.shape\n\n(200, 5)\n\n\nThat is, the true linear regression model is \\[\ny = 2 + x_1 + x_2 + x_3 + x_4 + x_5 + \\epsilon.\n\\]\nA regression model for the observed data can be fitted as\n\nimport statsmodels.api as sma\nxmat = sma.add_constant(x)\nmymod = sma.OLS(y, xmat)\nmyfit = mymod.fit()\nmyfit.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:            y          R-squared:             0.309\n\n\n  Model:                   OLS         Adj. R-squared:        0.292\n\n\n  Method:             Least Squares    F-statistic:           17.38\n\n\n  Date:             Wed, 22 Feb 2023   Prob (F-statistic): 3.31e-14\n\n\n  Time:                 18:44:31       Log-Likelihood:      -272.91\n\n\n  No. Observations:         200        AIC:                   557.8\n\n\n  Df Residuals:             194        BIC:                   577.6\n\n\n  Df Model:                   5                                    \n\n\n  Covariance Type:      nonrobust                                  \n\n\n\n\n           coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  const     1.8754     0.282     6.656  0.000     1.320     2.431\n\n\n  x1        1.1703     0.248     4.723  0.000     0.682     1.659\n\n\n  x2        0.8988     0.235     3.825  0.000     0.435     1.362\n\n\n  x3        0.9784     0.238     4.114  0.000     0.509     1.448\n\n\n  x4        1.3418     0.250     5.367  0.000     0.849     1.835\n\n\n  x5        0.6027     0.239     2.519  0.013     0.131     1.075\n\n\n\n\n  Omnibus:        0.810   Durbin-Watson:         1.978\n\n\n  Prob(Omnibus):  0.667   Jarque-Bera (JB):      0.903\n\n\n  Skew:          -0.144   Prob(JB):              0.637\n\n\n  Kurtosis:       2.839   Cond. No.               8.31\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nQuestions to review:\n\nHow are the regression coefficients interpreted? Intercept?\nWhy does it make sense to center the covariates?\n\nNow we form a data frame with the variables\n\nimport pandas as pd\ndf = np.concatenate((y.reshape((nobs, 1)), x), axis = 1)\ndf = pd.DataFrame(data = df,\n                  columns = [\"y\"] + [\"x\" + str(i) for i in range(1,\n                  ncov + 1)])\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 200 entries, 0 to 199\nData columns (total 6 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   y       200 non-null    float64\n 1   x1      200 non-null    float64\n 2   x2      200 non-null    float64\n 3   x3      200 non-null    float64\n 4   x4      200 non-null    float64\n 5   x5      200 non-null    float64\ndtypes: float64(6)\nmemory usage: 9.5 KB\n\n\nLet’s use a formula to specify the regression model as in R, and fit a robust linear model (rlm) instead of OLS. Note that the model specification and the function interface is similar to R.\n\nimport statsmodels.formula.api as smf\nmymod = smf.rlm(formula = \"y ~ x1 + x2 + x3 + x4 + x5\", data = df)\nmyfit = mymod.fit()\nmyfit.summary()\n\n\n\nRobust linear Model Regression Results\n\n  Dep. Variable:          y          No. Observations:      200\n\n\n  Model:                 RLM         Df Residuals:          194\n\n\n  Method:               IRLS         Df Model:                5\n\n\n  Norm:                HuberT                                  \n\n\n  Scale Est.:            mad                                   \n\n\n  Cov Type:              H1                                    \n\n\n  Date:           Wed, 22 Feb 2023                             \n\n\n  Time:               18:44:31                                 \n\n\n  No. Iterations:        16                                    \n\n\n\n\n               coef     std err      z      P>|z|  [0.025    0.975]  \n\n\n  Intercept     1.8353     0.294     6.246  0.000     1.259     2.411\n\n\n  x1            1.1254     0.258     4.355  0.000     0.619     1.632\n\n\n  x2            0.9664     0.245     3.944  0.000     0.486     1.447\n\n\n  x3            0.9995     0.248     4.029  0.000     0.513     1.486\n\n\n  x4            1.3275     0.261     5.091  0.000     0.816     1.839\n\n\n  x5            0.6768     0.250     2.712  0.007     0.188     1.166\n\nIf the model instance has been used for another fit with different fit parameters, then the fit options might not be the correct ones anymore .\n\n\nFor model diagnostics, one can check residual plots.\n\nimport matplotlib.pyplot as plt\n\nmyOlsFit = smf.ols(formula = \"y ~ x1 + x2 + x3 + x4 + x5\", data = df).fit()\nfig = plt.figure(figsize = (6, 6))\n## residual versus x1; can do the same for other covariates\nfig = sma.graphics.plot_regress_exog(myOlsFit, 'x1', fig=fig)\n\neval_env: 1\n\n\n\n\n\nSee more on residual diagnostics and specification tests."
  },
  {
    "objectID": "stats.html#generalized-linear-regression",
    "href": "stats.html#generalized-linear-regression",
    "title": "6  Statistical Tests and Models",
    "section": "6.3 Generalized Linear Regression",
    "text": "6.3 Generalized Linear Regression\nA linear regression model cannot be applied to presence/absence or count data.\nBinary or count data need to be modeled under a generlized framework. Consider a binary or count variable \\(Y\\) with possible covariates \\(X\\). A generalized model describes a transformation \\(g\\) of the conditional mean \\(E[Y | X]\\) by a linear predictor \\(X^{\\top}\\beta\\). That is \\[\ng( E[Y | X] ) = X^{\\top} \\beta.\n\\] The transformation \\(g\\) is known as the link function.\nFor logistic regression with binary outcomes, the link function is the logit function \\[\ng(u) = \\log \\frac{u}{1 - u}, \\quad u \\in (0, 1).\n\\]\nWhat is the interpretation of the regression coefficients in a logistic regression? Intercept?\nA logistic regression can be fit with statsmodels.api.glm.\nLet’s generate some binary data first by dichotomizing existing variables.\n\nimport statsmodels.genmod as smg\neta = x.dot([2, 2, 2, 2, 2]) - 5\np = smg.families.links.Logit().inverse(eta)\ndf[\"yb\"] = np.random.binomial(1, p, p.size)\n\nFit a logistic regression for y1b.\n\nmylogistic = smf.glm(formula = 'yb ~ x1 + x2 + x3 + x4 + x5', data = df,\n                     family = smg.families.Binomial())\nmylfit = mylogistic.fit()\nmylfit.summary()\n\n\n\nGeneralized Linear Model Regression Results\n\n  Dep. Variable:          yb          No. Observations:       200 \n\n\n  Model:                  GLM         Df Residuals:           194 \n\n\n  Model Family:        Binomial       Df Model:                 5 \n\n\n  Link Function:         Logit        Scale:                1.0000\n\n\n  Method:                IRLS         Log-Likelihood:      -110.80\n\n\n  Date:            Wed, 22 Feb 2023   Deviance:             221.61\n\n\n  Time:                18:44:32       Pearson chi2:          196. \n\n\n  No. Iterations:          4          Pseudo R-squ. (CS):  0.2410 \n\n\n  Covariance Type:     nonrobust                                  \n\n\n\n\n               coef     std err      z      P>|z|  [0.025    0.975]  \n\n\n  Intercept    -4.6982     0.820    -5.728  0.000    -6.306    -3.091\n\n\n  x1            2.1654     0.618     3.505  0.000     0.955     3.376\n\n\n  x2            1.1788     0.579     2.035  0.042     0.044     2.314\n\n\n  x3            1.5921     0.587     2.713  0.007     0.442     2.742\n\n\n  x4            2.2310     0.630     3.539  0.000     0.995     3.466\n\n\n  x5            2.5496     0.598     4.263  0.000     1.377     3.722\n\n\n\n\nIf we treat y1b as count data, a Poisson regression can be fitted.\n\nmyPois = smf.glm(formula = 'yb ~ x1 + x2 + x3 + x4 + x5', data = df,\n                 family = smg.families.Poisson())\nmypfit = myPois.fit()\nmypfit.summary()\n\n\n\nGeneralized Linear Model Regression Results\n\n  Dep. Variable:          yb          No. Observations:       200 \n\n\n  Model:                  GLM         Df Residuals:           194 \n\n\n  Model Family:         Poisson       Df Model:                 5 \n\n\n  Link Function:          Log         Scale:                1.0000\n\n\n  Method:                IRLS         Log-Likelihood:      -161.33\n\n\n  Date:            Wed, 22 Feb 2023   Deviance:             112.66\n\n\n  Time:                18:44:32       Pearson chi2:          89.6 \n\n\n  No. Iterations:          5          Pseudo R-squ. (CS):  0.1071 \n\n\n  Covariance Type:     nonrobust                                  \n\n\n\n\n               coef     std err      z      P>|z|  [0.025    0.975]  \n\n\n  Intercept    -2.4529     0.435    -5.644  0.000    -3.305    -1.601\n\n\n  x1            0.7638     0.361     2.117  0.034     0.057     1.471\n\n\n  x2            0.3698     0.330     1.121  0.262    -0.277     1.017\n\n\n  x3            0.5396     0.345     1.562  0.118    -0.137     1.217\n\n\n  x4            0.7895     0.368     2.144  0.032     0.068     1.511\n\n\n  x5            0.9477     0.352     2.691  0.007     0.257     1.638"
  },
  {
    "objectID": "visual.html#matplotlib",
    "href": "visual.html#matplotlib",
    "title": "7  Visualization",
    "section": "7.1 Matplotlib",
    "text": "7.1 Matplotlib\nThe matplotlib library can provide methods in plotting and arranging data visually in order to help viewers understand the main concepts of the data analysis. In this chapter, a progression of graphs will be shown to demonstrate some of the capabilities the library has to graph and plot data.\nThere are several types of graphs that can be used, such as:\n\nScatterplot\nLine plot\n3D plot\n\nThe library can be installed using either pip or conda. For example:\n\n# pip install matplotlib\n\n\n7.1.1 Usage\nLet’s start with a simple scatter plot. We would need to import the libraries as shown. For this example, we use the pyplot submodule, abbreviated to plt. We will use randomly generated data in 3 dimensions (x,y,z).\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(8465);\n\nx = np.random.uniform(0, 3, 10);\ny = np.random.uniform(0, 3, 10);\nz = np.random.uniform(0, 3, 10);\n\nplt.scatter(x, y)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.show()\n\n\n\n\nWe could start plotting another plot, but we have not saved our scatterplot as an object. Thus, it will get overridden by whatever we plot next. If we want to keep a plot, we can save as a figure object. In addition, if we need multiple plots together, we can use a subplot shown as follows.\n\nfigure, (fig1, fig2) = plt.subplots(1, 2, figsize = (8, 6))\n\nfig1.scatter(y, z, marker = '^')\nfig2.scatter(x, y, color = 'red')\n\nplt.show()\n\n\n\n\nWe can also do 3d plots alongside 2d, but we need a different function in order to do so. The following uses 3d axes to plot the scatterplot.\n\nfigure = plt.figure()\n\n# Make 3D axes for fig1\n\nfig2 = figure.add_subplot(122, projection = '3d')\nfig1 = figure.add_subplot(121)\n\n# Plot\n\nfig1.plot(x, z, label = \"Line Graph\")\nfig2.scatter(x, y, z, c = z, cmap = 'cool', label = \"Scatter in 3D\")\nfig1.legend()\nfig2.legend()\n\nplt.show()\n\n\n\n\n\n\n7.1.2 Animation (to be completed)\nAnimations can also be done through matplotlib. This requires the use of the animation submodule which has a variety functions that can be used to plot animations. Inputs required include the frames and other functions needed to update the plots per frame.\n\nimport matplotlib.animation as animation\n\ndef updatept(self):\n    z = 10;\n\nWe can use the FuncAnimation(args, updatept(), frames) to update.\n\n\n7.1.3 Conclusion\nWe have demonstrated some capabilities of the matplotlib library but more complex methods of plotting and arranging visual elements can be found in the documentation."
  },
  {
    "objectID": "visual.html#gg-plot-with-plotnine",
    "href": "visual.html#gg-plot-with-plotnine",
    "title": "7  Visualization",
    "section": "7.2 GG-Plot with Plotnine",
    "text": "7.2 GG-Plot with Plotnine\nThe plotnine package facilitates the creation of highly-informative plots of structured data based on the R implementation of ggplot2. The plotnine package is built on the top of Matplotlib and interacts well with Pandas.\n\n7.2.1 Installation\nWe need to install the package from our command before we start to use it.\nUsing pip:\npip install plotnine        \npip install plotnine[all]  # For the whole package of Plotnine\nOr using conda:\nconda install -c conda-forge plotnine`\n\n\n7.2.2 Import\nNow we can call plotnine in our python code\n\nimport plotnine as p9\nfrom plotnine import *\nfrom plotnine.data import *\n\n\n\n7.2.3 Some fundimental plots via plotnine\nActually there are plenty plots that Plotnine can make, but because of the time limitation we will only introduce these four\n\nBar Chart\nScatter Plot\nHistogram\nBox Plot\n\nExamples will be illustrated with the new york crash dataset, and since the dataset is too large, I will extract the first 50 crashes to do the illustration:\n\nimport pandas as pd\nimport numpy as np\ndf = pd.read_csv('data/nyc_crashes_202301.csv')\ndf1 = df.head(50)\ndf1.head()\n\n\n\n\n\n  \n    \n      \n      CRASH DATE\n      CRASH TIME\n      BOROUGH\n      ZIP CODE\n      LATITUDE\n      LONGITUDE\n      LOCATION\n      ON STREET NAME\n      CROSS STREET NAME\n      OFF STREET NAME\n      ...\n      CONTRIBUTING FACTOR VEHICLE 2\n      CONTRIBUTING FACTOR VEHICLE 3\n      CONTRIBUTING FACTOR VEHICLE 4\n      CONTRIBUTING FACTOR VEHICLE 5\n      COLLISION_ID\n      VEHICLE TYPE CODE 1\n      VEHICLE TYPE CODE 2\n      VEHICLE TYPE CODE 3\n      VEHICLE TYPE CODE 4\n      VEHICLE TYPE CODE 5\n    \n  \n  \n    \n      0\n      01/01/2023\n      14:38\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      BROOKLYN QUEENS EXPRESSWAY RAMP\n      NaN\n      NaN\n      ...\n      Driver Inattention/Distraction\n      Driver Inattention/Distraction\n      NaN\n      NaN\n      4594563\n      Sedan\n      Sedan\n      Sedan\n      NaN\n      NaN\n    \n    \n      1\n      01/01/2023\n      8:04\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NASSAU EXPRESSWAY\n      NaN\n      NaN\n      ...\n      Unspecified\n      NaN\n      NaN\n      NaN\n      4594599\n      Sedan\n      Sedan\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      01/01/2023\n      18:05\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      10 AVENUE\n      11 AVENUE\n      NaN\n      ...\n      Unspecified\n      NaN\n      NaN\n      NaN\n      4594810\n      Sedan\n      Taxi\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      01/01/2023\n      23:45\n      NaN\n      NaN\n      40.769737\n      -73.91244\n      (40.769737, -73.91244)\n      ASTORIA BOULEVARD\n      37 STREET\n      NaN\n      ...\n      Driver Inattention/Distraction\n      NaN\n      NaN\n      NaN\n      4594595\n      Taxi\n      Taxi\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      01/01/2023\n      4:50\n      NaN\n      NaN\n      40.830555\n      -73.85072\n      (40.830555, -73.85072)\n      CASTLE HILL AVENUE\n      EAST 177 STREET\n      NaN\n      ...\n      Unspecified\n      NaN\n      NaN\n      NaN\n      4594761\n      Station Wagon/Sport Utility Vehicle\n      Sedan\n      NaN\n      NaN\n      NaN\n    \n  \n\n5 rows × 29 columns\n\n\n\n\n7.2.3.1 Bar Chart\ngeom_bar(mapping=None, data=None, stat=‘count’, position=‘stack’, na_rm=False, inherit_aes=True, show_legend=None, raster=False, width=None, **kwargs)\nSuppose we are curious about the types of vehicle in the crash, we can make a bar chart to illlustrate that\n\n( # The brackets means print   \n    ggplot(df1)  # The data we are using\n    + geom_bar(aes(x = 'VEHICLE TYPE CODE 1') ) # The plot we want to make\n)\n\n\n\n\n<ggplot: (303974937)>\n\n\nSome improvement of the chart:\n\nBlack is too dreary! We want to make this graph more vivid and fancy(maybe by adding color)\nIn here the words in x axis are really hard to see, so we might make some arrangement for the angle of these words\nAnd also, we want to have a title for the graph, and maybe change the label for axis\nSometimes we may want the spesific counts for the bars – by adding a label\nSuppose we want to fliped the data to verticle – we can do that too\n\n\n(\n    ggplot(df1, # The dataset we are using\n           aes(x = 'VEHICLE TYPE CODE 1', fill='VEHICLE TYPE CODE 1'))  # x is the specific column in the dataset we are using, 'fill' color the columns of Vehicle Type Code 1\"\n    + geom_bar() # The plot we want to make\n    + theme(axis_text_x=element_text(angle=75)) #We want the text to have an angle\n    + ggtitle('Vehicle Counts') # Make a title for the chart\n    + xlab(\"Vehicle_Type\") # Change x lable of the graph\n    + ylab(\"Count\") # Change y lable of the graph\n   #+ coord_flip() # Flipped the data to verticle\n)\n\n\n\n\n<ggplot: (303541745)>\n\n\n\n\n7.2.3.2 Scatter Plot\ngeom_point(mapping=None, data=None, stat=‘identity’, position=‘identity’, na_rm=False, inherit_aes=True, show_legend=None, raster=False, **kwargs)\nSuppose we are curious about the place where Crashes happend, we may do a scatter plot for the longitude and latitude\n\n(\n    ggplot(df1, #The dataset we are using\n       aes(x = 'LONGITUDE', y='LATITUDE')) # Make x and y axis\n        + geom_point() # Fill the points inside the graph\n       #+ geom_smooth(method = 'lm') # It is senseless to do this in here but this is the way we fit a line for scatter plots\n)\n\n/usr/local/lib/python3.11/site-packages/plotnine/layer.py:411: PlotnineWarning: geom_point : Removed 7 rows containing missing values.\n\n\n\n\n\n<ggplot: (304061305)>\n\n\nSome Improvements: 1. Sometimes we might want to change the shape of the dot to something else\n\nWe might find the points are uniform, we may want to change the size of the points too\n\n\n(\n    ggplot(df1, # The dataset we are using\n        aes(x = 'LONGITUDE', y='LATITUDE', size = 'LATITUDE')) # Make x and y axis, and make point size by latitude\n        + geom_point( # Fill the point inside the graph\n         aes(shape='VEHICLE TYPE CODE 1')) # Change the shape of the dots according to Vehicle Type\n)\n\n/usr/local/lib/python3.11/site-packages/plotnine/layer.py:411: PlotnineWarning: geom_point : Removed 8 rows containing missing values.\n/usr/local/lib/python3.11/site-packages/plotnine/guides/guides.py:253: PlotnineWarning: geom_point legend : Removed 1 rows containing missing values.\n\n\n\n\n\n<ggplot: (304056145)>\n\n\nThe Dataset might be too small to see the clustering, we might need to have a bigger one– with some clean up\nAnd also, we can anticipate that a lump of black dots is not beautiful– we might want to change its color to build something fancy!\n\ndf2 = df.head(10000) # A little bit data cleaning process\ndf2[\"LATITUDE\"] = df2[\"LATITUDE\"].replace([0.0], np.nan)\ndf2[\"LONGITUDE\"] = df2[\"LONGITUDE\"].replace([0.0], np.nan)\n\n(\n    ggplot(df2, # The dataset we are using\n        aes(x = 'LONGITUDE', y='LATITUDE', color = 'LATITUDE')) # We have our x as Longitude, y as latitude, and we colored the clusters by its latitude\n        + geom_point()\n        + scale_color_gradient(low='#10098f', high='#0ABAB5',guide='colorbar') # From low lattitude to high lattitude colors -- according to colorbar(p.s. Ultramarine and Tiffany blue, my favorites blue colors)         \n)\n\n/usr/local/lib/python3.11/site-packages/plotnine/layer.py:411: PlotnineWarning: geom_point : Removed 561 rows containing missing values.\n\n\n\n\n\n<ggplot: (303984633)>\n\n\n\n\n7.2.3.3 Histogram\ngeom_histogram(mapping=None, data=None, stat=‘bin’, position=‘stack’, na_rm=False, inherit_aes=True, show_legend=None, raster=False, **kwargs)\nI can not find a continuous variable in the NYC Car Crach dataset, so it might be better to import other dataset to do that\nIn here I will use a dataset plant in Python called diamonds\n\ndiamonds.head()\n\n\n\n\n\n  \n    \n      \n      carat\n      cut\n      color\n      clarity\n      depth\n      table\n      price\n      x\n      y\n      z\n    \n  \n  \n    \n      0\n      0.23\n      Ideal\n      E\n      SI2\n      61.5\n      55.0\n      326\n      3.95\n      3.98\n      2.43\n    \n    \n      1\n      0.21\n      Premium\n      E\n      SI1\n      59.8\n      61.0\n      326\n      3.89\n      3.84\n      2.31\n    \n    \n      2\n      0.23\n      Good\n      E\n      VS1\n      56.9\n      65.0\n      327\n      4.05\n      4.07\n      2.31\n    \n    \n      3\n      0.29\n      Premium\n      I\n      VS2\n      62.4\n      58.0\n      334\n      4.20\n      4.23\n      2.63\n    \n    \n      4\n      0.31\n      Good\n      J\n      SI2\n      63.3\n      58.0\n      335\n      4.34\n      4.35\n      2.75\n    \n  \n\n\n\n\nSuppose we are curious about the carats of these diamonds, we can make a histogram for that\n\n(\n    ggplot(diamonds, # The dataset we are using\n           aes(x='carat')) # The data column we are using\n    + geom_histogram() # We want to do a histogram\n)\n\n/usr/local/lib/python3.11/site-packages/plotnine/stats/stat_bin.py:95: PlotnineWarning: 'stat_bin()' using 'bins = 142'. Pick better value with 'binwidth'.\n\n\n\n\n\n<ggplot: (304258913)>\n\n\nSome Improvements:\n\nWe can make the graph look nicer by defining the number of bins and bins’ width, this graph waste too much places\nWhen we dealing with this data, we might find out that the count is way too large, so we might want to do some normalization to a number that closer to the number of carat(1 maybe)\nSometimes we might want to see the proportion of the graph, we can handle that by some improvements\nWe can also filled the color of the gram with some other variables to see other characristics of these variables, for example, we might curious about the quality of cut of each diamonds\n\n\n(\n    ggplot(diamonds, aes(x = 'carat',\n                        #y = after_stat('count'), # Specify each bin is a count\n                        #y = after_stat('ncount'), # Normalise the count to 1\n                        #y = after_stat('density'), # Density\n                        #y = after_stat('width*density'), # Do some little calculation\n                      fill = 'cut'))  # Filled color by variable'cut'\n    + geom_histogram(binwidth= 0.5) # Change the width of the bin\n)\n\n\n\n\n<ggplot: (304357769)>\n\n\nWe can even make the plot more fancy by its own theme!\n\n(\n    ggplot(diamonds, aes(x = 'carat',\n                      y = after_stat('count'), # Specify each bin is a count\n                     #y = after_stat('ncount'), # Normalise the count to 1\n                     #y = after_stat('density'), # Density\n                     #y = after_stat('width*density')), # Show proportion \n                      fill = 'cut'))  # Filled color by variable'cut'\n    + geom_histogram(binwidth= 0.50) # Change the width of the bin\n   #+ theme_xkcd() # Add a theme to makes it better!\n   #+ theme(rect=element_rect(color='black', size=3, fill='#EEBB0050')) # An example of customize a theme\n    + theme(\n    panel_grid=element_line(color='purple'),\n    panel_grid_major=element_line(size=1.4, alpha=1),\n    panel_grid_major_x=element_line(linetype='dashed'),\n    panel_grid_major_y=element_line(linetype='dashdot'),\n    panel_grid_minor=element_line(alpha=.25),\n    panel_grid_minor_x=element_line(color='red'),\n    panel_grid_minor_y=element_line(color='green'),\n    panel_ontop=False  # Put the points behind the grid\n )\n)\n\n\n\n\n<ggplot: (304281633)>\n\n\n\n\n7.2.3.4 Boxplot\nBack to the NYC Crash Data, suppose we want to analysis the relationship among numbers of persons injured and borough, we might build a boxplot to see that\n\n(\n    ggplot(df1, # The data we are using\n             aes(\"BOROUGH\" , \"NUMBER OF PERSONS INJURED\")) # We define our axis\n    + geom_boxplot() # The plot we are using\n)\n\n\n\n\n<ggplot: (304229633)>\n\n\nSome Improvements:\n\nAdd a title to the plot, change the title of x and y axis\nWe may want to change the color of the boxes..? Sometimes?\nWe can change the theme of the plot\nSometimes we may want to see all the points of the boxplot, we can do that with plotnine\n\n\n(\n    ggplot(df1, # The data we are using\n             aes(\"BOROUGH\" , \"NUMBER OF PERSONS INJURED\")) \n    + geom_boxplot(color = \"#0437F2\") # The plot we are using, and change the color in here\n    + xlab(\"Borough\") # Change the title of x axis\n    + ylab(\"Number of persons injured\") # Change the title of y axis\n    + ggtitle(\"Person Injured within each borough\") # Add a title for the graph\n    + theme_bw() # Maybe we can add a theme sometimes?\n   #+ geom_jitter() # This function can add all the points of the boxplot\n)\n\n\n\n\n<ggplot: (301913469)>\n\n\n\n\n\n7.2.4 Sub Graphs\nAs any other library supporting the Grammar of Graphics, plotnine has a special technique called facet that allows to split one plot into multiple plots based on a factor variable included in the dataset\nFor the sub graphs plotnine we are going to talk about two important grammar– facet_wrap and facet_grid\nThe examples will be illustrated via diamonds dataset\n\ndiamonds.head()\n\n\n\n\n\n  \n    \n      \n      carat\n      cut\n      color\n      clarity\n      depth\n      table\n      price\n      x\n      y\n      z\n    \n  \n  \n    \n      0\n      0.23\n      Ideal\n      E\n      SI2\n      61.5\n      55.0\n      326\n      3.95\n      3.98\n      2.43\n    \n    \n      1\n      0.21\n      Premium\n      E\n      SI1\n      59.8\n      61.0\n      326\n      3.89\n      3.84\n      2.31\n    \n    \n      2\n      0.23\n      Good\n      E\n      VS1\n      56.9\n      65.0\n      327\n      4.05\n      4.07\n      2.31\n    \n    \n      3\n      0.29\n      Premium\n      I\n      VS2\n      62.4\n      58.0\n      334\n      4.20\n      4.23\n      2.63\n    \n    \n      4\n      0.31\n      Good\n      J\n      SI2\n      63.3\n      58.0\n      335\n      4.34\n      4.35\n      2.75\n    \n  \n\n\n\n\n\n7.2.4.1 facet_wrap\nplotnine.facets.facet_wrap(facets=None, nrow=None, ncol=None, scales=‘fixed’, shrink=True, labeller=‘label_value’, as_table=True, drop=True, dir=‘h’)\nSometimes we might want to see a lot of charts inside one large one, we can also do this within Facet_wrap\nFor example, in the diamond dataset, Suppose we are curious about the carat vs. price graphs for each levels of cut, we can do a plot like that\n\n(\n    ggplot(diamonds, aes(x = 'carat', y = 'price'))\n    + geom_point(color = '#4EE2EC') # Diamond blue!\n    + labs(x='carat', y='price')\n   #+ facet_wrap('cut', # Distinguish the levels of cut within the plot of carat vs. price\n                 #ncol = 2) # Change the number of columns\n)\n\n\n\n\n<ggplot: (304464869)>\n\n\n\n\n7.2.4.2 Facet_grid\nplotnine.facets.facet_grid(facets, margins=False, scales=‘fixed’, space=‘fixed’, shrink=True, labeller=‘label_value’, as_table=True, drop=True)\nSometimes we may want to see the facets with more than one variables, we can use Facet_grid\nIn this case, suppose we are curious about the graphs of carat vs. price for each levels of cut and clarity\n\n(\n    ggplot(diamonds, aes(x='carat', y='price', \n                         color = 'depth' # If we want to see another dimension of data, we might use color to illustrate that\n                        ))\n    + geom_point() \n    + labs(x='carat', y='price')\n   #+ facet_grid('cut ~ clarity') # Cut levels at right and clarities at top\n   #+ facet_grid('cut ~ .') # Cut levels only, at top\n   #+ facet_grid('. ~ clarity') # Clarities only, at right\n    + scale_color_gradient(low='#10098f', high='#0ABAB5',guide='colorbar') #The color will represent depth, from low to high by light to dense of the color\n)\n\n\n\n\n<ggplot: (304096745)>\n\n\nWe can also seperate this two-dimensional plot to one dimensional by list all the posible combinations of these characters on the side\nIn this case we can use facet_grid to generate those plots\nAnd also, we might be interested in the trend of these variables, so we may estiamte a linear regression for them\n\n(\n    ggplot(diamonds, aes(x='carat', y='price')) # The plot we want to make\n    + geom_point()\n   #+ geom_smooth() # Estimate Linear Regression\n    + facet_grid('cut+clarity ~ .') # We want to see the carat vs. price data seperated by cut+clarity\n    + theme(strip_text_y = element_text(angle = 0,              # Change facet text angle\n                                        ha = 'left'             # Change text alignment\n                                       ),\n            strip_background_y = element_text(color = '#cfe4ee' # Change background colour of facet background, in this case-- diamond blue!\n                                              , width = 0.2     # Adjust width of facet background to fit facet text\n                                             ),\n            figure_size=(12, 30)                                 # Adjust width & height of figure to fit y-axis\n           )\n)\n\n\n\n\n<ggplot: (304162821)>\n\n\n\n\n\n7.2.5 Some useful resources\nLearn all you need to know about Plotnine via its own website: https://plotnine.readthedocs.io/en/stable/index.html.\nIn case you are interested in data visualization via python, check out this website! https://pythonplot.com/.\nAnd finally there is a useful data visualization Github I found, read it if you are interested! https://github.com/pmaji/practical-python-data-viz-guide."
  },
  {
    "objectID": "supervised.html#introduction",
    "href": "supervised.html#introduction",
    "title": "8  Supervised Learning",
    "section": "8.1 Introduction",
    "text": "8.1 Introduction\nSupervised learning uses labeled datasets to train algorithms that to classify data or predict outcomes accurately. As input data is fed into the model, it adjusts its weights until the model has been fitted appropriately, which occurs as part of the cross validation process.\nIn contrast, unsupervised learning uses unlabeled data to discover patterns that help solve for clustering or association problems. This is particularly useful when subject matter experts are unsure of common properties within a data set."
  },
  {
    "objectID": "supervised.html#classification",
    "href": "supervised.html#classification",
    "title": "8  Supervised Learning",
    "section": "8.2 Classification",
    "text": "8.2 Classification\n\n8.2.1 What is classification problem?\n\nClassificaiton: outcome variable is categorical\nRegression: outcome variable is continuous\nBoth problems can have many covariates (predictors/features)\n\n\n\n8.2.2 Confusion matrix\nhttps://en.wikipedia.org/wiki/Confusion_matrix\n\n\n8.2.3 Measure of classification performance\n\nAccuracy: subset accuracy the set of labels predicted for a sample must exactly match the corresponding set of labels in y_true.\nPrecision: the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.\nRecall: the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\nF1 score: 2 * (precision * recall) / (precision + recall). A harmonic mean of the precision and recall\n\n\n\n8.2.4 Cross-validation\n\nGoal: strike a bias-variance tradeoff.\nK-fold: hold out each fold as testing data.\nScores: minimized to train a model"
  },
  {
    "objectID": "supervised.html#support-vector-machines",
    "href": "supervised.html#support-vector-machines",
    "title": "8  Supervised Learning",
    "section": "8.3 Support Vector Machines",
    "text": "8.3 Support Vector Machines\n\n8.3.1 Introduction\nSupport Vector Machine (SVM) is a type of suppervised learning models that can be used to analyze classification and regression. In this section will develop the intuition behind support vector machines and provide some examples.\n\n\n8.3.2 Package that need to install\nBefore we begin ensure that these this package are installed in your python\npip install scikit-learn\nScikit-learn is a python package that provides efficient versions of a large number of common algorithms It constist of all type of machine learning model which is wildly known such as:\n\nLinear Regression\nLogistic Regression\nDecision Trees\nGaussian Process\n\nFurthermore, it also provide function that can be used anytime and use it on the provided machine learning algorithm. There are two type of functions:\n\nAvalable dataset functions such as Iris dataset load_iris\nRandomly generated datasets function such as make_moon , make_circle etc.\n\n\n\n8.3.3 Support Vector Classifier\nBefore we get into SVM , let us take a look at this simple classification problem. Consider a distinguishable datasets\n\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nseed = 220\n\nfrom sklearn.datasets import make_blobs\nX, y = make_blobs(n_samples=50, centers=2,\n                  random_state= seed, cluster_std=1)\nplt.scatter(X[:, 0], X[:, 1], c=y, s=50);\n\n\n\n\nOne of the solution we can do is to draw lines as a way to seperate these two classes.\n\ndef xfit(m,b):\n    t = np.linspace(-5,5,50)\n    y = m*t + b\n\n    return y\n\nX, y = make_blobs(n_samples=50, centers=2,\n                  random_state= 220, cluster_std=1)\n    \nax = plt.gca()\nax.scatter(X[:, 0], X[:, 1], c=y, s=50)\nt = np.linspace(-5,5,50)\ny1 = xfit(7,-5)\ny2 = xfit(15,9)\ny3 = xfit(-5,-4)\nax.plot(t,y1,label = 'Line 1')\nax.plot(t,y2,label = 'Line 2')\nax.plot(t,y3,label = 'Line 3')\nax.set_xlim(-5, 5)\nax.set_ylim(-7, 0)\nax.legend();\n\n\n\n\nHow do we find the best line that divide them both? In other word we need to find the optimal line or best decision boundary.\nLets import Support Vector Machine module for now to help us find the best line to classify the data set.\n\nfrom sklearn.svm import SVC # \"Support vector classifier\"\nmodel = SVC(kernel='linear', C=1E10)\n# For now lets not think about the purpose of C\nmodel.fit(X, y)\n\nax = plt.gca()\nax.set_xlim(-5, 5)\nax.set_ylim(-7, 0)\n\nxlim = ax.get_xlim()\nylim = ax.get_ylim()\n\n# Create a mesh grid\nx_grid = np.linspace(xlim[0], xlim[1], 30)\ny_grid = np.linspace(ylim[0], ylim[1], 30)\nY_mesh, X_mesh = np.meshgrid(y_grid,x_grid)\nxy = np.vstack([X_mesh.ravel(),Y_mesh.ravel()]).T\n\nP = model.decision_function(xy).reshape(X_mesh.shape)\n\nax.contour(X_mesh, Y_mesh, P, colors='k',\n               levels=[-1, 0 ,1], alpha=0.5,\n               linestyles= ['--','-','--']);\n\nax.scatter(X[:, 0], X[:, 1], c=y, s=50);\n\nax.scatter(model.support_vectors_[:, 0],\n                   model.support_vectors_[:, 1],\n                   s=300, linewidth=2, facecolor ='none', edgecolor = 'black');\n\n\n\n\nThere is a name for this line. Is called margin, it is the shortest distance between the selected observation and the line. In this case we are using the largest margin to seperate the observation. We called it Maximal Margin Classifier.\nThe selected observation (circled points) are called Support Vectors. For simple explaination, it is the points that used to create the margin.\nWhat if we have a weird observation as shown below? What happend if we try to use Maximal Margin Classifier? Lets add a point on an interesting location.\n\n# Addiing a point near yellow side and name it blue\nfrom sklearn.datasets import make_blobs\nX, y = make_blobs(n_samples=50, centers=2,\n                  random_state= 220, cluster_std=1)\n\nX_new = [2, -4]\n\nX = np.vstack([X,X_new])\n\ny_new = np.array([1]).reshape(1)\n\ny = np.append(y, [0], axis=0)\n\nax = plt.subplot()\nax.scatter(X[:, 0], X[:, 1], c=y, s=51);\n\n\n\n\nUsing Maximum Margin Classifier\n\nmodel = SVC(kernel='linear', C=1E10)\nmodel.fit(X, y)\n\nax = plt.gca()\nax.set_xlim(-5, 5)\nax.set_ylim(-7, 0)\n\nxlim = ax.get_xlim()\nylim = ax.get_ylim()\n\n# Create a mesh grid\nx_grid = np.linspace(xlim[0], xlim[1], 30)\ny_grid = np.linspace(ylim[0], ylim[1], 30)\nY_mesh, X_mesh = np.meshgrid(y_grid,x_grid)\nxy = np.vstack([X_mesh.ravel(),Y_mesh.ravel()]).T\n\nP = model.decision_function(xy).reshape(X_mesh.shape)\n\nax.contour(X_mesh, Y_mesh, P, colors='k',\n               levels=[-1, 0 ,1], alpha=0.5,\n               linestyles= ['--','-','--']);\n\nax.scatter(X[:, 0], X[:, 1], c=y, s=50);\n\n\n\n\nAs you can see Maximal Margin Classifier might not be a useful in this case. We must make the margin that is not sensitve to outliers and allow a few misclassifications. So we need to implement Soft Margin to get a better prediction. This is where parameter C comes in.\n\n# New fit with modifiying the C\n\nmodel = SVC(kernel='linear', C=0.1)\nmodel.fit(X, y)\n\nax = plt.gca()\nax.set_xlim(-5, 5)\nax.set_ylim(-7, 0)\n\nxlim = ax.get_xlim()\nylim = ax.get_ylim()\n\n# Create a mesh grid\nx_grid = np.linspace(xlim[0], xlim[1], 30)\ny_grid = np.linspace(ylim[0], ylim[1], 30)\nY_mesh, X_mesh = np.meshgrid(y_grid,x_grid)\nxy = np.vstack([X_mesh.ravel(),Y_mesh.ravel()]).T\n\nP = model.decision_function(xy).reshape(X_mesh.shape)\n\nax.contour(X_mesh, Y_mesh, P, colors='k',\n               levels=0, alpha=0.5, linestyles= '-');\n\nax.scatter(X[:, 0], X[:, 1], c=y, s=50);\n\n\n\n\nIncreasing the parameter C will greatly influence the classification line location\n\nX, y = make_blobs(n_samples=100, centers=2,\n                  random_state=0, cluster_std=1.2)\n\nfig, ax = plt.subplots(1, 2)\nfig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n\nfor axi, C in zip(ax, [100.0, 0.1]):\n    model = SVC(kernel='linear', C=C).fit(X, y)\n    \n\n    axi.set_xlim(-3, 6)\n    axi.set_ylim(-2, 7)\n\n    xlim = axi.get_xlim()\n    ylim = axi.get_ylim()\n\n    # Create a mesh grid\n    x_grid = np.linspace(xlim[0], xlim[1], 30)\n    y_grid = np.linspace(ylim[0], ylim[1], 30)\n    Y_mesh, X_mesh = np.meshgrid(y_grid,x_grid)\n    xy = np.vstack([X_mesh.ravel(),Y_mesh.ravel()]).T\n\n    P = model.decision_function(xy).reshape(X_mesh.shape)\n\n    axi.contour(X_mesh, Y_mesh, P, colors='k',\n               levels=[-1,0,1], alpha=0.5, linestyles=['--','-','--']);\n\n    axi.scatter(X[:, 0], X[:, 1], c=y, s=50)\n    axi.set_title('C = {0:.1f}'.format(C), size=14)\n\n\n\n\n\n8.3.3.1 Support Vector Machine\nNow we have some basic understanding on classifiying thing, lets take a look at the sample problem below.\n\nfrom sklearn.datasets import make_circles\nX, y = make_circles(100, factor=.1, noise=.1)\n\nplt.scatter(X[:, 0], X[:, 1], c=y, s=50);\n\n\n\n\nIf we apply a standard Support Vector Classifier the result will be like this.\n\nclf = SVC(kernel='linear').fit(X, y)\n\nax = plt.gca()\nax.set_xlim(-1.5, 1.5)\nax.set_ylim(-1.5, 1.5)\n\nxlim = ax.get_xlim()\nylim = ax.get_ylim()\n\n# Create a mesh grid\nx_grid = np.linspace(xlim[0], xlim[1], 30)\ny_grid = np.linspace(ylim[0], ylim[1], 30)\nY_mesh, X_mesh = np.meshgrid(y_grid,x_grid)\nxy = np.vstack([X_mesh.ravel(),Y_mesh.ravel()]).T\n\nP = clf.decision_function(xy).reshape(X_mesh.shape)\n\nax.contour(X_mesh, Y_mesh, P, colors='k',\n               levels=0, alpha=0.5, linestyles= '-');\n\nax.scatter(X[:, 0], X[:, 1], c=y, s=50);\n\n\n\n\nThis is not a good classifier. We need a way to make it better. Instead of just using the available data, let us try to convert a data to a better dimension space.\n\nr = np.exp(-(X ** 2).sum(1))\n\nIn this case we will implement a kernel that will translate our data to a new diemension. This is one of the way to fit a nonlinear relationship with a linear classifier.\n\nax = plt.subplot(projection='3d')\nax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=50);\n#ax.view_init(elev=-90, azim=30)\nax.set_xlabel('x');\nax.set_ylabel('y');\nax.set_zlabel('r');\n\n\n\n\nNow you can see that it is seperated. We can apply the Support Vector Classifier to the dataset\n\nr = r.reshape(100,1)\n\nb = np.concatenate((X,r),1)\n\nfrom sklearn.svm import SVC \n\nclf = SVC(kernel='linear').fit(b, y)\n\nax = plt.gca()\nax.set_xlim(-1.5, 1.5)\nax.set_ylim(-1.5, 1.5)\n\nxlim = ax.get_xlim()\nylim = ax.get_ylim()\n\n# Create a mesh grid\nx_grid = np.linspace(xlim[0], xlim[1], 30)\ny_grid = np.linspace(ylim[0], ylim[1], 30)\nY_mesh, X_mesh = np.meshgrid(y_grid,x_grid)\nxy = np.vstack([X_mesh.ravel(),Y_mesh.ravel()]).T\n\nr_1 = np.exp(-(xy ** 2).sum(1))\n\nr_1 = r_1.reshape(900,1)\n\nb_1 = np.concatenate((xy,r_1),1)\n\nP = clf.decision_function(b_1).reshape(X_mesh.shape)\n\nax.contour(X_mesh, Y_mesh, P, colors='k',\n                levels=0, alpha=0.5, linestyles= '-');\n\nax.scatter(X[:, 0], X[:, 1], c=y, s=50);\n\n\n\n\nOr you can just use SVC radial basis fucntion kernel to automatically create a decision boundary for you.\n\nclf = SVC(kernel='rbf').fit(X, y)\n\nax = plt.gca()\nax.set_xlim(-1.5, 1.5)\nax.set_ylim(-1.5, 1.5)\n\nxlim = ax.get_xlim()\nylim = ax.get_ylim()\n\n# Create a mesh grid\nx_grid = np.linspace(xlim[0], xlim[1], 30)\ny_grid = np.linspace(ylim[0], ylim[1], 30)\nY_mesh, X_mesh = np.meshgrid(y_grid,x_grid)\nxy = np.vstack([X_mesh.ravel(),Y_mesh.ravel()]).T\n\nP = clf.decision_function(xy).reshape(X_mesh.shape)\n\nax.contour(X_mesh, Y_mesh, P, colors='k',\n               levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-','--']);\n\nax.scatter(X[:, 0], X[:, 1], c=y, s=50);\n\nax.scatter(clf.support_vectors_[:, 0],\n                   clf.support_vectors_[:, 1],\n                   s=300, linewidth=2, facecolor ='none', edgecolor = 'black');\n\n\n\n\nAs for summary, Support Vector Machine follow these steps:\n\nStart with a data in low dimension.\nUse kernel to move the data to a higher dimension.\nFind a Support Vector Classifier that seperate the data into two groups.\n\n\n\n8.3.3.2 Kernel\nLet talk more about the kernel. There are mutiple type of kernel. We will go through a few of them. Generaly, they call as a kernel trick or kernel method or kernel function. For simple explanation, these kernel can be view as a method on how we transform the data points into. It may need to transform to a higher dimension it may not.\n\nLinear Kernel The linear kernel is a kernel that uses the dot product of the input vectors to measure their similarity: \\[k(x,x')= (x\\cdot x')\\]\nPolynomial Kernel\n\nFor homogeneous case: \\[k(x,x')= (x\\cdot x')^d\\] where if \\(d = 1\\) it wil be act as linear kernel.\nFor inhomogeneous case: \\[k(x,x')= (x\\cdot x' + r )^d\\] where r is a coefficient.\n\nRadial Basis Function Kernel (or rbf) is a well know kernel that can transform the data to a infinite dimension space.\n\nThe function is known as:\n\\(k(x,x') = \\exp\\left(-\\gamma\\left\\Vert x-x' \\right\\Vert^2\\right)\\)\n\\(\\gamma >0\\). Sometimes parametrized using \\(\\gamma = \\frac{1}{2\\sigma^2}\\)\n\n\n8.3.3.3 Regression Problem\nWe will talk a little on Regression Problem and how it works on Support Vector Machine.\nLets consider a data output as shown below.\n\nfrom sklearn.datasets import make_regression\nimport matplotlib.pyplot as plt\n\nX, y = make_regression(n_samples=100, n_features=1, noise=10, random_state = 2220)\n\nplt.scatter(X, y, marker='o')\nplt.show()\n\n\n\n\nSo how Support Vector Machine works for regression problem? Instead of giving some math formulas. Let do a fit and show the output of the graph.\n\nfrom sklearn.svm import SVR\n\nmodel = SVR(kernel='linear', C = 100, epsilon = 10)\n\nmodel.fit(X, y)\n\nX_new = np.linspace(-3, 3, 100).reshape(-1, 1)\ny_pred = model.predict(X_new)\n\nplt.scatter(X, y, marker='o')\nplt.plot(X_new, y_pred, color='red')\nplt.plot(X_new, y_pred + model.epsilon, color='black', linestyle='--')\nplt.plot(X_new, y_pred - model.epsilon, color='black', linestyle='--')\nplt.show()\n\n\n\n\nAs you can see for regression problem Support Vector Machine for Regression or SVR create a two black lines as the decision boundary and the red line as the hyperplane. Our objective is to ensure points are within the boundary. The best fit line is the hyperplane that has a maximum number of points.\nYou can control the model by adjust the C value and epsilon value. C value change the slope of the line, lower the value will reduce the slope of the fit line. epsilon change the distance of the decision boundary, lower the epsilon reduce the distance of the dicision boundary.\n\n\n8.3.3.4 Example: Classification\nLet take a look at our NYC database. We would like to create a machine learning model with SVM.\n\nimport pandas as pd\n\njan23 = pd.read_csv(\"data/nyc_crashes_202301_cleaned.csv\")\njan23.head()\n\n\n\n\n\n  \n    \n      \n      CRASH DATE\n      CRASH TIME\n      BOROUGH\n      ZIP CODE\n      LATITUDE\n      LONGITUDE\n      LOCATION\n      ON STREET NAME\n      CROSS STREET NAME\n      OFF STREET NAME\n      ...\n      Unnamed: 30\n      Unnamed: 31\n      Unnamed: 32\n      Unnamed: 33\n      Unnamed: 34\n      Unnamed: 35\n      Unnamed: 36\n      Unnamed: 37\n      Unnamed: 38\n      Unnamed: 39\n    \n  \n  \n    \n      0\n      1/1/23\n      14:38\n      BROOKLYN\n      11211.0\n      40.719094\n      -73.946108\n      (40.7190938,-73.9461082)\n      BROOKLYN QUEENS EXPRESSWAY RAMP\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      1/1/23\n      8:04\n      QUEENS\n      11430.0\n      40.659508\n      -73.773687\n      (40.6595077,-73.7736867)\n      NASSAU EXPRESSWAY\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      1/1/23\n      18:05\n      MANHATTAN\n      10011.0\n      40.742454\n      -74.008686\n      (40.7424543,-74.008686)\n      10 AVENUE\n      11 AVENUE\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      1/1/23\n      23:45\n      QUEENS\n      11103.0\n      40.769737\n      -73.912440\n      (40.769737, -73.91244)\n      ASTORIA BOULEVARD\n      37 STREET\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      1/1/23\n      4:50\n      BRONX\n      10462.0\n      40.830555\n      -73.850720\n      (40.830555, -73.85072)\n      CASTLE HILL AVENUE\n      EAST 177 STREET\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n5 rows × 40 columns\n\n\n\nLet us merge with uszipcode database to increase the number of input value to predict injury.\n\n#Calculate the sum\njan23['sum'] = jan23['NUMBER OF PERSONS INJURED'] + jan23['NUMBER OF PEDESTRIANS INJURED']+ jan23['NUMBER OF CYCLIST INJURED'] + jan23['NUMBER OF MOTORIST INJURED']\n\nfor index in jan23.index:\n    if jan23['sum'][index] > 0:\n        jan23.loc[index,['injured']] = 1\n    else:\n        jan23.loc[index,['injured']] = 0\n        \nfrom uszipcode import SearchEngine\n\nsearch = SearchEngine()\n\nresultlist = []\n\nfor index in jan23.index:\n    checkZip = jan23['ZIP CODE'][index]\n    if np.isnan(checkZip) == False:\n        zipcode = int(checkZip)\n        result = search.by_zipcode(zipcode)\n        resultlist.append(result.to_dict())\n    else:\n        resultlist.append({})\n\nZipcode_data = pd.DataFrame.from_records(resultlist)\n\nmerge = pd.concat([jan23, Zipcode_data], axis=1)\n\n# Drop the repeated zipcode\nmerge = merge.drop(['zipcode','lat','lng'],axis = 1)\n\nmerge = merge[merge['population'].notnull()]\n\nFocus_data = merge[['radius_in_miles', 'population', 'population_density',\n'land_area_in_sqmi', 'water_area_in_sqmi', 'housing_units',\n'occupied_housing_units','median_home_value','median_household_income','injured']]\n\nThese are the focus data that we will apply SVM to.\n\nFocus_data.head()\n\n\n\n\n\n  \n    \n      \n      radius_in_miles\n      population\n      population_density\n      land_area_in_sqmi\n      water_area_in_sqmi\n      housing_units\n      occupied_housing_units\n      median_home_value\n      median_household_income\n      injured\n    \n  \n  \n    \n      0\n      2.000000\n      90117.0\n      39209.0\n      2.30\n      0.07\n      37180.0\n      33489.0\n      655500.0\n      46848.0\n      1.0\n    \n    \n      2\n      0.909091\n      50984.0\n      77436.0\n      0.66\n      0.00\n      33252.0\n      30294.0\n      914500.0\n      104238.0\n      0.0\n    \n    \n      3\n      0.852273\n      38780.0\n      54537.0\n      0.71\n      0.00\n      18518.0\n      16890.0\n      648900.0\n      55129.0\n      1.0\n    \n    \n      4\n      1.000000\n      75784.0\n      51207.0\n      1.48\n      0.00\n      31331.0\n      29855.0\n      271300.0\n      45864.0\n      0.0\n    \n    \n      5\n      2.000000\n      80018.0\n      36934.0\n      2.17\n      0.05\n      34885.0\n      30601.0\n      524100.0\n      51725.0\n      0.0\n    \n  \n\n\n\n\nTo reduce the complexity, we will get 1000 sample from the dataset and import train_test_split to split up our data to measure the performance.\n\nrandom_sample = Focus_data.sample(n=1000, random_state=220)\n\n#Create X input\nX = random_sample[['radius_in_miles', 'population', 'population_density',\n'land_area_in_sqmi', 'water_area_in_sqmi', 'housing_units',\n'occupied_housing_units','median_home_value','median_household_income']].values\n\n#Create Y for output\ny  = random_sample['injured'].values\n\nfrom sklearn.model_selection import train_test_split \n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n\nApply SVM to our dataset and make a prediction on X_test\n\nfrom sklearn.svm import SVC \n\nclf = SVC(kernel='rbf').fit(X_train, y_train)\n\n#Make prediction using X_test\ny_pred = clf.predict(X_test)\n\nCheck our accuracy of our model by importing accuracy_score from sklearn.metrics\n\nfrom sklearn.metrics import accuracy_score\n\naccuracy = accuracy_score(y_test, y_pred)\n\naccuracy\n\n0.685\n\n\n\n\n\n8.3.4 Conclusion\nSupport Vector Machines is one of the powerful tool mainly for classifications.\n\nTheir dependence on relatively few support vectors means that they are very compact models, and take up very little memory.\nOnce the model is trained, the prediction phase is very fast.\nBecause they are affected only by points near the margin, they work well with high-dimensional data—even data with more dimensions than samples, which is a challenging regime for other algorithms.\nTheir integration with kernel methods makes them very versatile, able to adapt to many types of data.\n\nHowever, SVMs have several disadvantages as well:\n\nThe scaling with the number of samples \\(N\\) is \\(O[N^3]\\) at worst, or \\(O[N^2]\\) for efficient implementations. For large numbers of training samples, this computational cost can be prohibitive.\nThe results are strongly dependent on a suitable choice for the softening parameter \\(C\\).This must be carefully chosen via cross-validation, which can be expensive as datasets grow in size.\nThe results do not have a direct probabilistic interpretation. This can be estimated via an internal cross-validation (see the probability parameter of SVC), but this extra estimation is costly.\n\n\n\n8.3.5 References\n\nIn-Depth: Support Vector Machines\nSupport Vector Machines Video"
  },
  {
    "objectID": "nycrash.html#first-glance",
    "href": "nycrash.html#first-glance",
    "title": "9  NYC Crash Data",
    "section": "9.1 First Glance",
    "text": "9.1 First Glance\nConsider the NYC Crash Data in January 2022.\n\nimport pandas as pd\n\njan23 = pd.read_csv(\"data/nyc_crashes_202301.csv\")\njan23.head()\njan23.describe()\n\n\n\n\n\n  \n    \n      \n      ZIP CODE\n      LATITUDE\n      LONGITUDE\n      NUMBER OF PERSONS INJURED\n      NUMBER OF PERSONS KILLED\n      NUMBER OF PEDESTRIANS INJURED\n      NUMBER OF PEDESTRIANS KILLED\n      NUMBER OF CYCLIST INJURED\n      NUMBER OF CYCLIST KILLED\n      NUMBER OF MOTORIST INJURED\n      NUMBER OF MOTORIST KILLED\n      COLLISION_ID\n    \n  \n  \n    \n      count\n      4796.000000\n      6764.000000\n      6764.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7.244000e+03\n    \n    \n      mean\n      10893.521685\n      40.234660\n      -73.033403\n      0.502761\n      0.002347\n      0.116372\n      0.000690\n      0.033269\n      0.000414\n      0.333103\n      0.001242\n      4.599022e+06\n    \n    \n      std\n      526.392428\n      4.430595\n      8.041457\n      0.813641\n      0.051164\n      0.397927\n      0.026265\n      0.180118\n      0.020348\n      0.749174\n      0.038951\n      2.365885e+03\n    \n    \n      min\n      10001.000000\n      0.000000\n      -74.250150\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      4.594332e+06\n    \n    \n      25%\n      10457.000000\n      40.663230\n      -73.964888\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      4.597113e+06\n    \n    \n      50%\n      11208.000000\n      40.711880\n      -73.921230\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      4.599058e+06\n    \n    \n      75%\n      11239.000000\n      40.775640\n      -73.865389\n      1.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      4.600953e+06\n    \n    \n      max\n      11694.000000\n      40.912827\n      0.000000\n      21.000000\n      2.000000\n      19.000000\n      1.000000\n      2.000000\n      1.000000\n      8.000000\n      2.000000\n      4.605324e+06\n    \n  \n\n\n\n\nFrequency tables for categorical variables.\n\njan23[\"BOROUGH\"].value_counts(dropna=False)\n\nNaN              2448\nBROOKLYN         1653\nQUEENS           1316\nBRONX             821\nMANHATTAN         811\nSTATEN ISLAND     195\nName: BOROUGH, dtype: int64\n\n\nSome tables are too long.\n\n# jan23[\"CONTRIBUTING FACTOR VEHICLE 1\"].value_counts(dropna=False)\nwith pd.option_context('display.max_rows', None):\n    print(jan23[\"VEHICLE TYPE CODE 1\"].value_counts(dropna=False))\n\nSedan                                  3478\nStation Wagon/Sport Utility Vehicle    2505\nTaxi                                    177\nPick-up Truck                           162\nNaN                                     136\nBus                                     135\nBox Truck                               110\nBike                                     84\nAmbulance                                55\nTractor Truck Diesel                     53\nE-Bike                                   50\nVan                                      43\nMotorcycle                               28\nE-Scooter                                28\nDump                                     18\nGarbage or Refuse                        17\nFlat Bed                                 17\nMoped                                    17\nPK                                       16\nConvertible                              11\nChassis Cab                               9\nCarry All                                 9\nTanker                                    7\nTow Truck / Wrecker                       7\nTractor Truck Gasoline                    7\nLIMO                                      5\nAMBULANCE                                 5\nMotorscooter                              5\nFlat Rack                                 3\nMotorbike                                 3\n4 dr sedan                                3\nConcrete Mixer                            2\nFiretruck                                 2\nMOTOR SCOO                                2\nSTAK                                      2\nSANITATION                                1\nBeverage Truck                            1\nSCHOOL BUS                                1\nWaste truc                                1\nForklift                                  1\nAMBU                                      1\nUTILITY                                   1\nLog                                       1\nPAS                                       1\n3-Door                                    1\nFDNY AMBUL                                1\nMulti-Wheeled Vehicle                     1\nScooter                                   1\nStake or Rack                             1\nLift Boom                                 1\nGas Moped                                 1\nBulk Agriculture                          1\nArmored Truck                             1\nTRAILER                                   1\nelectric s                                1\nMotorized                                 1\nVAn                                       1\nELECTRIC S                                1\nUTIL                                      1\nFORK LIFT                                 1\nSTREET SWE                                1\nGARBAGE TR                                1\nChevy                                     1\ncart                                      1\nTow Truck                                 1\nNYC FDNY #                                1\nScooter ga                                1\nVAN TRUCK                                 1\nName: VEHICLE TYPE CODE 1, dtype: int64\n\n\nCross-tables\n\npd.crosstab(index = jan23[\"CONTRIBUTING FACTOR VEHICLE 1\"],\n            columns = jan23[\"BOROUGH\"], dropna = False)\n\n\n\n\n\n  \n    \n      BOROUGH\n      BRONX\n      BROOKLYN\n      MANHATTAN\n      QUEENS\n      STATEN ISLAND\n    \n    \n      CONTRIBUTING FACTOR VEHICLE 1\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Accelerator Defective\n      3\n      4\n      1\n      1\n      0\n    \n    \n      Aggressive Driving/Road Rage\n      9\n      12\n      6\n      12\n      0\n    \n    \n      Alcohol Involvement\n      7\n      32\n      9\n      30\n      8\n    \n    \n      Animals Action\n      0\n      0\n      0\n      0\n      1\n    \n    \n      Backing Unsafely\n      31\n      57\n      29\n      54\n      10\n    \n    \n      Brakes Defective\n      3\n      6\n      2\n      4\n      1\n    \n    \n      Cell Phone (hand-Held)\n      0\n      1\n      0\n      0\n      0\n    \n    \n      Driver Inattention/Distraction\n      177\n      415\n      211\n      307\n      45\n    \n    \n      Driver Inexperience\n      15\n      41\n      20\n      18\n      6\n    \n    \n      Driverless/Runaway Vehicle\n      1\n      1\n      2\n      3\n      0\n    \n    \n      Drugs (illegal)\n      2\n      5\n      1\n      0\n      0\n    \n    \n      Failure to Keep Right\n      1\n      2\n      0\n      2\n      0\n    \n    \n      Failure to Yield Right-of-Way\n      58\n      108\n      51\n      157\n      12\n    \n    \n      Fatigued/Drowsy\n      0\n      0\n      0\n      3\n      1\n    \n    \n      Fell Asleep\n      3\n      8\n      3\n      10\n      0\n    \n    \n      Following Too Closely\n      30\n      56\n      38\n      69\n      10\n    \n    \n      Glare\n      2\n      2\n      1\n      2\n      1\n    \n    \n      Illnes\n      1\n      1\n      0\n      5\n      1\n    \n    \n      Lane Marking Improper/Inadequate\n      1\n      0\n      0\n      0\n      0\n    \n    \n      Lost Consciousness\n      2\n      3\n      1\n      4\n      2\n    \n    \n      Obstruction/Debris\n      1\n      1\n      1\n      1\n      1\n    \n    \n      Other Electronic Device\n      0\n      0\n      1\n      1\n      0\n    \n    \n      Other Lighting Defects\n      1\n      0\n      0\n      0\n      0\n    \n    \n      Other Vehicular\n      40\n      33\n      28\n      29\n      3\n    \n    \n      Outside Car Distraction\n      5\n      5\n      0\n      3\n      0\n    \n    \n      Oversized Vehicle\n      5\n      7\n      2\n      3\n      1\n    \n    \n      Passenger Distraction\n      1\n      4\n      5\n      4\n      0\n    \n    \n      Passing Too Closely\n      41\n      70\n      36\n      44\n      1\n    \n    \n      Passing or Lane Usage Improper\n      26\n      68\n      33\n      80\n      9\n    \n    \n      Pavement Defective\n      0\n      2\n      0\n      0\n      0\n    \n    \n      Pavement Slippery\n      1\n      6\n      5\n      5\n      0\n    \n    \n      Pedestrian/Bicyclist/Other Pedestrian Error/Confusion\n      10\n      14\n      17\n      3\n      1\n    \n    \n      Physical Disability\n      1\n      0\n      0\n      1\n      1\n    \n    \n      Reaction to Uninvolved Vehicle\n      1\n      6\n      10\n      12\n      4\n    \n    \n      Steering Failure\n      5\n      4\n      0\n      7\n      1\n    \n    \n      Texting\n      0\n      0\n      0\n      0\n      1\n    \n    \n      Tinted Windows\n      0\n      1\n      1\n      0\n      0\n    \n    \n      Tire Failure/Inadequate\n      0\n      2\n      0\n      3\n      1\n    \n    \n      Traffic Control Device Improper/Non-Working\n      0\n      0\n      0\n      1\n      0\n    \n    \n      Traffic Control Disregarded\n      25\n      39\n      16\n      50\n      8\n    \n    \n      Turning Improperly\n      22\n      48\n      20\n      35\n      11\n    \n    \n      Unsafe Lane Changing\n      7\n      16\n      14\n      9\n      2\n    \n    \n      Unsafe Speed\n      29\n      32\n      22\n      56\n      8\n    \n    \n      Unspecified\n      229\n      508\n      209\n      273\n      38\n    \n    \n      View Obstructed/Limited\n      15\n      15\n      7\n      12\n      4\n    \n    \n      Windshield Inadequate\n      1\n      0\n      0\n      0\n      0"
  },
  {
    "objectID": "nycrash.html#some-cleaning",
    "href": "nycrash.html#some-cleaning",
    "title": "9  NYC Crash Data",
    "section": "9.2 Some Cleaning",
    "text": "9.2 Some Cleaning\nQuestions from Dr. Douglas Bates:\n\nThe CRASH_DATEs are all in the correct month and there are no missing values\nThere are no missing values in the CRASH_TIMEs but there are 117 values of exactly 00:00:00. Is this a matter of bad luck when the clock strikes midnight?\nOver 1/3 of the ZIP_CODE and BOROUGH values are missing. There are the same number of missing values in these columns - do they always co-occur? If LATITUDE and LONGITUDE are available, can they be used to infer the ZIP_CODE?\nThere are 178 unique non-missing ZIP_CODE values as stated in the Jamboree description. (“Trust, but verify.”) Is there really a zip code of 10000 in New York?\nThere are 20 values of 0.0 for LATITUDE and LONGITUDE? These are obviously incorrect - should they be coded as missing?\nIs it redundant to keep the LOCATIO in addition to LATITUDE and LONGITUDE?\nThe COLLISION_ID is unique to each row and can be used as a key. The values are not consecutive - why not?\nThe NUMBER_OF_... columns seem reasonable. A further consistency check is suggested in the Jamboree tasks.\nIn the CONTRIBUTING_FACTOR_… columns, is Unspecified different from missing?\nThe codes in the VEHICLE_TYPE_CODE_... columns are the usual hodge-podge of results from “free-form” data entry. Should unk, UNK, UNKNOWN, and Unknown be converted to missing?\nIn contrast, the codes in the CONTRIBUTING_FACTOR_... columns appear to be standardized (not sure why Illnes isn’t Illness).\n\n\nwith pd.option_context('display.max_rows', None):\n    print(jan23[\"CRASH TIME\"].value_counts())\n\n0:00     116\n15:00     75\n18:00     72\n17:00     69\n10:00     65\n8:00      61\n8:30      59\n13:00     58\n7:00      58\n14:00     56\n19:00     55\n16:00     54\n23:00     54\n12:00     54\n9:00      53\n20:00     53\n17:30     51\n14:30     51\n10:30     51\n12:30     51\n11:00     49\n9:30      42\n15:30     40\n16:30     39\n18:30     38\n21:00     38\n6:00      36\n11:30     35\n20:30     35\n6:30      33\n4:00      32\n1:00      31\n5:00      31\n22:00     31\n13:30     30\n23:30     27\n22:30     27\n6:50      27\n21:30     26\n18:20     26\n15:20     25\n17:20     25\n15:15     24\n18:45     24\n14:50     23\n7:30      23\n15:45     23\n1:30      23\n14:40     23\n8:20      23\n19:30     23\n9:45      22\n13:20     22\n3:00      22\n16:20     22\n7:50      22\n18:40     21\n21:45     21\n16:45     21\n17:45     21\n10:50     21\n16:10     21\n8:40      20\n10:40     20\n8:15      20\n20:50     19\n1:20      19\n13:40     19\n18:35     19\n11:20     19\n17:40     19\n6:20      19\n9:40      19\n2:00      19\n18:50     18\n18:10     18\n9:20      18\n9:15      18\n20:15     18\n12:50     18\n16:35     18\n0:30      18\n12:20     18\n14:45     18\n22:40     18\n12:15     18\n16:40     18\n12:45     18\n19:40     18\n10:45     17\n17:50     17\n19:50     17\n19:20     17\n8:45      17\n15:50     17\n17:55     17\n10:20     17\n12:10     17\n3:30      16\n13:45     16\n17:35     16\n7:45      16\n11:40     16\n15:10     16\n16:15     16\n11:45     16\n14:15     16\n15:40     16\n13:15     15\n9:50      15\n17:15     15\n1:45      15\n6:45      15\n19:10     15\n12:40     15\n18:05     15\n22:20     14\n16:50     14\n17:25     14\n21:40     14\n20:40     14\n5:30      14\n8:50      14\n11:15     14\n22:45     14\n12:25     14\n11:50     14\n16:55     14\n11:10     14\n14:20     13\n14:35     13\n8:55      13\n19:45     13\n17:10     13\n11:55     13\n8:25      13\n4:30      13\n7:20      13\n14:55     13\n6:40      13\n23:40     12\n6:55      12\n19:15     12\n6:05      12\n11:05     12\n10:15     12\n7:05      12\n8:35      12\n7:15      12\n13:10     12\n15:35     12\n22:15     12\n23:45     12\n5:45      12\n19:05     12\n21:50     11\n7:10      11\n16:25     11\n9:25      11\n14:10     11\n19:55     11\n14:05     11\n17:05     11\n5:35      11\n22:10     11\n20:10     11\n10:25     11\n13:25     11\n15:05     10\n20:45     10\n6:15      10\n19:35     10\n23:15     10\n3:50      10\n20:35     10\n0:40      10\n5:10      10\n18:15     10\n23:20     10\n21:15     10\n2:20      10\n7:25      10\n9:05       9\n4:50       9\n23:50      9\n18:29      9\n1:50       9\n20:20      9\n9:10       9\n13:05      9\n2:30       9\n19:25      9\n16:05      9\n11:04      9\n13:50      9\n20:25      9\n22:50      9\n7:40       9\n15:25      9\n0:20       9\n16:49      9\n10:35      9\n18:25      9\n1:10       8\n22:41      8\n21:20      8\n0:55       8\n3:45       8\n0:10       8\n12:35      8\n3:10       8\n1:55       8\n23:05      8\n8:17       8\n12:05      8\n5:40       8\n18:55      8\n8:10       8\n8:05       8\n17:16      8\n5:50       8\n10:55      8\n23:35      7\n18:08      7\n18:51      7\n11:34      7\n21:10      7\n6:10       7\n2:40       7\n22:25      7\n10:05      7\n17:54      7\n3:20       7\n16:48      7\n18:34      7\n5:20       7\n15:55      7\n4:20       7\n20:05      7\n1:15       7\n10:28      7\n4:40       7\n12:55      7\n18:18      7\n1:05       7\n23:25      7\n15:43      7\n4:15       7\n3:15       7\n7:35       7\n22:35      7\n21:35      7\n19:19      6\n14:51      6\n11:25      6\n10:10      6\n17:13      6\n20:37      6\n20:23      6\n14:58      6\n17:08      6\n19:49      6\n17:39      6\n21:08      6\n23:13      6\n22:55      6\n12:08      6\n10:19      6\n23:23      6\n7:48       6\n16:28      6\n14:22      6\n18:22      6\n8:44       6\n18:58      6\n16:33      6\n6:54       6\n7:55       6\n7:06       6\n8:08       6\n2:15       6\n21:04      6\n21:25      6\n19:18      6\n5:55       6\n3:25       6\n5:05       6\n0:45       6\n20:55      6\n17:29      6\n15:33      6\n2:35       6\n0:50       6\n14:54      6\n15:57      6\n13:24      6\n13:36      6\n18:28      6\n2:50       6\n8:14       6\n6:25       6\n10:37      6\n21:01      5\n19:51      5\n0:06       5\n17:11      5\n3:40       5\n16:37      5\n18:16      5\n14:57      5\n20:14      5\n19:57      5\n1:25       5\n7:19       5\n18:06      5\n13:34      5\n13:52      5\n20:59      5\n8:28       5\n19:36      5\n13:28      5\n14:11      5\n12:46      5\n17:44      5\n2:27       5\n16:38      5\n17:46      5\n8:58       5\n19:26      5\n15:26      5\n22:18      5\n11:56      5\n14:25      5\n11:32      5\n18:48      5\n19:11      5\n8:47       5\n13:04      5\n18:07      5\n20:24      5\n9:35       5\n21:58      5\n23:06      5\n8:16       5\n19:52      5\n12:51      5\n17:28      5\n4:37       5\n18:04      5\n17:26      5\n13:48      5\n13:55      5\n10:54      5\n18:52      5\n17:58      5\n21:06      5\n19:21      5\n16:16      5\n14:28      5\n17:12      5\n18:37      5\n22:12      5\n11:35      5\n18:44      5\n14:44      5\n1:40       5\n13:07      5\n18:24      5\n0:25       5\n12:48      5\n0:09       5\n21:05      5\n13:35      5\n0:35       5\n21:55      5\n9:22       5\n20:48      5\n21:29      5\n21:07      5\n11:17      5\n16:58      4\n13:56      4\n6:37       4\n14:21      4\n7:41       4\n20:18      4\n5:44       4\n6:49       4\n16:02      4\n20:49      4\n14:39      4\n5:12       4\n22:05      4\n13:09      4\n8:43       4\n4:10       4\n12:54      4\n19:28      4\n17:14      4\n5:38       4\n1:21       4\n15:21      4\n0:18       4\n0:15       4\n18:57      4\n21:03      4\n19:06      4\n17:48      4\n23:38      4\n12:18      4\n6:53       4\n21:23      4\n19:59      4\n16:31      4\n10:04      4\n13:54      4\n8:02       4\n6:52       4\n16:51      4\n23:33      4\n0:42       4\n17:17      4\n9:44       4\n19:12      4\n19:13      4\n22:17      4\n16:27      4\n15:48      4\n10:48      4\n8:52       4\n7:18       4\n17:52      4\n7:08       4\n15:04      4\n9:59       4\n14:26      4\n20:17      4\n17:57      4\n6:38       4\n20:32      4\n23:19      4\n18:14      4\n11:24      4\n19:47      4\n12:03      4\n10:18      4\n14:06      4\n14:38      4\n19:16      4\n3:35       4\n4:25       4\n16:13      4\n8:12       4\n14:53      4\n21:13      4\n6:58       4\n14:56      4\n8:36       4\n18:26      4\n13:53      4\n11:23      4\n22:14      4\n15:41      4\n9:41       4\n13:57      4\n13:02      4\n15:16      4\n15:13      4\n10:11      4\n4:05       4\n20:06      4\n11:42      4\n14:37      4\n5:07       4\n0:05       4\n10:56      4\n8:21       4\n21:47      4\n9:55       4\n16:04      4\n17:36      4\n22:58      4\n8:34       4\n8:46       4\n16:14      4\n15:58      4\n15:42      4\n6:27       4\n19:23      4\n16:44      4\n8:59       4\n23:10      4\n12:37      4\n18:43      4\n21:16      4\n5:15       4\n22:23      4\n11:18      4\n17:49      4\n4:55       4\n15:08      4\n10:38      4\n9:28       4\n13:18      4\n20:31      4\n16:47      4\n13:43      4\n9:11       4\n9:57       3\n12:28      3\n12:07      3\n2:45       3\n3:17       3\n3:02       3\n0:13       3\n12:47      3\n12:29      3\n23:39      3\n23:48      3\n12:33      3\n18:38      3\n20:41      3\n16:12      3\n16:43      3\n14:14      3\n16:32      3\n23:09      3\n21:44      3\n18:09      3\n22:39      3\n17:51      3\n6:59       3\n1:18       3\n7:42       3\n23:54      3\n17:34      3\n11:19      3\n5:27       3\n8:41       3\n23:55      3\n23:07      3\n21:51      3\n14:24      3\n9:03       3\n23:01      3\n7:39       3\n22:13      3\n22:46      3\n3:05       3\n18:17      3\n19:46      3\n7:51       3\n15:52      3\n9:07       3\n20:51      3\n11:38      3\n14:33      3\n14:02      3\n8:48       3\n8:57       3\n15:22      3\n18:56      3\n18:47      3\n15:44      3\n21:21      3\n4:45       3\n17:06      3\n16:59      3\n17:21      3\n17:42      3\n20:44      3\n18:12      3\n12:01      3\n17:43      3\n18:02      3\n7:52       3\n22:53      3\n10:39      3\n14:01      3\n19:33      3\n17:41      3\n20:42      3\n12:34      3\n6:35       3\n2:55       3\n20:11      3\n1:42       3\n17:47      3\n12:38      3\n23:57      3\n18:42      3\n16:24      3\n4:34       3\n9:12       3\n0:19       3\n17:18      3\n2:10       3\n2:07       3\n0:47       3\n22:08      3\n0:57       3\n21:09      3\n7:38       3\n14:03      3\n4:36       3\n20:39      3\n9:54       3\n13:44      3\n19:04      3\n14:16      3\n11:29      3\n2:21       3\n15:12      3\n13:46      3\n1:29       3\n11:53      3\n14:27      3\n18:03      3\n20:52      3\n12:11      3\n18:41      3\n7:26       3\n10:33      3\n21:33      3\n8:13       3\n19:09      3\n0:11       3\n1:35       3\n14:29      3\n17:38      3\n6:07       3\n19:03      3\n3:23       3\n0:01       3\n16:26      3\n22:52      3\n19:53      3\n11:51      3\n17:31      3\n21:54      3\n17:07      3\n3:11       3\n8:42       3\n9:26       3\n9:42       3\n16:21      3\n17:22      3\n13:37      3\n9:53       3\n20:07      3\n17:33      3\n23:31      3\n19:32      3\n0:04       3\n18:32      3\n19:48      3\n22:11      3\n20:26      3\n7:12       3\n16:36      3\n16:46      3\n8:49       3\n11:13      3\n15:32      3\n14:12      3\n21:11      3\n12:39      3\n9:51       3\n1:34       3\n18:19      3\n14:04      3\n17:59      3\n20:12      3\n9:18       3\n10:17      3\n21:38      3\n6:06       3\n3:29       3\n21:02      3\n22:36      3\n4:35       3\n10:57      3\n22:24      3\n19:38      3\n9:29       3\n9:47       3\n8:24       3\n11:16      3\n17:53      3\n13:31      3\n18:27      3\n17:02      3\n13:14      3\n10:36      3\n16:41      3\n3:16       3\n19:54      3\n12:59      3\n2:26       3\n9:01       3\n21:42      2\n9:06       2\n11:52      2\n0:07       2\n11:49      2\n18:13      2\n17:24      2\n12:04      2\n5:59       2\n15:51      2\n11:48      2\n17:19      2\n3:37       2\n19:31      2\n7:01       2\n8:38       2\n4:51       2\n21:56      2\n15:19      2\n19:56      2\n21:22      2\n18:39      2\n20:38      2\n15:11      2\n14:31      2\n0:24       2\n3:53       2\n9:04       2\n20:57      2\n20:16      2\n15:47      2\n20:28      2\n7:09       2\n9:49       2\n20:29      2\n16:22      2\n14:47      2\n9:17       2\n9:19       2\n11:33      2\n17:37      2\n18:23      2\n8:27       2\n21:52      2\n5:49       2\n1:51       2\n14:13      2\n20:09      2\n11:44      2\n12:23      2\n9:08       2\n22:51      2\n10:32      2\n13:49      2\n16:08      2\n16:17      2\n6:46       2\n22:38      2\n12:16      2\n9:31       2\n19:17      2\n7:56       2\n12:52      2\n2:53       2\n14:36      2\n22:27      2\n15:49      2\n13:27      2\n19:58      2\n15:29      2\n10:34      2\n12:53      2\n12:13      2\n18:53      2\n5:25       2\n12:14      2\n14:52      2\n10:46      2\n4:57       2\n15:54      2\n23:14      2\n14:59      2\n1:44       2\n1:47       2\n15:38      2\n15:06      2\n6:31       2\n23:12      2\n19:14      2\n7:44       2\n10:12      2\n11:28      2\n13:59      2\n0:12       2\n23:18      2\n15:39      2\n21:27      2\n23:28      2\n4:19       2\n7:04       2\n21:41      2\n20:13      2\n4:28       2\n23:24      2\n3:55       2\n19:22      2\n7:46       2\n14:17      2\n21:26      2\n16:42      2\n12:12      2\n9:43       2\n2:52       2\n17:23      2\n16:18      2\n4:42       2\n8:06       2\n6:19       2\n15:01      2\n15:34      2\n23:44      2\n5:47       2\n13:17      2\n15:28      2\n18:36      2\n10:41      2\n10:24      2\n17:27      2\n19:08      2\n1:13       2\n21:32      2\n2:29       2\n1:52       2\n4:16       2\n1:24       2\n17:04      2\n16:53      2\n20:47      2\n21:59      2\n20:22      2\n3:48       2\n0:34       2\n2:41       2\n12:44      2\n3:14       2\n18:46      2\n9:39       2\n6:21       2\n14:19      2\n4:54       2\n14:42      2\n0:02       2\n10:51      2\n3:07       2\n5:42       2\n3:04       2\n0:26       2\n2:11       2\n16:01      2\n20:53      2\n9:23       2\n8:04       2\n10:53      2\n16:29      2\n15:03      2\n22:47      2\n0:17       2\n7:17       2\n7:34       2\n10:26      2\n18:11      2\n18:21      2\n21:18      2\n10:01      2\n8:29       2\n15:36      2\n23:11      2\n2:25       2\n11:57      2\n2:42       2\n17:56      2\n20:02      2\n16:56      2\n6:18       2\n22:49      2\n6:44       2\n0:49       2\n13:13      2\n17:09      2\n23:59      2\n2:51       2\n17:32      2\n8:33       2\n13:41      2\n0:16       2\n23:21      2\n14:48      2\n9:46       2\n10:22      2\n12:41      2\n13:12      2\n19:41      2\n4:43       2\n4:53       2\n3:12       2\n0:43       2\n21:19      2\n16:07      2\n13:06      2\n21:37      2\n20:36      2\n14:09      2\n14:23      2\n23:02      2\n18:54      2\n23:42      2\n13:26      2\n5:01       2\n23:08      2\n0:32       2\n16:03      2\n1:32       2\n23:41      2\n8:09       2\n3:54       2\n11:47      2\n8:26       2\n22:04      2\n10:16      2\n23:52      2\n15:23      2\n16:34      2\n2:08       2\n8:01       2\n2:36       2\n5:36       2\n4:32       2\n0:27       2\n20:54      2\n15:18      2\n7:07       2\n22:01      2\n22:26      2\n18:49      2\n6:36       2\n20:04      2\n9:32       2\n22:16      2\n1:17       2\n12:57      2\n15:02      2\n1:01       2\n2:05       2\n1:38       2\n7:03       2\n16:11      2\n0:38       2\n13:22      2\n10:07      2\n13:01      2\n0:08       2\n5:09       2\n13:38      2\n23:16      2\n11:09      1\n2:56       1\n19:43      1\n1:36       1\n7:13       1\n16:09      1\n13:21      1\n3:57       1\n12:36      1\n22:42      1\n22:03      1\n3:56       1\n6:17       1\n11:07      1\n4:46       1\n13:32      1\n1:16       1\n2:22       1\n10:42      1\n4:07       1\n23:51      1\n9:36       1\n2:34       1\n20:19      1\n16:52      1\n11:46      1\n4:31       1\n10:23      1\n18:33      1\n7:58       1\n10:29      1\n3:46       1\n1:11       1\n1:22       1\n1:09       1\n22:33      1\n5:19       1\n11:36      1\n9:34       1\n12:17      1\n3:59       1\n23:53      1\n2:04       1\n13:39      1\n4:47       1\n10:02      1\n10:03      1\n6:43       1\n0:23       1\n21:53      1\n0:03       1\n2:54       1\n2:18       1\n6:33       1\n2:38       1\n0:58       1\n6:08       1\n14:32      1\n22:07      1\n13:47      1\n19:37      1\n5:37       1\n1:59       1\n6:14       1\n22:34      1\n5:22       1\n20:58      1\n1:19       1\n3:08       1\n0:41       1\n18:01      1\n9:16       1\n10:21      1\n7:29       1\n13:16      1\n1:12       1\n18:59      1\n2:59       1\n1:57       1\n11:03      1\n5:34       1\n5:33       1\n6:24       1\n0:37       1\n1:49       1\n1:56       1\n2:23       1\n11:22      1\n13:19      1\n4:48       1\n2:17       1\n0:28       1\n19:29      1\n7:37       1\n19:02      1\n2:47       1\n4:39       1\n4:27       1\n0:53       1\n4:09       1\n2:48       1\n3:47       1\n3:36       1\n11:43      1\n3:09       1\n4:49       1\n1:08       1\n6:47       1\n14:49      1\n0:33       1\n4:17       1\n16:54      1\n6:09       1\n12:42      1\n22:29      1\n23:22      1\n22:31      1\n0:31       1\n5:11       1\n5:32       1\n5:39       1\n7:31       1\n3:28       1\n8:32       1\n19:34      1\n5:41       1\n10:13      1\n12:27      1\n1:26       1\n6:23       1\n1:02       1\n22:56      1\n19:44      1\n22:54      1\n7:24       1\n6:29       1\n12:24      1\n8:31       1\n3:26       1\n22:37      1\n9:24       1\n23:46      1\n23:27      1\n1:31       1\n3:52       1\n7:54       1\n8:51       1\n9:27       1\n1:14       1\n5:24       1\n22:09      1\n3:43       1\n10:59      1\n5:02       1\n3:19       1\n6:51       1\n1:58       1\n5:46       1\n1:37       1\n15:07      1\n13:33      1\n12:06      1\n16:19      1\n11:59      1\n6:03       1\n22:02      1\n11:08      1\n20:01      1\n6:04       1\n15:14      1\n23:32      1\n21:36      1\n7:57       1\n19:01      1\n10:14      1\n12:56      1\n15:17      1\n10:43      1\n16:39      1\n3:03       1\n11:39      1\n12:49      1\n2:28       1\n9:58       1\n5:29       1\n11:58      1\n8:18       1\n11:12      1\n22:59      1\n6:41       1\n5:03       1\n7:33       1\n22:21      1\n6:56       1\n8:56       1\n17:03      1\n1:03       1\n7:59       1\n7:43       1\n15:31      1\n12:19      1\n15:24      1\n7:49       1\n6:16       1\n20:43      1\n20:21      1\n10:06      1\n6:22       1\n14:43      1\n15:46      1\n11:11      1\n7:23       1\n15:53      1\n5:28       1\n2:44       1\n10:08      1\n9:33       1\n19:07      1\n20:56      1\n23:47      1\n5:21       1\n7:22       1\n9:02       1\n3:44       1\n13:29      1\n4:58       1\n2:33       1\n2:13       1\n5:52       1\n11:37      1\n17:01      1\n23:17      1\n2:58       1\n6:13       1\n6:26       1\n7:28       1\n3:24       1\n11:06      1\n19:27      1\n4:08       1\n10:09      1\n13:08      1\n21:14      1\n0:56       1\n12:32      1\n12:02      1\n5:06       1\n15:27      1\n0:48       1\n11:54      1\n3:38       1\n4:23       1\n7:53       1\n14:46      1\n6:01       1\n21:12      1\n8:37       1\n2:02       1\n21:43      1\n3:42       1\n6:48       1\n23:49      1\n22:48      1\n1:39       1\n0:44       1\n22:43      1\n20:08      1\n5:56       1\n8:53       1\n13:58      1\n4:06       1\n2:03       1\n5:26       1\n11:14      1\n8:03       1\n0:36       1\n2:14       1\n23:56      1\n6:42       1\n8:23       1\n14:18      1\nName: CRASH TIME, dtype: int64\n\n\nFor example, here are some cleaning steps:\n\nimport numpy as np\n\njan23[\"CONTRIBUTING FACTOR VEHICLE 1\"] = (\n    jan23[\"CONTRIBUTING FACTOR VEHICLE 1\"].replace([\"Unspecified\"], np.nan))\njan23[\"CONTRIBUTING FACTOR VEHICLE 2\"] = (\n    jan23[\"CONTRIBUTING FACTOR VEHICLE 2\"].replace([\"Unspecified\"], np.nan))\njan23[\"CONTRIBUTING FACTOR VEHICLE 3\"] = (\n    jan23[\"CONTRIBUTING FACTOR VEHICLE 3\"].replace([\"Unspecified\"], np.nan))\njan23[\"CONTRIBUTING FACTOR VEHICLE 4\"] = (\n    jan23[\"CONTRIBUTING FACTOR VEHICLE 4\"].replace([\"Unspecified\"], np.nan))\njan23[\"CONTRIBUTING FACTOR VEHICLE 5\"] = (\n    jan23[\"CONTRIBUTING FACTOR VEHICLE 5\"].replace([\"Unspecified\"], np.nan))\njan23[\"LATITUDE\"] = jan23[\"LATITUDE\"].replace([0.0], np.nan)\njan23[\"LONGITUDE\"] = jan23[\"LONGITUDE\"].replace([0.0], np.nan)\njan23.describe()\n\n\n\n\n\n  \n    \n      \n      ZIP CODE\n      LATITUDE\n      LONGITUDE\n      NUMBER OF PERSONS INJURED\n      NUMBER OF PERSONS KILLED\n      NUMBER OF PEDESTRIANS INJURED\n      NUMBER OF PEDESTRIANS KILLED\n      NUMBER OF CYCLIST INJURED\n      NUMBER OF CYCLIST KILLED\n      NUMBER OF MOTORIST INJURED\n      NUMBER OF MOTORIST KILLED\n      COLLISION_ID\n    \n  \n  \n    \n      count\n      4796.000000\n      6683.000000\n      6683.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7244.000000\n      7.244000e+03\n    \n    \n      mean\n      10893.521685\n      40.722317\n      -73.918590\n      0.502761\n      0.002347\n      0.116372\n      0.000690\n      0.033269\n      0.000414\n      0.333103\n      0.001242\n      4.599022e+06\n    \n    \n      std\n      526.392428\n      0.081602\n      0.085654\n      0.813641\n      0.051164\n      0.397927\n      0.026265\n      0.180118\n      0.020348\n      0.749174\n      0.038951\n      2.365885e+03\n    \n    \n      min\n      10001.000000\n      40.504658\n      -74.250150\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      4.594332e+06\n    \n    \n      25%\n      10457.000000\n      40.665350\n      -73.965530\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      4.597113e+06\n    \n    \n      50%\n      11208.000000\n      40.713196\n      -73.922740\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      4.599058e+06\n    \n    \n      75%\n      11239.000000\n      40.777754\n      -73.867714\n      1.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      4.600953e+06\n    \n    \n      max\n      11694.000000\n      40.912827\n      -73.702095\n      21.000000\n      2.000000\n      19.000000\n      1.000000\n      2.000000\n      1.000000\n      8.000000\n      2.000000\n      4.605324e+06\n    \n  \n\n\n\n\nBy the data dictionary, OFF STREET NAME is the street address of the collision site. Some records have OFF STREET NAME but missing LATITUDE and LONGITUDE. The geocode can be filled by geocoding the street address with package"
  },
  {
    "objectID": "nycrash.html#filling-the-missing-zip-codes-by-reverse-geocoding",
    "href": "nycrash.html#filling-the-missing-zip-codes-by-reverse-geocoding",
    "title": "9  NYC Crash Data",
    "section": "9.3 Filling the Missing Zip Codes by Reverse Geocoding",
    "text": "9.3 Filling the Missing Zip Codes by Reverse Geocoding\nThe package uszipcode is the most powerful and easy to use programmable zipcode database in Python. It provides information about 42,724 zipcodes in the US with data crawled from <data.census.gov>. See its documentation for details.\n\nfrom uszipcode import SearchEngine\n\nsr = SearchEngine()\nsr.by_zipcode(\"10001\")\n\nSimpleZipcode(zipcode='10001', zipcode_type='STANDARD', major_city='New York', post_office_city='New York, NY', common_city_list=['New York'], county='New York County', state='NY', lat=40.75, lng=-74.0, timezone='America/New_York', radius_in_miles=0.9090909090909091, area_code_list='718,917,347,646', population=21102, population_density=33959.0, land_area_in_sqmi=0.62, water_area_in_sqmi=0.0, housing_units=12476, occupied_housing_units=11031, median_home_value=650200, median_household_income=81671, bounds_west=-74.008621, bounds_east=-73.984076, bounds_north=40.759731, bounds_south=40.743451)\n\n\nWe can use uszipcode to reverse geocode a point by its coordinates. The returned zipcode can be used to handle missing zipcode.\n\nz = sr.by_coordinates(40.769993, -73.915825, radius = 1)\nz[0].zipcode\nz[0].median_home_value\n\n597700\n\n\nOnce we have found the zipcode, we can find its borough. See the complete NYC zip code list.\n\ndef nyczip2burough(zip):\n    nzip = int(zip)\n    if nzip >= 10001 and nzip <= 10282:\n        return \"MANHATTAN\"\n    elif nzip >= 10301 and nzip <= 10314:\n        return \"STATEN ISLAND\"\n    elif nzip >= 10451 and nzip <= 10475:\n        return \"BRONX\"\n    elif nzip >= 11004 and nzip <= 11109:\n        return \"QUEENS\"\n    elif nzip >= 11351 and nzip <= 11697:\n        return \"QUEENS\"\n    elif nzip >= 11201 and nzip <= 11256:\n        return \"BROOKLYN\"\n    else:\n        return np.nan\n\nLet’s try it out:\n\nnyczip2burough(z[0].zipcode)\n\n'QUEENS'\n\n\nHere is a vectorized version:\n\nimport numpy as np\nimport pandas as pd\nfrom typing import Union, List\n\ndef nyczip2borough(zips: Union[np.ndarray, pd.Series]) -> Union[np.ndarray, pd.Series]:\n    zips = zips.values if isinstance(zips, pd.Series) else zips\n    condlist = [\n        (zips >= 10001) & (zips <= 10282),\n        (zips >= 10301) & (zips <= 10314),\n        (zips >= 10451) & (zips <= 10475),\n        (zips >= 11004) & (zips <= 11109),\n        (zips >= 11351) & (zips <= 11697),\n        (zips >= 11201) & (zips <= 11256),\n    ]\n    choicelist = [\n        \"MANHATTAN\",\n        \"STATEN ISLAND\",\n        \"BRONX\",\n        \"QUEENS\",\n        \"QUEENS\",\n        \"BROOKLYN\",\n    ]\n    result = np.select(condlist, choicelist, default=np.nan)\n    return pd.Series(result) if isinstance(zips, pd.Series) else result\n\nTry it out\n\nnyczip2borough(jan23[\"ZIP CODE\"].dropna().head(10))\n\narray(['BROOKLYN', 'QUEENS', 'MANHATTAN', 'QUEENS', 'BROOKLYN', 'QUEENS',\n       'QUEENS', 'BROOKLYN', 'QUEENS', 'STATEN ISLAND'], dtype='<U32')\n\n\nThe uszipcode package provides databases at the zip code level from the US Census. Such information could be merged with the NYC crash data for further analysis.\n\nfrom uszipcode import SearchEngine, SimpleZipcode\nimport os\n# set the default database file location\ndb_file = os.path.join(os.getenv(\"HOME\"), \"simple_db.sqlite\")\nsearch = SearchEngine(db_file_path=db_file)\nsearch.by_zipcode(\"10030\")\n\nSimpleZipcode(zipcode='10030', zipcode_type='STANDARD', major_city='New York', post_office_city='New York, NY', common_city_list=['New York'], county='New York County', state='NY', lat=40.82, lng=-73.94, timezone='America/New_York', radius_in_miles=0.5681818181818182, area_code_list='212,646,917', population=26999, population_density=96790.0, land_area_in_sqmi=0.28, water_area_in_sqmi=0.0, housing_units=12976, occupied_housing_units=11395, median_home_value=509000, median_household_income=31925, bounds_west=-73.948677, bounds_east=-73.936232, bounds_north=40.824032, bounds_south=40.812791)\n\n\nThe SQL database of US zip code is stored in $HOME/.uszipcode. It can be imported as a pandas dataframe.\n\nimport sqlite3\nimport pandas as pd\n# change to your own path after installing uszipcode\ncon = sqlite3.connect(db_file)\nzipdf = pd.read_sql_query(\"SELECT * from simple_zipcode\", con)\nzipdf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 42724 entries, 0 to 42723\nData columns (total 24 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   zipcode                  42724 non-null  object \n 1   zipcode_type             42724 non-null  object \n 2   major_city               42724 non-null  object \n 3   post_office_city         33104 non-null  object \n 4   common_city_list         41877 non-null  object \n 5   county                   42724 non-null  object \n 6   state                    42724 non-null  object \n 7   lat                      42724 non-null  float64\n 8   lng                      42724 non-null  float64\n 9   timezone                 42724 non-null  object \n 10  radius_in_miles          33104 non-null  float64\n 11  area_code_list           42724 non-null  object \n 12  population               31448 non-null  float64\n 13  population_density       31448 non-null  float64\n 14  land_area_in_sqmi        31448 non-null  float64\n 15  water_area_in_sqmi       31448 non-null  float64\n 16  housing_units            31448 non-null  float64\n 17  occupied_housing_units   31448 non-null  float64\n 18  median_home_value        31448 non-null  float64\n 19  median_household_income  31448 non-null  float64\n 20  bounds_west              33104 non-null  float64\n 21  bounds_east              33104 non-null  float64\n 22  bounds_north             33104 non-null  float64\n 23  bounds_south             33104 non-null  float64\ndtypes: float64(15), object(9)\nmemory usage: 7.8+ MB\n\n\nThe zip code dataframe can be merged with the crash dataframe."
  },
  {
    "objectID": "nycrash.html#map-the-crash-sites",
    "href": "nycrash.html#map-the-crash-sites",
    "title": "9  NYC Crash Data",
    "section": "9.4 Map the Crash Sites",
    "text": "9.4 Map the Crash Sites\nWe can do this with package gmplot. See instructions from this tutorial.\n\nimport gmplot\nimport numpy as np\n\n# prepare the geododes\nlatitude  = jan23[\"LATITUDE\"].dropna().values\nlongitude = jan23[\"LONGITUDE\"].dropna().values\n\n# center of the map and zoom level\ngmap = gmplot.GoogleMapPlotter(40.769737, -73.91244, 14)\n\n# plot heatmap\ngmap.heatmap(latitude, longitude)\ngmap.scatter(latitude, longitude, c = 'r', marker = True)\n# gmap.scatter(latitude, longitude, '#FF0000', size = 50, marker = False)\n# Your Google_API_Key\n# gmap.apikey = \"put your key here\"\n# save it to html\ngmap.draw(r\"nycrashmap.html\")"
  },
  {
    "objectID": "exercises.html",
    "href": "exercises.html",
    "title": "10  Exercises",
    "section": "",
    "text": "Pick up Git basics and set up an account at GitHub if you don’t have one. Please practice the tips on Git in the notes. Make sure you have at least 10 commits in the repo, each with informative message. Keep checking the status of your repo with git status. My grader will grade the repo.\n\nClone the ids-s23 repo to your own computer.\nAdd your name and wishes to the Wishlist; commit with an informative message.\nRemove the Last, First entry from the list; commit.\nCreate a new file called add.qmd containing a few lines of texts; commit.\nRemove add.qmd (pretending that this is by accident; commit.\nRecover the accidently removed file add.qmd; add a long line (a paragraph without a hard break); add a short line (under 80 characters); commit.\nChange one word in the long line and one word in the short line; use git diff to see the difference from the last commit; commit.\nPut the repo into the GitHub Classroom homework repo with git remote add and git push.\n\nGet ready for contributing to the classnotes.\n\nCreate a fork of the ids-s23 repo into your own GitHub account.\nClone it to your local computer.\nMake a new branch to experiment with your changes.\nCheckout your branch and add your wishes to the wish list; push to your GitHub account.\nMake a pull request to my ids-s23 repo from your fork at GitHub. Make sure you have clear messages to document the changes.\n\nWrite a function to demonstrate the Monty Hall problem through simulation. The function takes two arguments ndoors and ntrials, representing the number of doors in the experiment and the number of trails in a simulation, respectively. The function should return the proportion of wins for both the switch and no-switch strategy. Apply your function with 3 doors and 5 doors, both with 1000 trials. Include sufficient text around the code to explain your them.\nWrite a function to do a Monte Carlo approximation of \\(\\pi\\). The function takes a Monte Carlo sample size n as input, and returns a point estimate of \\(\\pi\\) and a 95% confidence interval. Apply your function with sample size 1000, 2000, 4000, and 8000. Repeat the experiment 1000 times for each sample size and check the empirical probability that the confidence intervals cover the true value of \\(\\pi\\). Comment on the results.\nFind the first 10-digit prime number occurring in consecutive digits of \\(e\\). This was a Google recruiting ad\nThe NYC motor vehicle collisions data with documentation is available from NYC Open Data. The raw data needs some cleaning. (JY: Add variable name cleaning next year.)\n\nUse the filter from the website to download the crash data of January 2023; save it under a directory data with an informative name (e.g., nyc_crashes_202301.csv).\nGet basic summaries of each variable: missing percentage; descriptive statistics for continuous variables; frequency tables for discrete variables.\nAre the LATITUDE and LONGITIDE values all look legitimate? If not (e.g., zeroes), code them as missing values.\nIf OFF STREET NAME is not missing, are there any missing LATITUDE and LONGITUDE? If so, geocode the addresses.\n(Optional) Are the missing patterns of ON STREET NAME and LATITUDE the same? Summarize the missing patterns by a cross table. If ON STREET NAME and CROSS STREET NAME are available, use geocoding by intersection to fill the LATITUDE and LONGITUDE.\nAre ZIP CODE and BOROUGH always missing together? If LATITUDE and LONGITUDE are available, use reverse geocoding to fill the ZIP CODE and BOROUGH.\nPrint the whole frequency table of CONTRIBUTING FACTOR VEHICLE 1. Convert lower cases to uppercases and check the frequencies again.\nProvided an opportunity to meet the data provider, what suggestions do you have to make the data better based on your data exploration experience?\n\nExcept the first problem, use the cleaned data set with missing geocode imputed (data/nyc_crashes_202301_cleaned.csv).\n\nConstruct a contigency table for missing in geocode (latitude and longitude) by borough. Is the missing pattern the same across borough? Formulate a hypothesis and test it.\nConstruct a hour variable with integer values from 0 to 23. Plot the histogram of the number of crashes by hour. Plot it by borough.\nOverlay the locations of the crashes on a map of NYC. The map could be a static map or Google map.\nCreate a new variable injury which is one if the number of persons injured is 1 or more; and zero otherwise. Construct a cross table for injury versus borough. Test the null hypothesis that the two variables are not associated.\nMerge the crash data with the zip code database.\nFit a logistic model with injury as the outcome variable and covariates that are available in the data or can be engineered from the data. For example, zip code level covariates can be obtained by merging with the zip code database.\n\nUsing the cleaned NYC crash data, perform classification of injury with support vector machine and compare the results with the benchmark from regularized logistic regression. Use the last week’s data as testing data.\n\nExplain the parameters you used in your fitting for each method.\nExplain the confusion matrix retult from each fit.\nCompare the performance of the two approaches in terms of accuracy, precision, recall, F1-score, and AUC.\n\n(Mid-term team project) The NYC Open Data of 311 Service Requests contains all requests from 2010 to present. We consider a subset of it with request time between 00:00:00 01/15/2023 and 24:00:00 01/21/2023. The subset is available in CSV format as data/nyc311_011523-012123_by022023.csv. Read the data dictionary to understand the meaning of the variables,\n\nClean the data: fill missing fields as much as possible; check for obvious data entry errors (e.g., can Closed Date be earlier than Created Date?); summarize your suggestions to the data curator in several bullet points.\nRemove requests that are not made to NYPD and create a new variable duration, which represents the time period from the Crated Date to Closed Date. Note that duration may be censored for some requests. Visualize the distribution of uncensored duration by weekdays/weekend and by borough, and test whether the distributions are the same across weekdays/weekends and across borough.\nBuild a model to predict the duration for 311 requests to get closed. If your model has tuning parameters, justify their choices. Apply this model to the 311 requests of NYPD in the week of 01/22/2023. Assess the performance of your model.\nNow you know the data quite well. Come up with a research question of interest that can be answered by the data, which could be analytics or visualizations. Perform the needed analyses and answer your question."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "VanderPlas, Jake. 2016. Python Data Science Handbook:\nEssential Tools for Working with Data. O’Reilly Media,\nInc."
  }
]